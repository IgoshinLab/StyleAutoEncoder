{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reconstruction Evaluation\n",
    "In this notebook, we will reconstruct images with different models, and evaluate the reconstruction performance of different models.\n",
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "916c1a218a0c3664"
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "from metrics import metric_utils\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import random\n",
    "\n",
    "\n",
    "def resize_crop(img_dir, resize_by=1., resolution=512, brightness_norm=True, brightness_mean=107.2, locations=None):\n",
    "    if locations is None:\n",
    "        locations = [\"left\", \"right\"]\n",
    "    img = cv2.imread(img_dir, cv2.IMREAD_UNCHANGED)\n",
    "    if img.dtype != np.uint8:\n",
    "        img = np.uint8(img / 256)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_shape = img.shape\n",
    "    resize_shape = np.array([img_shape[1] * resize_by, img_shape[0] * resize_by], dtype=int)\n",
    "    if resize_by != 1:\n",
    "        img = cv2.resize(img, resize_shape, cv2.INTER_LANCZOS4)\n",
    "    imgs = []\n",
    "    for location in locations:\n",
    "        if location == \"left\":\n",
    "            new_img = img[(resize_shape[1] - resolution) // 2:(resize_shape[1] + resolution) // 2, :resolution]\n",
    "        elif location == \"right\":\n",
    "            new_img = img[(resize_shape[1] - resolution) // 2:(resize_shape[1] + resolution) // 2, -resolution:]\n",
    "        else:\n",
    "            new_img = img[(resize_shape[1] - resolution) // 2:(resize_shape[1] + resolution) // 2,\n",
    "                      (resize_shape[0] - resolution) // 2:(resize_shape[0] + resolution) // 2]\n",
    "        if brightness_norm:\n",
    "            obj_v = np.mean(new_img)\n",
    "            value = brightness_mean - obj_v\n",
    "            new_img = cv2.add(new_img, value)\n",
    "        imgs.append(new_img)\n",
    "    return imgs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:34:56.428822Z",
     "start_time": "2024-03-27T21:34:54.606917Z"
    }
   },
   "id": "681c5e59bd4988d6",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target images\n",
    "In the first step, we get the target images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df11eb32dbada626"
  },
  {
   "cell_type": "code",
   "source": [
    "# IMG_DIR = \"/home/xavier/Documents/dataset/Welch/Zethus\"\n",
    "IMG_DIR = \"/home/xavier/Documents/dataset/Welch/trainingset2/trainingset2\"\n",
    "img_pattern = re.compile(r'.*_(\\d{4}).jpg')\n",
    "\n",
    "img_names = []\n",
    "img_names_rep = []\n",
    "for strain_dir in glob.glob(os.path.join(IMG_DIR, \"*\")):\n",
    "    if len(glob.glob(os.path.join(strain_dir, \"Scope*\"))) > 1:\n",
    "        scope1, scope2 = random.sample(glob.glob(os.path.join(strain_dir, \"Scope*\")), 2)\n",
    "        img_files = sorted(glob.glob(os.path.join(scope1, \"*_????.jpg\")))\n",
    "        if img_files:\n",
    "            last_img_file = img_files[-1]\n",
    "            img_names.append(last_img_file)\n",
    "            mid_img_file = img_files[600]\n",
    "            img_names.append(mid_img_file)\n",
    "        img_files = sorted(glob.glob(os.path.join(scope2, \"*_????.jpg\")))\n",
    "        if img_files:\n",
    "            last_img_file = img_files[-1]\n",
    "            img_names_rep.append(last_img_file)\n",
    "            mid_img_file = img_files[600]\n",
    "            img_names_rep.append(mid_img_file)\n",
    "    # for scope_dir in glob.glob(os.path.join(strain_dir, \"Scope*\")):\n",
    "    #     img_files = sorted(glob.glob(os.path.join(scope_dir, \"*_????.jpg\")))\n",
    "    #     if img_files:\n",
    "    #         last_img_file = img_files[-1]\n",
    "    #         imgs.append(last_img_file)\n",
    "    # match = img_pattern.search(last_img_file)\n",
    "    # if match:\n",
    "    #     digits = match.group(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:40:28.518310Z",
     "start_time": "2024-03-27T21:40:25.802444Z"
    }
   },
   "id": "a23ceb43b158604",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check the running loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ee20e79ba8d7a14"
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "model_dict = {\n",
    "    5: \"/home/xavier/PycharmProjects/training-runs/new/e5/00001-stylegan2-trainingset2-gpus4-batch96-gamma10\",\n",
    "    7: \"/home/xavier/PycharmProjects/training-runs/new/e7/00001-stylegan2-trainingset2-gpus4-batch96-gamma10\",\n",
    "    10: \"/home/xavier/PycharmProjects/training-runs/new/e10/00001-stylegan2-trainingset2-gpus4-batch96-gamma10\",\n",
    "    12: \"/home/xavier/PycharmProjects/training-runs/new/e12/00008-stylegan2-trainingset2-gpus4-batch96-gamma10\",\n",
    "    13: \"/home/xavier/PycharmProjects/training-runs/new/e13/00008-stylegan2-trainingset2-gpus4-batch96-gamma10\",\n",
    "    14: \"/home/xavier/PycharmProjects/training-runs/new/e14/00010-stylegan2-trainingset2-gpus4-batch96-gamma10\",\n",
    "    18: \"/home/xavier/PycharmProjects/training-runs/new/e18/00001-stylegan2-trainingset2-gpus4-batch96-gamma10\",\n",
    "}\n",
    "for e_dim in model_dict:\n",
    "    log_path = model_dict[e_dim]\n",
    "\n",
    "    # Initialize an event accumulator\n",
    "    event_acc = EventAccumulator(log_path, size_guidance={'scalars': 0})\n",
    "    event_acc.Reload()  # Load the log files\n",
    "\n",
    "    # List all scalar tags\n",
    "    all_tags = event_acc.Tags()['scalars']\n",
    "\n",
    "    # Filter tags that start with \"Loss/\"\n",
    "    loss_tags = [tag for tag in all_tags if tag.startswith('Loss/G') or tag.startswith('Loss/D')]\n",
    "\n",
    "    loss_tot = 0\n",
    "    # Extract and print the loss values at epoch 1411 for these tags\n",
    "    for tag in loss_tags:\n",
    "        loss_values = event_acc.Scalars(tag)\n",
    "        for event in loss_values:\n",
    "            if event.step == 1411:\n",
    "                loss_tot += event.value\n",
    "                print(f\"{tag} at epoch {event.step}: {event.value}\")\n",
    "    print(f\"e = {e_dim}, loss = {loss_tot}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:40:51.233549Z",
     "start_time": "2024-03-27T21:40:31.169885Z"
    }
   },
   "id": "ad11cd217d98b1f7",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Model and Recalculate Loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6f024dcabf423c6"
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['CC'] = \"/usr/bin/gcc-9\"\n",
    "os.environ['CXX'] = \"/usr/bin/g++-9\"\n",
    "device = torch.device('cuda')\n",
    "model_dict = {\n",
    "    5: \"/home/xavier/PycharmProjects/training-runs/new/e5/00001-stylegan2-trainingset2-gpus4-batch96-gamma10/network-snapshot-001461.pkl\",\n",
    "    7: \"/home/xavier/PycharmProjects/training-runs/new/e7/00001-stylegan2-trainingset2-gpus4-batch96-gamma10/network-snapshot-001461.pkl\",\n",
    "    10: \"/home/xavier/PycharmProjects/training-runs/new/e10/00001-stylegan2-trainingset2-gpus4-batch96-gamma10/network-snapshot-001411.pkl\",\n",
    "    12: \"/home/xavier/PycharmProjects/training-runs/new/e12/00008-stylegan2-trainingset2-gpus4-batch96-gamma10/network-snapshot-001461.pkl\",\n",
    "    13: \"/home/xavier/PycharmProjects/training-runs/new/e13/00008-stylegan2-trainingset2-gpus4-batch96-gamma10/network-snapshot-001461.pkl\",\n",
    "    14: \"/home/xavier/PycharmProjects/training-runs/new/e14/00010-stylegan2-trainingset2-gpus4-batch96-gamma10/network-snapshot-001461.pkl\",\n",
    "    18: \"/home/xavier/PycharmProjects/training-runs/new/e18/00001-stylegan2-trainingset2-gpus4-batch96-gamma10/network-snapshot-001461.pkl\",\n",
    "}\n",
    "OUT_DIR = \"/home/xavier/PycharmProjects/TuringTest/images\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_image(image, subdir, filename):\n",
    "    path = os.path.join(OUT_DIR, subdir)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    cv2.imwrite(os.path.join(path, filename), image)\n",
    "\n",
    "\n",
    "imgs_orig = []\n",
    "imgs_other = []\n",
    "imgs_rep = []\n",
    "for i, img in enumerate(img_names):\n",
    "    left_crop, right_crop = resize_crop(img)\n",
    "    save_image(left_crop, 'left_crops', os.path.basename(img))\n",
    "    save_image(right_crop, 'right_crops', os.path.basename(img))\n",
    "    imgs_orig.append(left_crop[np.newaxis, np.newaxis, :, :])\n",
    "    imgs_other.append(right_crop[np.newaxis, np.newaxis, :, :])\n",
    "for img, couple_name in zip(img_names_rep, img_names):\n",
    "    center_crop = resize_crop(img, locations=['center'])[0]\n",
    "    save_image(center_crop, 'couple_crops', os.path.basename(couple_name))\n",
    "    imgs_rep.append(center_crop[np.newaxis, np.newaxis, :, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:41:08.858420Z",
     "start_time": "2024-03-27T21:40:52.537764Z"
    }
   },
   "id": "e5c049b118868fbd",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate cosine similarity between inception-V3 features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd462f0731a95b8e"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "detector_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl'\n",
    "detector_kwargs = dict(return_features=True)  # Return raw features before the softmax layer.\n",
    "detector = metric_utils.get_feature_detector(detector_url, device=torch.device('cuda'))\n",
    "\n",
    "# Preprocess and calculate features for original and other images\n",
    "img_l_features, img_r_features, img_rep_features = [], [], []\n",
    "\n",
    "for img_l, img_r, img_rep in zip(imgs_orig, imgs_other, imgs_rep):\n",
    "    img_l_tensor = torch.tensor(img_l, device=device, dtype=torch.float32).repeat([1, 3, 1, 1])\n",
    "    img_r_tensor = torch.tensor(img_r, device=device, dtype=torch.float32).repeat([1, 3, 1, 1])\n",
    "    img_rep_tensor = torch.tensor(img_rep, device=device, dtype=torch.float32).repeat([1, 3, 1, 1])\n",
    "    img_l_features.append(detector(img_l_tensor))\n",
    "    img_r_features.append(detector(img_r_tensor))\n",
    "    img_rep_features.append(detector(img_rep_tensor))\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "similarities_table = pd.DataFrame()\n",
    "\n",
    "# Calculate similarities between left and right images (original and other)\n",
    "similarities_lr = []\n",
    "similarities_lrep = []\n",
    "for feat_l, feat_r, feat_rep in zip(img_l_features, img_r_features, img_rep_features):\n",
    "    sim1 = cosine_similarity(feat_l.flatten(start_dim=1), feat_r.flatten(start_dim=1)).item()\n",
    "    similarities_lr.append(sim1)\n",
    "    sim2 = cosine_similarity(feat_l.flatten(start_dim=1), feat_rep.flatten(start_dim=1)).item()\n",
    "    similarities_lrep.append(sim2)\n",
    "similarities_table['Left-Right'] = similarities_lr\n",
    "similarities_table['Left-Rep'] = similarities_lrep"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:41:48.118672Z",
     "start_time": "2024-03-27T21:41:29.137417Z"
    }
   },
   "id": "2ff546e763b7783f",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute models and calculate loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cfd749a422c2391"
  },
  {
   "cell_type": "code",
   "source": [
    "from training.new_loss import StyleGAN2Loss\n",
    "\n",
    "loss_all = {}\n",
    "# Now calculate and add the similarities for each model's reconstructions\n",
    "for model_e, model_path in model_dict.items():\n",
    "    with dnnlib.util.open_url(model_path) as fp:\n",
    "        models = legacy.load_network_pkl(fp)\n",
    "    E, G, D = models['E_ema'].to(device), models['G_ema'].to(device), models['D'].to(device)\n",
    "\n",
    "    loss = StyleGAN2Loss(device, G, D, E)\n",
    "    loss_all[model_e] = 0\n",
    "    similarities = []\n",
    "    for feat_l, img_l, img_r, img_name in zip(img_l_features, imgs_orig, imgs_other, img_names):\n",
    "        img_l_tensor = torch.tensor(img_l, device=device, dtype=torch.float32).div(127.5).sub(1)\n",
    "        img_r_tensor = torch.tensor(img_r, device=device, dtype=torch.float32).div(127.5).sub(1)\n",
    "        mu, logvar = E.mu_var(img_l_tensor, None)\n",
    "        recon = G(mu, None).detach()\n",
    "        recon_clipped = torch.clip(recon, -1, 1)\n",
    "        recon_rescaled = recon_clipped.add(1).div(2).mul(255).type(torch.uint8)\n",
    "        recon_output = recon_rescaled.detach().cpu().numpy()[0, 0]\n",
    "        subdir = f'dim_{model_e}_reconstructions'\n",
    "        save_image(recon_output, subdir, os.path.basename(img_name))\n",
    "        recon_rescaled = recon_rescaled.repeat([1, 3, 1, 1])\n",
    "        feat_r = detector(recon_rescaled)\n",
    "\n",
    "        sim = cosine_similarity(feat_l.flatten(start_dim=1), feat_r.flatten(start_dim=1)).item()\n",
    "        similarities.append(sim)\n",
    "\n",
    "        # Calculate loss\n",
    "        # L_tot = L_G + L_D + L_VAE + L_reg\n",
    "        # L_G = loss_Gmain + loss_Gconstraints + loss_Gpl\n",
    "        gen_logits = loss.run_D([img_l_tensor, recon], [None, None], blur_sigma=0)\n",
    "        loss_Gmain = torch.nn.functional.softplus(-gen_logits)\n",
    "        # L_D = loss_Dreal + loss_Dgen + gradient_penalty\n",
    "        real_logits = loss.run_D([img_r_tensor, img_l_tensor], [None, None], blur_sigma=0)\n",
    "        loss_Dreal = torch.nn.functional.softplus(-real_logits)\n",
    "        loss_Dgen = torch.nn.functional.softplus(gen_logits)\n",
    "        # L_VAE = KLD_loss\n",
    "        # Lreg = loss_Dr1\n",
    "        loss_all[model_e] += (loss_Gmain + loss_Dreal + loss_Dgen).cpu().numpy()[0][0]\n",
    "    similarities_table[f'dim={model_e}'] = similarities\n",
    "    print(f\"{model_e}, {loss_all[model_e]}\")\n",
    "similarities_table.to_csv(os.path.join(OUT_DIR, \"similarities_table.csv\"))\n",
    "similarities_table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:46:23.114262Z",
     "start_time": "2024-03-27T21:41:54.944085Z"
    }
   },
   "id": "f7aad027f1b1076",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recover from error"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1a255ba00e0b778"
  },
  {
   "cell_type": "code",
   "source": [
    "# import shutil\n",
    "# \n",
    "# IMG_DIR = \"/home/xavier/Documents/dataset/Welch/trainingset2/trainingset2\"\n",
    "# SOURCE_DIR = \"/media/xavier/SHGP31/dataset/Welch/trainingset2/trainingset2\"\n",
    "# img_pattern = re.compile(r'.*_(\\d{4}).jpg')\n",
    "# \n",
    "# for strain_dir in glob.glob(os.path.join(IMG_DIR, \"*\")):\n",
    "#     strain_base = os.path.basename(strain_dir)\n",
    "#     for scope in glob.glob(os.path.join(strain_dir, \"Scope*\")):\n",
    "#         scope_base = os.path.basename(scope)\n",
    "#         img_files = sorted(glob.glob(os.path.join(scope, \"*_????.jpg\")))\n",
    "#         if img_files:\n",
    "#             last_img_file = img_files[-1]\n",
    "#             print(last_img_file)\n",
    "#             img_base = os.path.basename(last_img_file)\n",
    "#             shutil.copy(os.path.join(SOURCE_DIR, strain_base, scope_base, img_base),\n",
    "#                         os.path.join(IMG_DIR, strain_base, scope_base, img_base))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T23:04:21.074380264Z",
     "start_time": "2024-02-14T23:04:21.069050284Z"
    }
   },
   "id": "bdaa38af5e50881",
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db0b0c8df10427c2"
  },
  {
   "cell_type": "code",
   "source": [
    "# similarities_table.to_csv(os.path.join(OUT_DIR, \"similarities_table.csv\"))\n",
    "similarities_table = pd.read_csv(os.path.join(OUT_DIR, \"similarities_table.csv\"))\n",
    "plt.figure()\n",
    "plt.hist(similarities_table['Left-Right'], bins=20, alpha=0.5, label=f'Left-Right')\n",
    "plt.hist(similarities_table['Left-Rep'], bins=20, alpha=0.5, label=f'Left-Rep')\n",
    "# Plotting histograms for each model\n",
    "for model_e in model_dict.keys():\n",
    "    plt.hist(similarities_table[f'dim={model_e}'], bins=20, alpha=0.5, label=f'dim={model_e}')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T00:02:59.544504244Z",
     "start_time": "2024-03-08T00:02:59.226147970Z"
    }
   },
   "id": "3a82b5210efccabe",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "similarities_table.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "# Corrected loading and column naming\n",
    "similarities_table.columns = ['Left-Right', 'Left-Rep', 'dim=7', 'dim=10', 'dim=12', 'dim=13', 'dim=14', 'dim=18']\n",
    "\n",
    "# Perform ANOVA across the models\n",
    "model_columns = ['Left-Rep', 'dim=7', 'dim=10', 'dim=12', 'dim=13', 'dim=14', 'dim=18']\n",
    "anova_result = stats.f_oneway(*(similarities_table[col] for col in model_columns))\n",
    "\n",
    "print(f\"ANOVA result across models: F={anova_result.statistic:.4f}, p={anova_result.pvalue:.4f}\")\n",
    "\n",
    "# Check if there's a significant difference\n",
    "if anova_result.pvalue < 0.05:\n",
    "    print(\"There is a statistically significant difference between the models.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference between the models.\")\n",
    "\n",
    "# Preparing data for post-hoc Tukey's HSD if needed\n",
    "if anova_result.pvalue < 0.05:\n",
    "    data = np.concatenate([similarities_table[col].values for col in model_columns])\n",
    "    labels = np.concatenate([[col] * len(similarities_table) for col in model_columns])\n",
    "\n",
    "    # Performing Tukey's HSD\n",
    "    tukey_result = pairwise_tukeyhsd(data, labels, alpha=0.05)\n",
    "    print(tukey_result)\n",
    "\n",
    "    # Identifying the best model based on mean similarity scores\n",
    "    mean_similarities = similarities_table[model_columns].mean().sort_values(ascending=False)\n",
    "    best_model = mean_similarities.idxmax()\n",
    "    best_mean_similarity = mean_similarities.max()\n",
    "\n",
    "    print(\n",
    "        f\"\\nThe best model based on mean cosine similarity is {best_model} with a score of {best_mean_similarity:.4f}.\")\n",
    "\n",
    "    # Comparing \"Left-Right\" to the best model using a t-test\n",
    "    t_stat, p_value = stats.ttest_ind(similarities_table['Left-Right'], similarities_table[best_model])\n",
    "    print(f\"\\nT-test comparing 'Left-Right' to '{best_model}': t={t_stat:.4f}, p={p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"There is a statistically significant difference between 'Left-Right' and the best model.\")\n",
    "    else:\n",
    "        print(\"There is no statistically significant difference between 'Left-Right' and the best model.\")\n",
    "else:\n",
    "    print(\"\\nSince there's no significant difference between models, we skip further analysis.\")\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(similarities_table['Left-Right'], similarities_table['dim=13'])\n",
    "print(f\"\\nT-test comparing 'Left-Right' to 'dim=13': t={t_stat:.4f}, p={p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference between 'Left-Right' and the best model.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference between 'Left-Right' and the best model.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T00:04:12.686170391Z",
     "start_time": "2024-03-08T00:04:12.261798704Z"
    }
   },
   "id": "19b60ba975d35504",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ca0188e5583c0ab3",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
