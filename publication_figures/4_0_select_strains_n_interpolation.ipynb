{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438afc03acbf6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = \"/home/xavier/Documents/DAE_project\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1440e3d9be3ae9",
   "metadata": {},
   "source": [
    "# Image selection\n",
    "This is the notebook to select and crop images for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T03:50:25.445943097Z",
     "start_time": "2026-02-01T03:50:25.434929890Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ad046d029981f",
   "metadata": {},
   "source": [
    "# Get the annotation of movie folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad952e84ee718cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T03:50:26.973176128Z",
     "start_time": "2026-02-01T03:50:26.759501364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Scope</th>\n",
       "      <th>Directory</th>\n",
       "      <th>Mutant #</th>\n",
       "      <th>Reference</th>\n",
       "      <th>QC</th>\n",
       "      <th>Movies</th>\n",
       "      <th>FinalFramePhenotype</th>\n",
       "      <th>Source</th>\n",
       "      <th>QC- Error Code:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>636</td>\n",
       "      <td>38</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>DK425</td>\n",
       "      <td>https://journals.asm.org/doi/abs/10.1128/jb.17...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Weak</td>\n",
       "      <td>LJ Shimkets, D Kaiser - Journal of Bacteriolog...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>636</td>\n",
       "      <td>37</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>DK425</td>\n",
       "      <td>https://journals.asm.org/doi/abs/10.1128/jb.17...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Weak</td>\n",
       "      <td>LJ Shimkets, D Kaiser - Journal of Bacteriolog...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>636</td>\n",
       "      <td>39</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>DK425</td>\n",
       "      <td>https://journals.asm.org/doi/abs/10.1128/jb.17...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Weak</td>\n",
       "      <td>LJ Shimkets, D Kaiser - Journal of Bacteriolog...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>672</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>DK1253</td>\n",
       "      <td>https://www.pnas.org/doi/epdf/10.1073/pnas.76....</td>\n",
       "      <td>Pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Weak</td>\n",
       "      <td>L Kroos, A Kuspa, D Kaiser - Journal of bacter...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>672</td>\n",
       "      <td>2</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>DK1253</td>\n",
       "      <td>https://www.pnas.org/doi/epdf/10.1073/pnas.76....</td>\n",
       "      <td>Pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Weak</td>\n",
       "      <td>L Kroos, A Kuspa, D Kaiser - Journal of bacter...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>287</td>\n",
       "      <td>31</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>10536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Weak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>287</td>\n",
       "      <td>32</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>10536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pass</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Weak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>526</td>\n",
       "      <td>30</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>DK5257</td>\n",
       "      <td>https://genesdev.cshlp.org/content/1/8/840.short</td>\n",
       "      <td>?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Spots</td>\n",
       "      <td>L Kroos, D Kaiser - Genes &amp; development, 1987 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>526</td>\n",
       "      <td>29</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>DK5257</td>\n",
       "      <td>https://genesdev.cshlp.org/content/1/8/840.short</td>\n",
       "      <td>?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Spots</td>\n",
       "      <td>L Kroos, D Kaiser - Genes &amp; development, 1987 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>526</td>\n",
       "      <td>27</td>\n",
       "      <td>/home/xavier/Documents/DAE_project/dataset/Roy...</td>\n",
       "      <td>DK5257</td>\n",
       "      <td>https://genesdev.cshlp.org/content/1/8/840.short</td>\n",
       "      <td>?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Spots</td>\n",
       "      <td>L Kroos, D Kaiser - Genes &amp; development, 1987 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Run  Scope                                          Directory Mutant #  \\\n",
       "0    636     38  /home/xavier/Documents/DAE_project/dataset/Roy...    DK425   \n",
       "1    636     37  /home/xavier/Documents/DAE_project/dataset/Roy...    DK425   \n",
       "2    636     39  /home/xavier/Documents/DAE_project/dataset/Roy...    DK425   \n",
       "3    672      3  /home/xavier/Documents/DAE_project/dataset/Roy...   DK1253   \n",
       "4    672      2  /home/xavier/Documents/DAE_project/dataset/Roy...   DK1253   \n",
       "..   ...    ...                                                ...      ...   \n",
       "932  287     31  /home/xavier/Documents/DAE_project/dataset/Roy...    10536   \n",
       "933  287     32  /home/xavier/Documents/DAE_project/dataset/Roy...    10536   \n",
       "934  526     30  /home/xavier/Documents/DAE_project/dataset/Roy...   DK5257   \n",
       "935  526     29  /home/xavier/Documents/DAE_project/dataset/Roy...   DK5257   \n",
       "936  526     27  /home/xavier/Documents/DAE_project/dataset/Roy...   DK5257   \n",
       "\n",
       "                                             Reference    QC  Movies  \\\n",
       "0    https://journals.asm.org/doi/abs/10.1128/jb.17...  Pass     3.0   \n",
       "1    https://journals.asm.org/doi/abs/10.1128/jb.17...  Pass     3.0   \n",
       "2    https://journals.asm.org/doi/abs/10.1128/jb.17...  Pass     3.0   \n",
       "3    https://www.pnas.org/doi/epdf/10.1073/pnas.76....  Pass     3.0   \n",
       "4    https://www.pnas.org/doi/epdf/10.1073/pnas.76....  Pass     3.0   \n",
       "..                                                 ...   ...     ...   \n",
       "932                                                NaN  Pass     3.0   \n",
       "933                                                NaN  Pass     3.0   \n",
       "934   https://genesdev.cshlp.org/content/1/8/840.short     ?     3.0   \n",
       "935   https://genesdev.cshlp.org/content/1/8/840.short     ?     3.0   \n",
       "936   https://genesdev.cshlp.org/content/1/8/840.short     ?     3.0   \n",
       "\n",
       "    FinalFramePhenotype                                             Source  \\\n",
       "0                  Weak  LJ Shimkets, D Kaiser - Journal of Bacteriolog...   \n",
       "1                  Weak  LJ Shimkets, D Kaiser - Journal of Bacteriolog...   \n",
       "2                  Weak  LJ Shimkets, D Kaiser - Journal of Bacteriolog...   \n",
       "3                  Weak  L Kroos, A Kuspa, D Kaiser - Journal of bacter...   \n",
       "4                  Weak  L Kroos, A Kuspa, D Kaiser - Journal of bacter...   \n",
       "..                  ...                                                ...   \n",
       "932                Weak                                                NaN   \n",
       "933                Weak                                                NaN   \n",
       "934               Spots  L Kroos, D Kaiser - Genes & development, 1987 ...   \n",
       "935               Spots  L Kroos, D Kaiser - Genes & development, 1987 ...   \n",
       "936               Spots  L Kroos, D Kaiser - Genes & development, 1987 ...   \n",
       "\n",
       "    QC- Error Code:  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "..              ...  \n",
       "932             NaN  \n",
       "933             NaN  \n",
       "934             NaN  \n",
       "935             NaN  \n",
       "936             NaN  \n",
       "\n",
       "[937 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the root directory containing the datasets\n",
    "ROOT_DIR = f\"{WORKING_DIR}/dataset/Roy_training/images\"\n",
    "# Define the path to the label file\n",
    "LABEL_DIR = f\"{WORKING_DIR}/dataset/Roy_training/Caro_3d_9.7.22_2.20_new.xlsx\"\n",
    "container = []\n",
    "\n",
    "# Loop through each item in the root directory\n",
    "for strain in os.listdir(ROOT_DIR):\n",
    "    run_id = int(strain.split(\"Run\")[1])  # Extract run ID from the folder name\n",
    "\n",
    "    # Construct the path to the current strain folder\n",
    "    strain_path = os.path.join(ROOT_DIR, strain)\n",
    "\n",
    "    # Loop through each scope folder within the current strain folder\n",
    "    for scope in os.listdir(strain_path):\n",
    "        # Construct the full path to the current scope folder\n",
    "        scope_path = os.path.join(strain_path, scope)\n",
    "\n",
    "        # Append the run ID, scope ID, and the scope folder path to the container list\n",
    "        container.append([run_id, int(scope[-2:]), scope_path])\n",
    "\n",
    "# Convert the container list to a DataFrame\n",
    "container = pd.DataFrame(container, columns=[\"Run\", \"Scope\", \"Directory\"])\n",
    "\n",
    "# Load the label file into a DataFrame\n",
    "label_df = pd.read_excel(LABEL_DIR)\n",
    "\n",
    "# Merge the container DataFrame with the label DataFrame based on the \"Run\" column\n",
    "# This assumes 'Run' is a column in both DataFrames. If 'Run' is an index in label_df,\n",
    "# you might first need to reset the index using label_df.reset_index().\n",
    "annotated_container = pd.merge(container, label_df, on=\"Run\")\n",
    "\n",
    "# If the 'Run' column is not named the same in both DataFrames, you would specify\n",
    "# left_on and right_on instead of just on, e.g.:\n",
    "# annotated_container = pd.merge(container, label_df, left_on=\"Run\", right_on=\"YourRunColumnNameInLabelDF\")\n",
    "annotated_container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b99023e8fc6f45",
   "metadata": {},
   "source": [
    "# Get selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ce94b9f18e15bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T03:50:29.859846661Z",
     "start_time": "2026-02-01T03:50:29.788405527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1441 not found in /home/xavier/Documents/DAE_project/dataset/Roy_training/images/CS2_44_1622_1%agar_Run0320/Scope23\n",
      "1441 not found in /home/xavier/Documents/DAE_project/dataset/Roy_training/images/CS2_44_1622_1%agar_Run0320/Scope22\n",
      "1441 not found in /home/xavier/Documents/DAE_project/dataset/Roy_training/images/CS2_44_1622_1%agar_Run0320/Scope24\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = f\"{WORKING_DIR}/images/figure4/original\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "SELECTED_FRAMES = [1441]  # [1, 289, 577, 721, 865, 1153, 1441]\n",
    "selected_strains = [1622, 2232, 3557, 5205, 8615, 1224]  #[8615, 5205, 2232, 3557, 4398, 3186, 1218, 1253, 1224, 5206]\n",
    "selected_strains = selected_strains + [f\"DK{str(tmp)}\" for tmp in selected_strains]\n",
    "# filtered_df = annotated_container[annotated_container[\"Mutant #\"].isin(selected_strains)]\n",
    "selected_runs = [432]  # [351, 684, 407, 254, 340, 282, 607, 379, 574, 316, 485, 432, 584, 589]\n",
    "# filtered_df = annotated_container[annotated_container[\"Run\"].isin(selected_runs)]\n",
    "filtered_df = annotated_container[annotated_container[\"Mutant #\"].isin(selected_strains)]\n",
    "for _, row in filtered_df.iterrows():\n",
    "    images = os.listdir(row[\"Directory\"])\n",
    "    name_format = images[0][:-8] + \"%04d.jpg\"\n",
    "    for selected_frame in SELECTED_FRAMES:\n",
    "        try:\n",
    "            shutil.copy(os.path.join(row[\"Directory\"], name_format % selected_frame),\n",
    "                        os.path.join(OUT_DIR, f\"{row['Mutant #']}_\" + name_format % selected_frame))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{selected_frame} not found in {row['Directory']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e4caf21a8e7ac",
   "metadata": {},
   "source": [
    "# Helper functions for crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eee2cf74ed5b2ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T03:50:31.832948076Z",
     "start_time": "2026-02-01T03:50:31.730167583Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "\n",
    "def resize_crop(img_name, resize_by=1., resolution=512, brightness_norm=True, brightness_mean=107):\n",
    "    if isinstance(img_name, str):\n",
    "        img = cv2.imread(img_name, cv2.IMREAD_UNCHANGED)\n",
    "    else:\n",
    "        img = img_name\n",
    "    if img.dtype != np.uint8:\n",
    "        img = np.uint8(img / 256)\n",
    "    if img.ndim == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_shape = img.shape\n",
    "    resize_shape = np.array([img_shape[1] * resize_by, img_shape[0] * resize_by], dtype=int)\n",
    "    if resize_by != 1:\n",
    "        img = cv2.resize(img, resize_shape, cv2.INTER_LANCZOS4)\n",
    "    img = img[(resize_shape[1] - resolution) // 2:(resize_shape[1] + resolution) // 2,\n",
    "    (resize_shape[0] - resolution) // 2:(resize_shape[0] + resolution) // 2]\n",
    "    if brightness_norm:\n",
    "        obj_v = np.mean(img)\n",
    "        value = brightness_mean - obj_v\n",
    "        img = cv2.add(img, value)\n",
    "    return img\n",
    "\n",
    "\n",
    "def add_scale_bar(ax, length_px, length_unit, bar_height=5, bar_color='white', text_color='white', font_size=10,\n",
    "                  location=(0.05, 0.05)):\n",
    "    \"\"\"\n",
    "    Adds a scale bar to a Matplotlib axis.\n",
    "\n",
    "    :param ax: Matplotlib axis\n",
    "    :param length_px: Length of the scale bar in pixels\n",
    "    :param length_unit: Unit of length (e.g., '1 mm')\n",
    "    :param bar_height: Height of the scale bar in pixels\n",
    "    :param bar_color: Color of the scale bar\n",
    "    :param text_color: Color of the text\n",
    "    :param font_size: Font size of the text\n",
    "    :param location: Tuple indicating the relative position of the scale bar in the axes (from 0 to 1)\n",
    "    \"\"\"\n",
    "    # Calculate scale bar position\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    x_length = xlim[1] - xlim[0]\n",
    "    y_length = ylim[1] - ylim[0]\n",
    "\n",
    "    # Position in axes coordinates\n",
    "    x = xlim[0] + x_length * location[0]\n",
    "    y = ylim[0] + y_length * location[1]\n",
    "\n",
    "    # Add rectangle for scale bar\n",
    "    rectangle = plt.Rectangle((x, y), length_px, bar_height, color=bar_color)\n",
    "    ax.add_patch(rectangle)\n",
    "\n",
    "    # Add text\n",
    "    ax.text(x + length_px / 2, y + bar_height * 2, length_unit, color=text_color, ha='center', va='bottom',\n",
    "            fontsize=font_size)\n",
    "\n",
    "    # Plot image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    add_scale_bar(ax, length_px=20, length_unit='1 mm')  # Adjust length_px and length_unit based on your needs\n",
    "\n",
    "    # Hide axes\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01cbc7517c7351",
   "metadata": {},
   "source": [
    "# Crop images and reconstruct\n",
    "We changed the noise_const to make the consistent aggregate positions for each generation with the same noise seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acba54dd6db3cb45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T03:50:37.983756930Z",
     "start_time": "2026-02-01T03:50:34.447638462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
     ]
    }
   ],
   "source": [
    "SOURCE = f\"{WORKING_DIR}/images/figure4/original\"\n",
    "OUT_DIR = f\"{WORKING_DIR}/images/figure4/cropped\"\n",
    "# MODEL_DIR = \"/home/xavier/PycharmProjects/training-runs/e13/00001-stylegan2-trainingset2-gpus4-batch96-gamma10/network-snapshot-000756.pkl\"\n",
    "MODEL_DIR = f\"{WORKING_DIR}/models/network-snapshot-003024-patched.pkl\"\n",
    "\n",
    "import dnnlib\n",
    "import legacy\n",
    "import torch\n",
    "from training.networks_stylegan2 import SynthesisLayer\n",
    "\n",
    "os.environ['CC'] = \"/usr/bin/gcc-9\"\n",
    "os.environ['CXX'] = \"/usr/bin/g++-9\"\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "def reset_noise_const(G, seed):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    for block in G.synthesis.children():\n",
    "        for layer in block.children():\n",
    "            if layer.__class__.__name__ == \"SynthesisLayer\":  #isinstance(layer, SynthesisLayer):\n",
    "                resolution = layer.resolution\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    layer.noise_const.copy_(torch.randn([resolution, resolution]))\n",
    "\n",
    "\n",
    "with dnnlib.util.open_url(MODEL_DIR) as fp:\n",
    "    models = legacy.load_network_pkl(fp)\n",
    "    E = models['E_ema'].to(device)\n",
    "    G = models['G_ema'].to(device)  # type: ignore\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "for image in os.listdir(SOURCE):\n",
    "    processed = resize_crop(os.path.join(SOURCE, image))\n",
    "    cv2.imwrite(os.path.join(OUT_DIR, image), processed)\n",
    "    img = np.array(processed) / 127.5 - 1\n",
    "    img = torch.Tensor(img).to(device)\n",
    "    img = img[None, None, :, :]\n",
    "\n",
    "    z, logvar = E.mu_var(img, None)\n",
    "    reset_noise_const(G, 0)\n",
    "    synth_image = G(z, None, noise_mode=\"const\")\n",
    "    synth_image = (synth_image + 1) * 127.5\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()[0, :, :, 0]\n",
    "    cv2.imwrite(os.path.join(OUT_DIR, \"const1_\" + image), synth_image)\n",
    "    synth_image2 = G(z, None, noise_mode=\"random\")\n",
    "    synth_image2 = (synth_image2 + 1) * 127.5\n",
    "    synth_image2 = synth_image2.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()[0, :, :, 0]\n",
    "    cv2.imwrite(os.path.join(OUT_DIR, \"random1_\" + image), synth_image2)\n",
    "    reset_noise_const(G, 20)\n",
    "    synth_image = G(z, None, noise_mode=\"const\")\n",
    "    synth_image = (synth_image + 1) * 127.5\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()[0, :, :, 0]\n",
    "    cv2.imwrite(os.path.join(OUT_DIR, \"const2_\" + image), synth_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c685c020374650",
   "metadata": {},
   "source": [
    "# Multiple linear interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9060e1564431221d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T05:50:18.736502Z",
     "start_time": "2024-12-18T05:50:18.528982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output images already constructed in /home/xavier/Documents/DAE_project/images/figure4/interpolation/2232-1622/linear_interpolations (1802 files). Skipping.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Linear interpolation with replicate generation using reset_noise_const\n",
    "INTERP_OUT_DIR = f\"{WORKING_DIR}/images/figure4/interpolation/2232-1622/linear_interpolations\"\n",
    "os.makedirs(INTERP_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Check if output images already exist\n",
    "inner_num = 6\n",
    "n_replicas = 300\n",
    "expected_count = inner_num * n_replicas + 2  # +2 for original images (image0.tif and image5.tif)\n",
    "existing_files = len([f for f in os.listdir(INTERP_OUT_DIR) if f.endswith('.tif')])\n",
    "\n",
    "if existing_files >= expected_count:\n",
    "    print(f\"Output images already constructed in {INTERP_OUT_DIR} ({existing_files} files). Skipping.\")\n",
    "else:\n",
    "    # source images\n",
    "    img1_dir = f\"{WORKING_DIR}/images/figure4/original/DK2232_Run0351_scope40-00_1441.jpg\"\n",
    "    img2_dir = f\"{WORKING_DIR}/images/figure4/original/1622_Run0432_scope8-00_1441.jpg\"\n",
    "    img1 = resize_crop(img1_dir)\n",
    "    img2 = resize_crop(img2_dir)\n",
    "\n",
    "    cv2.imwrite(os.path.join(INTERP_OUT_DIR, f\"image0.tif\"), img1)\n",
    "    cv2.imwrite(os.path.join(INTERP_OUT_DIR, f\"image{inner_num-1}.tif\"), img2)\n",
    "    # number of replicate noise constants (changeable)\n",
    "\n",
    "    reset_noise_const(G, 4)\n",
    "    imgs = torch.Tensor([img1, img2]).to(device).to(torch.float32) / 127.5 - 1\n",
    "    imgs = imgs[:, None, :, :]\n",
    "    z, _ = E.mu_var(imgs, None)\n",
    "    z_orint = z[1, :] - z[0, :]\n",
    "\n",
    "    step_size = 1 / (inner_num - 1)\n",
    "    z_interpolate = torch.cat([(z[0, :] + z_orint * (i * step_size))[None, :] for i in range(inner_num)])\n",
    "    new_zs = z_interpolate\n",
    "\n",
    "    # Generate replicate images by resetting noise_const for each seed\n",
    "    for r in range(n_replicas):\n",
    "        reset_noise_const(G, r)\n",
    "        synth_batch = G(new_zs, None, noise_mode='const')\n",
    "        synth_batch = (synth_batch + 1) * 127.5\n",
    "        synth_batch = synth_batch.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()[:, :, :, 0]\n",
    "        for i in range(inner_num):\n",
    "            out_path = os.path.join(INTERP_OUT_DIR, f\"image{i}_rep{r}.tif\")\n",
    "            cv2.imwrite(out_path, synth_batch[i])\n",
    "\n",
    "    print(f\"Saved {n_replicas} replicates for {inner_num} interpolated steps to: {INTERP_OUT_DIR}\")\n",
    "\n",
    "# Optionally also save a single-ws synthesis (one replicate) for reference\n",
    "# reset_noise_const(G, 0)\n",
    "# ws = G.mapping(z, None)\n",
    "# ws_interpolate = torch.cat([(ws[0, :, :] + (ws[1, :, :] - ws[0, :, :]) * (i * step_size))[None, :] for i in range(inner_num)])\n",
    "# synth_ws = G.synthesis(ws_interpolate, noise_mode='const')\n",
    "# synth_ws = (synth_ws + 1) * 127.5\n",
    "# synth_ws = synth_ws.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()[:, :, :, 0]\n",
    "# for i in range(inner_num):\n",
    "#     cv2.imwrite(os.path.join(INTERP_OUT_DIR, f\"ws_image{i}.tif\"), synth_ws[i])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c936bcd",
   "metadata": {},
   "source": [
    "# Get human defined features\n",
    "* Run image segmentation in MATLAB\n",
    "  20100604-STIC/Agg_feat_ext_loop.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a2d6929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan of 300 replicates. Target: 100 valid sets.\n",
      "  -> Collected 10 valid replicates...\n",
      "  -> Collected 20 valid replicates...\n",
      "  -> Collected 30 valid replicates...\n",
      "  -> Collected 40 valid replicates...\n",
      "  -> Collected 50 valid replicates...\n",
      "  -> Collected 60 valid replicates...\n",
      "  -> Collected 70 valid replicates...\n",
      "  -> Collected 80 valid replicates...\n",
      "  -> Collected 90 valid replicates...\n",
      "  -> Collected 100 valid replicates...\n",
      "\n",
      "Target reached! Collected 100 valid replicates.\n",
      "\n",
      "Processing complete.\n",
      "Total valid replicates: 100\n",
      "Total rows generated: 600 (Should be 600)\n",
      "Data saved to: /home/xavier/Documents/DAE_project/images/figure4/interpolation/2232-1622/analysises/selected_replicates_features.csv\n",
      "\n",
      "First 5 rows of data:\n",
      "   Replicate_ID  Image_Step  area_mean_um2  area_min_um2  area_max_um2  \\\n",
      "0             1           0   10828.665535   1392.275911  27685.319111   \n",
      "1             1           1    8375.164718   1153.433600  23619.174400   \n",
      "2             1           2    6874.446332   1555.387733  20132.659200   \n",
      "3             1           3    6198.473299   2694.257778  10153.710933   \n",
      "4             1           4    5537.063822   2047.635911   9885.741511   \n",
      "\n",
      "   area_median_um2  area_std_um2   area_sum_um2  perimeter_mean_um  \\\n",
      "0     10544.014222   7656.617185  119115.320889         382.454513   \n",
      "1      8024.519111   6506.272219  108877.141333         339.273114   \n",
      "2      5592.405333   4445.197066   89367.802311         308.674129   \n",
      "3      5426.380800   2434.803352   80580.152889         292.391953   \n",
      "4      4316.637867   2612.670769   71981.829689         271.709896   \n",
      "\n",
      "   perimeter_min_um  ...  eccentricity_min  eccentricity_max  \\\n",
      "0        155.913112  ...          0.413272          0.888590   \n",
      "1        129.363609  ...          0.336525          0.904200   \n",
      "2        168.152596  ...          0.286445          0.893031   \n",
      "3        196.358526  ...          0.441794          0.737602   \n",
      "4        174.222099  ...          0.297522          0.747616   \n",
      "\n",
      "   eccentricity_median  eccentricity_std  axis_length_mean_um  \\\n",
      "0             0.706468          0.135997           136.630333   \n",
      "1             0.754119          0.149376           122.653516   \n",
      "2             0.642744          0.190452           107.497578   \n",
      "3             0.587169          0.087319            98.133377   \n",
      "4             0.557260          0.159082            91.554669   \n",
      "\n",
      "   axis_length_min_um  axis_length_max_um  axis_length_median_um  \\\n",
      "0           62.554120          271.183061             128.501176   \n",
      "1           47.359456          261.450172             113.313647   \n",
      "2           67.699943          241.323454              96.460007   \n",
      "3           64.790692          136.177986              90.304951   \n",
      "4           56.428976          127.649890              77.342259   \n",
      "\n",
      "   axis_length_std_um  count  \n",
      "0           61.187030     11  \n",
      "1           63.321009     13  \n",
      "2           43.723111     13  \n",
      "3           23.271612     13  \n",
      "4           26.299947     13  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import measure\n",
    "from skimage.filters import threshold_otsu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# Configuration & Directories\n",
    "# ==========================================\n",
    "SEG_DIR = f\"{WORKING_DIR}/images/figure4/interpolation/2232-1622/segmenteds\"\n",
    "OUT_DIR = f\"{WORKING_DIR}/images/figure4/interpolation/2232-1622/analysises\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Analysis Parameters\n",
    "TOTAL_REPLICATES_TO_CHECK = 300  # Total pool of noise codes to scan\n",
    "TARGET_VALID_REPLICATES = 100    # Stop after finding this many good replicates\n",
    "IMAGES_PER_REPLICATE = 6         # Number of interpolation steps (0 to 5)\n",
    "\n",
    "# Image scaling params\n",
    "original_size = [512, 512]\n",
    "final_size = [600, 600]\n",
    "ratio = 600 / 512\n",
    "\n",
    "# Quality thresholds (scaled for 600x600 resolution)\n",
    "AREA_MIN_SCALED = 100 / (1600/1296)**2 * (600/512)**2  # ~39.5\n",
    "AREA_MAX_SCALED = 100000 / (1600/1296)**2 * (600/512)**2  # ~39500\n",
    "ECCENTRICITY_THRESHOLD = 0.8\n",
    "AXIS_RATIO_THRESHOLD = 3.0\n",
    "\n",
    "# ==========================================\n",
    "# Helper Functions\n",
    "# ==========================================\n",
    "\n",
    "def compute_stats(region_props):\n",
    "    \"\"\"\n",
    "    Computes statistical features for a list of region properties.\n",
    "    Returns a dictionary of tuples (mean, min, max, median, sum/std, std).\n",
    "    \"\"\"\n",
    "    if not region_props:\n",
    "        # Return zeros if no regions found\n",
    "        return {\n",
    "            \"area\": (0, 0, 0, 0, 0, 0),\n",
    "            \"perimeter\": (0, 0, 0, 0, 0),\n",
    "            \"eccentricity\": (0, 0, 0, 0, 0),\n",
    "            \"axis_length\": (0, 0, 0, 0, 0),\n",
    "            \"count\": 0\n",
    "        }\n",
    "\n",
    "    # Extract raw lists\n",
    "    areas = [p.area for p in region_props]\n",
    "    perimeters = [p.perimeter for p in region_props]\n",
    "    eccentricities = [p.eccentricity for p in region_props]\n",
    "    axis_lengths = [p.major_axis_length for p in region_props]\n",
    "\n",
    "    # Calculate stats\n",
    "    # Note: Area is scaled back by ratio^2 and multiplied by 4 (for pixel_size=2um -> 4um^2)\n",
    "    # Lengths are scaled back by ratio and multiplied by 2 (for pixel_size=2um)\n",
    "    \n",
    "    stats = {\n",
    "        \"area\": (\n",
    "            np.mean(areas) / (ratio ** 2) * 4, \n",
    "            np.min(areas) / (ratio ** 2) * 4, \n",
    "            np.max(areas) / (ratio ** 2) * 4,\n",
    "            np.median(areas) / (ratio ** 2) * 4, \n",
    "            np.std(areas) / (ratio ** 2) * 4,\n",
    "            np.sum(areas) / (ratio ** 2) * 4\n",
    "        ),\n",
    "        \"perimeter\": (\n",
    "            np.mean(perimeters) / ratio * 2, \n",
    "            np.min(perimeters) / ratio * 2, \n",
    "            np.max(perimeters) / ratio * 2,\n",
    "            np.median(perimeters) / ratio * 2, \n",
    "            np.std(perimeters) / ratio * 2\n",
    "        ),\n",
    "        \"eccentricity\": (\n",
    "            np.mean(eccentricities), \n",
    "            np.min(eccentricities), \n",
    "            np.max(eccentricities), \n",
    "            np.median(eccentricities),\n",
    "            np.std(eccentricities)\n",
    "        ),\n",
    "        \"axis_length\": (\n",
    "            np.mean(axis_lengths) / ratio * 2, \n",
    "            np.min(axis_lengths) / ratio * 2,\n",
    "            np.max(axis_lengths) / ratio * 2,\n",
    "            np.median(axis_lengths) / ratio * 2,\n",
    "            np.std(axis_lengths) / ratio * 2\n",
    "        ),\n",
    "        \"count\": len(region_props)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def check_quality(props):\n",
    "    \"\"\"\n",
    "    Checks if the segmentation quality is acceptable for a single image.\n",
    "    Returns True if passed, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(props) == 0:\n",
    "        return False\n",
    "\n",
    "    too_many_small = 0\n",
    "    too_big = False\n",
    "    high_ecc_count = 0\n",
    "    bad_axis_ratio = 0\n",
    "    \n",
    "    for prop in props:\n",
    "        if prop.area < AREA_MIN_SCALED:\n",
    "            too_many_small += 1\n",
    "        elif prop.area > AREA_MAX_SCALED:\n",
    "            too_big = True\n",
    "        \n",
    "        if prop.eccentricity > ECCENTRICITY_THRESHOLD:\n",
    "            high_ecc_count += 1\n",
    "        \n",
    "        # Avoid division by zero for minor axis\n",
    "        if prop.minor_axis_length > 0:\n",
    "            if prop.major_axis_length > AXIS_RATIO_THRESHOLD * prop.minor_axis_length:\n",
    "                bad_axis_ratio += 1\n",
    "        else:\n",
    "             bad_axis_ratio += 1\n",
    "\n",
    "    # Quality Criteria\n",
    "    if too_many_small > len(props) // 2:\n",
    "        return False\n",
    "    if too_big:\n",
    "        return False\n",
    "    if high_ecc_count > len(props) // 2:\n",
    "        return False\n",
    "    if bad_axis_ratio > 0:\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "# ==========================================\n",
    "# Main Processing Loop\n",
    "# ==========================================\n",
    "\n",
    "# Define column names for the output DataFrame\n",
    "feature_names = [\n",
    "    \"area_mean_um2\", \"area_min_um2\", \"area_max_um2\", \"area_median_um2\", \"area_std_um2\", \"area_sum_um2\",\n",
    "    \"perimeter_mean_um\", \"perimeter_min_um\", \"perimeter_max_um\", \"perimeter_median_um\", \"perimeter_std_um\",\n",
    "    \"eccentricity_mean\", \"eccentricity_min\", \"eccentricity_max\", \"eccentricity_median\", \"eccentricity_std\",\n",
    "    \"axis_length_mean_um\", \"axis_length_min_um\", \"axis_length_max_um\", \"axis_length_median_um\", \"axis_length_std_um\",\n",
    "    \"count\"\n",
    "]\n",
    "cols = [\"Replicate_ID\", \"Image_Step\"] + feature_names\n",
    "\n",
    "valid_data = []\n",
    "collected_count = 0\n",
    "\n",
    "print(f\"Starting scan of {TOTAL_REPLICATES_TO_CHECK} replicates. Target: {TARGET_VALID_REPLICATES} valid sets.\")\n",
    "\n",
    "# Iterate through replicates (noise codes)\n",
    "for rep_idx in range(TOTAL_REPLICATES_TO_CHECK):\n",
    "    if collected_count >= TARGET_VALID_REPLICATES:\n",
    "        print(f\"\\nTarget reached! Collected {collected_count} valid replicates.\")\n",
    "        break\n",
    "\n",
    "    temp_replicate_rows = []\n",
    "    replicate_valid = True\n",
    "    \n",
    "    # Check all 6 interpolation steps for this replicate\n",
    "    for img_idx in range(IMAGES_PER_REPLICATE):\n",
    "        # Construct filepath: e.g., image0_rep0.tif, image1_rep0.tif, etc.\n",
    "        filename = f\"image{img_idx}_rep{rep_idx}.tif\"\n",
    "        fpath = os.path.join(SEG_DIR, filename)\n",
    "        \n",
    "        if not os.path.exists(fpath):\n",
    "            # Try alternative naming if necessary, or just fail\n",
    "            replicate_valid = False\n",
    "            # print(f\"  [Rep {rep_idx}] Missing file: {filename}\") # Optional logging\n",
    "            break\n",
    "            \n",
    "        # Load Image\n",
    "        bw_img = cv2.imread(fpath, cv2.IMREAD_UNCHANGED)\n",
    "        if bw_img is None:\n",
    "            replicate_valid = False\n",
    "            break\n",
    "\n",
    "        # Binarize\n",
    "        try:\n",
    "            thresh = threshold_otsu(bw_img)\n",
    "        except Exception:\n",
    "            thresh = np.mean(bw_img)\n",
    "            \n",
    "        bw = bw_img > thresh\n",
    "        labeled_bw = measure.label(bw)\n",
    "        props = measure.regionprops(labeled_bw)\n",
    "\n",
    "        # 1. Run Quality Check\n",
    "        if not check_quality(props):\n",
    "            replicate_valid = False\n",
    "            # print(f\"  [Rep {rep_idx}] Quality check failed on {filename}\") # Optional logging\n",
    "            break\n",
    "            \n",
    "        # 2. Compute Features (only if QC passed)\n",
    "        stats = compute_stats(props)\n",
    "        \n",
    "        # Flatten features into a list\n",
    "        row_features = []\n",
    "        row_features.extend(stats[\"area\"])\n",
    "        row_features.extend(stats[\"perimeter\"])\n",
    "        row_features.extend(stats[\"eccentricity\"])\n",
    "        row_features.extend(stats[\"axis_length\"])\n",
    "        row_features.append(stats[\"count\"])\n",
    "        \n",
    "        # Add metadata (Replicate ID and Image Step)\n",
    "        full_row = [rep_idx, img_idx] + row_features\n",
    "        temp_replicate_rows.append(full_row)\n",
    "\n",
    "    # If all images for this replicate passed\n",
    "    if replicate_valid and len(temp_replicate_rows) == IMAGES_PER_REPLICATE:\n",
    "        valid_data.extend(temp_replicate_rows)\n",
    "        collected_count += 1\n",
    "        if collected_count % 10 == 0:\n",
    "            print(f\"  -> Collected {collected_count} valid replicates...\")\n",
    "    else:\n",
    "        pass \n",
    "        # Optionally log why it failed (e.g., \"Skipped Rep {rep_idx}\")\n",
    "\n",
    "# ==========================================\n",
    "# Save Results\n",
    "# ==========================================\n",
    "\n",
    "if collected_count == 0:\n",
    "    print(\"\\nWarning: No valid replicates found matching criteria.\")\n",
    "else:\n",
    "    df = pd.DataFrame(valid_data, columns=cols)\n",
    "    \n",
    "    # Save the detailed long-format data\n",
    "    out_csv = os.path.join(OUT_DIR, \"selected_replicates_features.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    \n",
    "    print(f\"\\nProcessing complete.\")\n",
    "    print(f\"Total valid replicates: {collected_count}\")\n",
    "    print(f\"Total rows generated: {len(df)} (Should be {collected_count * 6})\")\n",
    "    print(f\"Data saved to: {out_csv}\")\n",
    "    print(\"\\nFirst 5 rows of data:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6f643",
   "metadata": {},
   "source": [
    "# Get feature graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de7d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Replicate Data ---\n",
      "Loaded 600 rows from /home/xavier/Documents/DAE_project/images/figure4/interpolation/2232-1622/analysises/selected_replicates_features.csv\n",
      "--- Step 2: Processing Reference Images (Ground Truth) ---\n",
      "Processing Reference Image: image0.tif\n",
      "Processing Reference Image: image5.tif\n",
      "--- Step 3: Generating Plots ---\n",
      "Computing global y-axis limits for each feature...\n",
      "Saved line plots (PNG and PDF).\n",
      "Saved heatmap to /home/xavier/Documents/DAE_project/images/figure4/interpolation/2232-1622/analysises/Heatmap_Features_Ref_vs_Rec.png and /home/xavier/Documents/DAE_project/images/figure4/interpolation/2232-1622/analysises/Heatmap_Features_Ref_vs_Rec.pdf\n",
      "Saved summary table to /home/xavier/Documents/DAE_project/images/figure4/interpolation/2232-1622/analysises/summary_comparison_table.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage import measure\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.stats import t\n",
    "\n",
    "# ==========================================\n",
    "# Configuration\n",
    "# ==========================================\n",
    "\n",
    "# Use the same directory settings as the extraction script\n",
    "SEG_DIR = f\"{WORKING_DIR}/images/figure4/interpolation/2232-1622/segmenteds\"\n",
    "OUT_DIR = f\"{WORKING_DIR}/images/figure4/interpolation/2232-1622/analysises\"\n",
    "INPUT_CSV = os.path.join(OUT_DIR, \"selected_replicates_features.csv\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "IMAGES_PER_REPLICATE = 6  # Steps 0 to 5\n",
    "REF_INDICES = [0, IMAGES_PER_REPLICATE - 1]  # [0, 5]\n",
    "\n",
    "# Image scaling params (Must match extraction script)\n",
    "ratio = 600 / 512\n",
    "\n",
    "# Feature names (Must match extraction script)\n",
    "feature_names = [\n",
    "    \"area_mean_um2\", \"area_min_um2\", \"area_max_um2\", \"area_median_um2\", \"area_std_um2\", \"area_sum_um2\",\n",
    "    \"perimeter_mean_um\", \"perimeter_min_um\", \"perimeter_max_um\", \"perimeter_median_um\", \"perimeter_std_um\",\n",
    "    \"eccentricity_mean\", \"eccentricity_min\", \"eccentricity_max\", \"eccentricity_median\", \"eccentricity_std\",\n",
    "    \"axis_length_mean_um\", \"axis_length_min_um\", \"axis_length_max_um\", \"axis_length_median_um\", \"axis_length_std_um\",\n",
    "    \"count\"\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# Visualization Settings for Adobe Illustrator\n",
    "# ==========================================\n",
    "# Set font type to 42 (TrueType) to ensure text is editable in Illustrator/PDF\n",
    "# instead of being converted to outlines (paths).\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "# Optional: Set a font family that is standard (e.g., Arial or Helvetica)\n",
    "plt.rcParams['font.family'] = 'sans-serif' \n",
    "\n",
    "# ==========================================\n",
    "# Helper Functions\n",
    "# ==========================================\n",
    "\n",
    "def compute_stats(region_props):\n",
    "    \"\"\"\n",
    "    Computes statistical features for a list of region properties.\n",
    "    \"\"\"\n",
    "    if not region_props:\n",
    "        return {\n",
    "            \"area\": (0, 0, 0, 0, 0, 0),\n",
    "            \"perimeter\": (0, 0, 0, 0, 0),\n",
    "            \"eccentricity\": (0, 0, 0, 0, 0),\n",
    "            \"axis_length\": (0, 0, 0, 0, 0),\n",
    "            \"count\": 0\n",
    "        }\n",
    "\n",
    "    areas = [p.area for p in region_props]\n",
    "    perimeters = [p.perimeter for p in region_props]\n",
    "    eccentricities = [p.eccentricity for p in region_props]\n",
    "    axis_lengths = [p.major_axis_length for p in region_props]\n",
    "\n",
    "    stats = {\n",
    "        \"area\": (\n",
    "            np.mean(areas) / (ratio ** 2) * 4, np.min(areas) / (ratio ** 2) * 4, np.max(areas) / (ratio ** 2) * 4,\n",
    "            np.median(areas) / (ratio ** 2) * 4, np.std(areas) / (ratio ** 2) * 4, np.sum(areas) / (ratio ** 2) * 4\n",
    "        ),\n",
    "        \"perimeter\": (\n",
    "            np.mean(perimeters) / ratio * 2, np.min(perimeters) / ratio * 2, np.max(perimeters) / ratio * 2,\n",
    "            np.median(perimeters) / ratio * 2, np.std(perimeters) / ratio * 2\n",
    "        ),\n",
    "        \"eccentricity\": (\n",
    "            np.mean(eccentricities), np.min(eccentricities), np.max(eccentricities), np.median(eccentricities),\n",
    "            np.std(eccentricities)\n",
    "        ),\n",
    "        \"axis_length\": (\n",
    "            np.mean(axis_lengths) / ratio * 2, np.min(axis_lengths) / ratio * 2, np.max(axis_lengths) / ratio * 2,\n",
    "            np.median(axis_lengths) / ratio * 2, np.std(axis_lengths) / ratio * 2\n",
    "        ),\n",
    "        \"count\": len(region_props)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def get_single_image_features(filepath):\n",
    "    \"\"\"\n",
    "    Loads an image, processes it, and returns a dictionary of features mapping to feature_names.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: Reference file not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "    bw_img = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n",
    "    if bw_img is None:\n",
    "        print(f\"Warning: Could not read image: {filepath}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        thresh = threshold_otsu(bw_img)\n",
    "    except Exception:\n",
    "        thresh = np.mean(bw_img)\n",
    "    \n",
    "    bw = bw_img > thresh\n",
    "    labeled_bw = measure.label(bw)\n",
    "    props = measure.regionprops(labeled_bw)\n",
    "    \n",
    "    # Compute raw stats\n",
    "    stats = compute_stats(props)\n",
    "    \n",
    "    # Flatten to list consistent with feature_names\n",
    "    flat_features = []\n",
    "    flat_features.extend(stats[\"area\"])\n",
    "    flat_features.extend(stats[\"perimeter\"])\n",
    "    flat_features.extend(stats[\"eccentricity\"])\n",
    "    flat_features.extend(stats[\"axis_length\"])\n",
    "    flat_features.append(stats[\"count\"])\n",
    "    \n",
    "    return dict(zip(feature_names, flat_features))\n",
    "\n",
    "def ci95(std, n):\n",
    "    \"\"\"Calculates 95% Confidence Interval margin.\"\"\"\n",
    "    std = np.asarray(std, float)\n",
    "    n = np.asarray(n, float)\n",
    "    out = np.full_like(std, np.nan, dtype=float)\n",
    "    # Avoid division by zero or invalid sqrt\n",
    "    mask = np.isfinite(std) & (n > 1)\n",
    "    # t.ppf(0.975, df) is the two-tailed critical value for 95% CI\n",
    "    out[mask] = t.ppf(0.975, df=n[mask]-1) * (std[mask] / np.sqrt(n[mask]))\n",
    "    return out\n",
    "\n",
    "# ==========================================\n",
    "# Main Analysis Logic\n",
    "# ==========================================\n",
    "\n",
    "print(\"--- Step 1: Loading Replicate Data ---\")\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    raise FileNotFoundError(f\"Input file {INPUT_CSV} not found. Please run feature extraction first.\")\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(f\"Loaded {len(df)} rows from {INPUT_CSV}\")\n",
    "\n",
    "# Group by 'Image_Step' to get stats across replicates\n",
    "grouped = df.groupby(\"Image_Step\")[feature_names]\n",
    "df_mean = grouped.mean().reindex(range(IMAGES_PER_REPLICATE))\n",
    "df_std = grouped.std().reindex(range(IMAGES_PER_REPLICATE))\n",
    "df_count = grouped.count().iloc[:, 0].reindex(range(IMAGES_PER_REPLICATE)) # Count of valid replicates per step\n",
    "\n",
    "# Calculate 95% CI margins\n",
    "df_ci = pd.DataFrame(index=df_mean.index, columns=feature_names)\n",
    "for col in feature_names:\n",
    "    df_ci[col] = ci95(df_std[col].values, df_count.values)\n",
    "\n",
    "print(\"--- Step 2: Processing Reference Images (Ground Truth) ---\")\n",
    "ref_data = {}\n",
    "for idx in REF_INDICES:\n",
    "    # Filenames for original images: image0.tif and image5.tif\n",
    "    fname = f\"image{idx}.tif\"\n",
    "    fpath = os.path.join(SEG_DIR, fname)\n",
    "    print(f\"Processing Reference Image: {fname}\")\n",
    "    \n",
    "    feats = get_single_image_features(fpath)\n",
    "    if feats:\n",
    "        ref_data[idx] = feats\n",
    "    else:\n",
    "        print(f\"  -> Failed to extract features for {fname}\")\n",
    "\n",
    "# ==========================================\n",
    "# Visualization\n",
    "# ==========================================\n",
    "print(\"--- Step 3: Generating Plots ---\")\n",
    "\n",
    "alphas = np.arange(IMAGES_PER_REPLICATE)\n",
    "\n",
    "# 计算每个特征的全局 y 轴范围，以确保所有图表对齐\n",
    "print(\"Computing global y-axis limits for each feature...\")\n",
    "feature_ylims = {}\n",
    "for feat in feature_names:\n",
    "    all_vals = []\n",
    "    \n",
    "    # 收集重建数据的所有值（均值 + CI 界限）\n",
    "    mean_vals = df_mean[feat].values\n",
    "    ci_vals = df_ci[feat].values\n",
    "    all_vals.extend(mean_vals - ci_vals)  # Lower CI bound\n",
    "    all_vals.extend(mean_vals + ci_vals)  # Upper CI bound\n",
    "    \n",
    "    # 收集参考数据的所有值\n",
    "    for ref_idx in REF_INDICES:\n",
    "        if ref_idx in ref_data and feat in ref_data[ref_idx]:\n",
    "            all_vals.append(ref_data[ref_idx][feat])\n",
    "    \n",
    "    # 计算最小值/最大值，加上 10% 的 padding\n",
    "    all_vals = np.array([v for v in all_vals if np.isfinite(v)])\n",
    "    if len(all_vals) > 0:\n",
    "        v_min = np.min(all_vals)\n",
    "        v_max = np.max(all_vals)\n",
    "        v_range = v_max - v_min\n",
    "        padding = v_range * 0.1 if v_range > 0 else 0.1\n",
    "        feature_ylims[feat] = (v_min - padding, v_max + padding)\n",
    "    else:\n",
    "        feature_ylims[feat] = (0, 1)\n",
    "\n",
    "# 3A. Line Plots for each feature\n",
    "for feat in feature_names:\n",
    "    mean_vals = df_mean[feat].values\n",
    "    ci_vals = df_ci[feat].values\n",
    "    \n",
    "    plt.figure(figsize=(7, 5))\n",
    "    \n",
    "    # 1. Plot Replicate Trajectory (Mean + CI)\n",
    "    plt.plot(alphas, mean_vals, marker=\"o\", linewidth=2, label=\"Reconstructed (Mean)\", color=\"#1f77b4\")\n",
    "    \n",
    "    # Fill CI\n",
    "    lower_bound = mean_vals - ci_vals\n",
    "    upper_bound = mean_vals + ci_vals\n",
    "    # Handle NaNs in CI\n",
    "    valid_mask = np.isfinite(mean_vals) & np.isfinite(ci_vals)\n",
    "    if np.any(valid_mask):\n",
    "         plt.fill_between(alphas[valid_mask], lower_bound[valid_mask], upper_bound[valid_mask], \n",
    "                          color=\"#1f77b4\", alpha=0.2, label=\"95% CI\")\n",
    "\n",
    "    # 2. Plot Reference Points (Stars)\n",
    "    # Start (Image 0)\n",
    "    if 0 in ref_data and feat in ref_data[0]:\n",
    "        val0 = ref_data[0][feat]\n",
    "        plt.scatter(0, val0, marker=\"*\", s=200, color=\"red\", zorder=10, \n",
    "                    edgecolor=\"black\", label=\"Original Image 0\")\n",
    "        \n",
    "    # End (Image 5)\n",
    "    end_idx = IMAGES_PER_REPLICATE - 1\n",
    "    if end_idx in ref_data and feat in ref_data[end_idx]:\n",
    "        val_end = ref_data[end_idx][feat]\n",
    "        plt.scatter(end_idx, val_end, marker=\"*\", s=200, color=\"orange\", zorder=10, \n",
    "                    edgecolor=\"black\", label=\"Original Image 5\")\n",
    "\n",
    "    # 设置固定的 y 轴范围以确保所有图表的纵轴对齐\n",
    "    plt.ylim(feature_ylims[feat])\n",
    "\n",
    "    # plt.title(f\"Feature: {feat}\")\n",
    "    plt.xlabel(\"Interpolation Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.xticks(alphas, [f\"Step {i}\" for i in alphas])\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    \n",
    "    # 使用固定的左边界而不是 tight_layout，确保所有图表的左边界对齐\n",
    "    plt.subplots_adjust(left=0.15, right=0.95, top=0.95, bottom=0.1)\n",
    "    \n",
    "    # Save as PNG (preview) and PDF (Illustrator editable)\n",
    "    out_png = os.path.join(OUT_DIR, f\"Plot_{feat}_with_Ref.png\")\n",
    "    out_pdf = os.path.join(OUT_DIR, f\"Plot_{feat}_with_Ref.pdf\")\n",
    "    \n",
    "    plt.savefig(out_png, dpi=300)\n",
    "    plt.savefig(out_pdf, format='pdf', transparent=True)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Saved line plots (PNG and PDF).\")\n",
    "\n",
    "# 3B. Heatmap (Z-scored)\n",
    "# Structure: [Ref0, Step0, Step1 ... Step5, Ref5]\n",
    "# Note: This allows comparison of how 'far' the start/end reconstructions are from ground truth\n",
    "heatmap_cols = []\n",
    "heatmap_labels = []\n",
    "\n",
    "# Prepare Reference 0 Column\n",
    "if 0 in ref_data:\n",
    "    col_ref0 = pd.Series(ref_data[0])\n",
    "    heatmap_cols.append(col_ref0)\n",
    "    heatmap_labels.append(\"Ref_0\")\n",
    "\n",
    "# Prepare Replicate Columns (Means)\n",
    "for i in range(IMAGES_PER_REPLICATE):\n",
    "    heatmap_cols.append(df_mean.loc[i])\n",
    "    heatmap_labels.append(f\"Rec_{i}\")\n",
    "\n",
    "# Prepare Reference 5 Column\n",
    "end_idx = IMAGES_PER_REPLICATE - 1\n",
    "if end_idx in ref_data:\n",
    "    col_refEnd = pd.Series(ref_data[end_idx])\n",
    "    heatmap_cols.append(col_refEnd)\n",
    "    heatmap_labels.append(f\"Ref_{end_idx}\")\n",
    "\n",
    "# Construct DataFrame\n",
    "if heatmap_cols:\n",
    "    hm_df = pd.concat(heatmap_cols, axis=1)\n",
    "    hm_df.columns = heatmap_labels\n",
    "    \n",
    "    # Calculate Z-score per feature (row-wise)\n",
    "    # (x - mean) / std\n",
    "    hm_mean = hm_df.mean(axis=1)\n",
    "    hm_std = hm_df.std(axis=1)\n",
    "    # Avoid div by zero\n",
    "    hm_std[hm_std == 0] = 1.0\n",
    "    \n",
    "    hm_z = hm_df.sub(hm_mean, axis=0).div(hm_std, axis=0)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(hm_z, cmap=\"coolwarm\", center=0, annot=False, linewidths=.5, square=False)\n",
    "    # plt.title(\"Z-Score Heatmap: Originals vs Reconstructions\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save as PNG (preview) and PDF (Illustrator editable)\n",
    "    out_hm_png = os.path.join(OUT_DIR, \"Heatmap_Features_Ref_vs_Rec.png\")\n",
    "    out_hm_pdf = os.path.join(OUT_DIR, \"Heatmap_Features_Ref_vs_Rec.pdf\")\n",
    "    \n",
    "    plt.savefig(out_hm_png, dpi=300)\n",
    "    plt.savefig(out_hm_pdf, format='pdf', transparent=True)\n",
    "    plt.close()\n",
    "    print(f\"Saved heatmap to {out_hm_png} and {out_hm_pdf}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Save Summary Table\n",
    "# ==========================================\n",
    "summary_df = df_mean.copy()\n",
    "summary_df.columns = [f\"{c}_mean\" for c in summary_df.columns]\n",
    "\n",
    "# Add Std columns\n",
    "for col in feature_names:\n",
    "    summary_df[f\"{col}_std\"] = df_std[col]\n",
    "    \n",
    "# Add Ref columns (optional, strictly for the requested table)\n",
    "# We will create a separate row or column for Reference values? \n",
    "# Usually, a table comparing values is best.\n",
    "# Let's create a table that has rows: Ref0, Rec0...Rec5, Ref5\n",
    "\n",
    "table_rows = []\n",
    "# Row for Ref 0\n",
    "if 0 in ref_data:\n",
    "    row = {\"Type\": \"Original\", \"Step\": 0}\n",
    "    row.update(ref_data[0])\n",
    "    table_rows.append(row)\n",
    "\n",
    "# Rows for Replicates\n",
    "for i in range(IMAGES_PER_REPLICATE):\n",
    "    row = {\"Type\": \"Reconstructed_Mean\", \"Step\": i}\n",
    "    for feat in feature_names:\n",
    "        row[feat] = df_mean.loc[i, feat]\n",
    "        row[f\"{feat}_std\"] = df_std.loc[i, feat] # Add std as separate col\n",
    "    table_rows.append(row)\n",
    "\n",
    "# Row for Ref End\n",
    "if end_idx in ref_data:\n",
    "    row = {\"Type\": \"Original\", \"Step\": end_idx}\n",
    "    row.update(ref_data[end_idx])\n",
    "    table_rows.append(row)\n",
    "\n",
    "final_table = pd.DataFrame(table_rows)\n",
    "table_csv = os.path.join(OUT_DIR, \"summary_comparison_table.csv\")\n",
    "final_table.to_csv(table_csv, index=False)\n",
    "print(f\"Saved summary table to {table_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f941722c3b844369",
   "metadata": {},
   "source": [
    "# Add scale bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd5643cb44b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"/home/xavier/PycharmProjects/imgs_for_publication/WT_examples/1622_Run0432_scope7-00_1441.jpg\"\n",
    "image = resize_crop(img_name)\n",
    "\n",
    "# Create subplot\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Plot image\n",
    "ax.imshow(image, cmap=\"gray\", vmin=0, vmax=255)\n",
    "\n",
    "# Create scale bar\n",
    "scalebar = ScaleBar(2, \"um\", length_fraction=0.25)\n",
    "ax.add_artist(scalebar)\n",
    "# fig.tight_layout()\n",
    "# # Show\n",
    "# plt.savefig(os.path.join(OUT_DIR, \"scalebar_\" + os.path.basename(img_name)), bbox_inches='tight', pad_inches=0)\n",
    "fig.canvas.draw()\n",
    "image_with_scalebar = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "image_with_scalebar = image_with_scalebar.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "cv2.imwrite(os.path.join(OUT_DIR, \"scalebar_\" + os.path.basename(img_name)),\n",
    "            cv2.cvtColor(image_with_scalebar, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1d47547c24a45",
   "metadata": {},
   "source": [
    "# Reconstruct Shimkets' images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf79ba5baae1cf5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T22:18:17.151331Z",
     "start_time": "2025-01-13T22:18:14.717549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist=8.108855\n",
      "[-1.329601    0.39474863  0.73954904  1.9658827   0.12316137 -2.135191\n",
      " -0.17577764  0.6221569  -0.67273796 -0.45459506  0.49929696  0.7063217\n",
      " -0.51487744]\n",
      "Done!\n",
      "dist=7.981588\n",
      "[-1.270959    0.20502125  1.0424296   1.8978251  -0.3397985  -1.8977989\n",
      "  0.7800083   0.79577076 -0.07292919 -0.9170438   0.44937772  0.7122258\n",
      " -0.3741231 ]\n",
      "Done!\n",
      "dist=8.108855\n",
      "[ 1.329601   -0.39474863 -0.73954904 -1.9658827  -0.12316137  2.135191\n",
      "  0.17577764 -0.6221569   0.67273796  0.45459506 -0.49929696 -0.7063217\n",
      "  0.51487744]\n",
      "Done!\n",
      "dist=1.207601\n",
      "[ 0.17565313 -0.49039993  0.77105755 -0.2121663  -1.1860625   0.64896584\n",
      "  2.4716263   0.43802774  1.5529954  -1.1809089  -0.1344085  -0.00310794\n",
      "  0.36876637]\n",
      "Done!\n",
      "dist=7.981588\n",
      "[ 1.270959   -0.20502125 -1.0424296  -1.8978251   0.3397985   1.8977989\n",
      " -0.7800083  -0.79577076  0.07292919  0.9170438  -0.44937772 -0.7122258\n",
      "  0.3741231 ]\n",
      "Done!\n",
      "dist=1.207601\n",
      "[-0.17565313  0.49039993 -0.77105755  0.2121663   1.1860625  -0.64896584\n",
      " -2.4716263  -0.43802774 -1.5529954   1.1809089   0.1344085   0.00310794\n",
      " -0.36876637]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def extract_frame(video_path, frame_number):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Set the frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    # Read the frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Save the frame as an image\n",
    "        ans = frame  # cv2.imwrite(output_path, frame)\n",
    "    else:\n",
    "        print(f\"Error: Unable to read frame {frame_number} from {video_path}\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    return ans\n",
    "\n",
    "\n",
    "inner_num = 5\n",
    "fp = \"/media/xavier/Storage/feature_extraction/movie/movies\"\n",
    "# DK1622 with IPTG\n",
    "# movie1 = \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_1/LS3934 010614_563.avi\"\n",
    "# movie1 = \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_0/LS3934 6N 021014_786.avi\"\n",
    "# movie2 = \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_6/LS3934 60N 030414_543.avi\"\n",
    "movies = {0: \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_1/LS3934 010614_563.avi\",\n",
    "          2: \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_5/LS3934 2N 022814_573.avi\",\n",
    "          20: \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_4/LS3934 20N 031014_582.avi\",\n",
    "          200: \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_3/LS3934 200N 022414_588.avi\",\n",
    "          2000: \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_2/LS3934 2000N 022214_579.avi\"\n",
    "          }\n",
    "movies = {0: \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_1/LS3934 010614_563.avi\",\n",
    "          6: \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_0/LS3934 6N 021014_786.avi\",\n",
    "          60: \"/media/xavier/Storage/feature_extraction/movie/movies/LS3934_6/LS3934 60N 030414_543.avi\"\n",
    "          }\n",
    "OUT_DIR = \"/home/xavier/PycharmProjects/imgs_for_publication/figure3/interpolation/LS3934_6\"\n",
    "for m1 in movies:\n",
    "    movie1 = movies[m1]\n",
    "    for m2 in movies:\n",
    "        if m1 == m2:\n",
    "            continue\n",
    "        movie2 = movies[m2]\n",
    "        current_out = os.path.join(OUT_DIR, f\"{m1}_{m2}.png\")\n",
    "        os.makedirs(current_out, exist_ok=True)\n",
    "        img1 = extract_frame(movie1, 500)\n",
    "        img2 = extract_frame(movie2, 500)\n",
    "        img1 = resize_crop(img1, resize_by=23 / 20)\n",
    "        img2 = resize_crop(img2, resize_by=23 / 20)\n",
    "        name1 = movie1.split(\"/\")[-1][:-4] + '.png'\n",
    "        name2 = movie2.split(\"/\")[-1][:-4] + '.png'\n",
    "        cv2.imwrite(os.path.join(current_out, name1), img1)\n",
    "        cv2.imwrite(os.path.join(current_out, name2), img2)\n",
    "\n",
    "        reset_noise_const(G, 4)\n",
    "        imgs = torch.Tensor([img1, img2]).to(device).to(torch.float32) / 127.5 - 1\n",
    "        imgs = imgs[:, None, :, :]\n",
    "        z, _ = E.mu_var(imgs, None)\n",
    "        z_orint = z[1, :] - z[0, :]\n",
    "        z_orient_np = z_orint.cpu().numpy()\n",
    "        print(\"dist=%f\" % np.mean(np.square(z_orient_np)))\n",
    "        print(z_orient_np / np.sqrt(np.mean(np.square(z_orient_np))))\n",
    "\n",
    "        step_size = 1 / (inner_num - 1)\n",
    "        z_interpolate = torch.cat([(z[0, :] + z_orint * (i * step_size))[None, :] for i in range(inner_num)])\n",
    "        new_zs = z_interpolate  # torch.cat([z, z_interpolate], dim=0)\n",
    "        ws = G.mapping(z, None)\n",
    "        w_orint = ws[1, :, :] - ws[0, :, :]\n",
    "        w_orient_np = w_orint[0, :].cpu().numpy()\n",
    "        # print(w_orient_np / np.sqrt(np.mean(np.square(w_orient_np))))\n",
    "\n",
    "        ws_interpolate = torch.cat([(ws[0, :, :] + w_orint * (i * step_size))[None, :] for i in range(inner_num)])\n",
    "        synth_image = G(new_zs, None, noise_mode='const')\n",
    "        synth_image = (synth_image + 1) * 127.5\n",
    "        synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()[:, :, :, 0]\n",
    "        for i in range(inner_num):\n",
    "            cv2.imwrite(os.path.join(current_out, f\"image{i}.png\"), synth_image[i])\n",
    "            # PIL.Image.fromarray(synth_image[i], 'L').save(f'{recon_dir}/image{i}.pdf')\n",
    "\n",
    "        synth_image = G.synthesis(ws_interpolate, noise_mode='const')\n",
    "        synth_image = (synth_image + 1) * 127.5\n",
    "        synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()[:, :, :, 0]\n",
    "        for i in range(inner_num):\n",
    "            cv2.imwrite(os.path.join(current_out, f\"ws_image{i}.png\"), synth_image[i])\n",
    "            # PIL.Image.fromarray(synth_image[i], 'L').save(f'{recon_dir}/ws_image{i}.pdf')\n",
    "\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05ceb10ff53a497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T23:37:19.003253Z",
     "start_time": "2025-01-09T23:37:18.920839Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "fp = \"/media/xavier/Storage/feature_extraction/movie/movies\"\n",
    "ans = []\n",
    "for item in os.listdir(fp):\n",
    "    ans.extend(os.listdir(os.path.join(fp, item)))\n",
    "df = pd.DataFrame(ans, columns=['movie'])\n",
    "df['movie'] = df['movie'].sort_values().unique()\n",
    "df.to_csv(\"/media/xavier/Storage/feature_extraction/movie/movies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e787a7df58113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def extract_frame(video_path, frame_number):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Set the frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    # Read the frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Save the frame as an image\n",
    "        ans = frame  # cv2.imwrite(output_path, frame)\n",
    "    else:\n",
    "        print(f\"Error: Unable to read frame {frame_number} from {video_path}\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "946a0be99151d300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T23:08:23.735866Z",
     "start_time": "2025-01-22T23:08:23.730996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_5/LS3934 2N 022014_565.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_5/LS3934 2N 022814_573.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_5/LS3934 2N 030614_615.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_6/LS3934 60N 030214_538.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_6/LS3934 60N 030414_543.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_6/LS3934 60N 032214_540.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_7/LS3934 .2N 013114_532.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_7/LS3934 0.2N 030814_509.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_8/LS3934 0.6N 031214_596.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_8/LS3934 0.6N 031414_567.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_8/LS3934 0.6N 031614_625.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_0/LS3934 6N 021014_786.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_0/LS3934 6N 021314_800.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_0/LS3934 6N 031814_552.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_1/LS3934 010414_578.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_1/LS3934 010614_563.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_1/LS3934 012314_800.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_2/LS3934 2000N 022214_579.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_3/LS3934 200N 020314_800.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_3/LS3934 200N 020714_601.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_3/LS3934 200N 022414_588.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_3/LS3934 200N 022614_547.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_4/LS3934 20N 011314_512.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_4/LS3934 20N 020514_595.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_4/LS3934 20N 031014_582.avi\n",
      "/media/xavier/Storage/feature_extraction/movie/movies/LS3934_4/LS3934 20N 032014_551.avi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "fp = \"/media/xavier/Storage/feature_extraction/movie/movies\"\n",
    "OUT_DIR = \"/media/xavier/Storage/feature_extraction/movie/LS3934\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "ans = []\n",
    "for item in os.listdir(fp):\n",
    "    if item.startswith('LS3934'):\n",
    "        for movie_name in os.listdir(os.path.join(fp, item)):\n",
    "            print(os.path.join(fp, item, movie_name))\n",
    "            # # Open the video file\n",
    "            # cap = cv2.VideoCapture(os.path.join(fp, item, movie_name))\n",
    "            #\n",
    "            # # Get the total number of frames\n",
    "            # frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            #\n",
    "            # # Set the video position to the last frame\n",
    "            # cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count - 1)\n",
    "            #\n",
    "            # # Read the last frame\n",
    "            # ret, last_frame = cap.read()\n",
    "            # print(item, movie_name)\n",
    "            # # Check if the frame was successfully read\n",
    "            # if ret:\n",
    "            #     cv2.imwrite(os.path.join(OUT_DIR, item + '_' + movie_name.split(\".\")[0]) + '.jpg', last_frame)\n",
    "            #     cv2.waitKey(0)  # Wait until a key is pressed\n",
    "            #     cv2.destroyAllWindows()\n",
    "            # else:\n",
    "            #     print(\"Failed to read the last frame.\")\n",
    "            #\n",
    "            # # Release the video capture object\n",
    "            # cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d72c60a1e495e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st3-pure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
