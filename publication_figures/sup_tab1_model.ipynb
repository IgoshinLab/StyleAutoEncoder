{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "WORKING_DIR = \"/home/xavier/Documents/DAE_project\"",
   "id": "67afef5e4bacd825"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Use own method to print",
   "id": "1f1a6ee1d75d6554"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T07:35:18.713695Z",
     "start_time": "2025-09-25T07:35:17.935951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "import os\n",
    "\n",
    "# The StyleGAN2 repo's own utility for printing model summaries\n",
    "from torch_utils import misc\n",
    "\n",
    "# Set compiler environment variables if needed for custom CUDA extensions\n",
    "os.environ['CC'] = \"/usr/bin/gcc-9\"\n",
    "os.environ['CXX'] = \"/usr/bin/g++-9\"\n",
    "\n",
    "\n",
    "def analyze_full_model_architecture(network_pkl_path: str):\n",
    "    \"\"\"\n",
    "    Loads a StyleGAN2-based model and prints the architecture summary for the\n",
    "    Generator (G), Discriminator (D), and Encoder (E) using the built-in\n",
    "    misc.print_module_summary() utility.\n",
    "    \"\"\"\n",
    "    print(f\"Loading models from '{network_pkl_path}'...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        with dnnlib.util.open_url(network_pkl_path) as fp:\n",
    "            models = legacy.load_network_pkl(fp)\n",
    "            G = models['G_ema'].to(device).eval()\n",
    "            D = models['D'].to(device).eval()\n",
    "            E = models['E_ema'].to(device).eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Models G, D, and E loaded successfully.\")\n",
    "\n",
    "    # --- Use misc.print_module_summary() as requested ---\n",
    "\n",
    "    # 1. Define a batch size for creating dummy inputs\n",
    "    batch_gpu = 1\n",
    "    print(f\"Using batch size: {batch_gpu} for summary generation.\\n\")\n",
    "\n",
    "    # 2. Create dummy input tensors based on the Generator's parameters\n",
    "    # This ensures the inputs have the correct dimensions for all models.\n",
    "    z = torch.empty([batch_gpu, G.z_dim], device=device)\n",
    "    c = torch.empty([batch_gpu, G.c_dim], device=device)\n",
    "    img = torch.empty([batch_gpu, G.img_channels, G.img_resolution, G.img_resolution], device=device)\n",
    "\n",
    "    print(\"--- Encoder (E) Summary ---\")\n",
    "    misc.print_module_summary(E, [img, c])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    # 3. Print the summary for each model\n",
    "    print(\"--- Generator (G) Summary ---\")\n",
    "    misc.print_module_summary(G, [z, c])\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "    print(\"--- Discriminator (D) Summary ---\")\n",
    "    # Using the exact input structure you provided\n",
    "    misc.print_module_summary(D, [[img, img], [c, c]])\n",
    "\n",
    "    print(\"\\nSummary generation complete.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Please modify the path to your model here ---\n",
    "    network_pkl = f\"{WORKING_DIR}/models/network-snapshot-001512.pkl\"\n",
    "\n",
    "    # Run the analysis\n",
    "    analyze_full_model_architecture(network_pkl)\n",
    "\n"
   ],
   "id": "e245fbd10ff589c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from '/home/xavier/PycharmProjects/training-runs/e13-post/from302kimgs/00001-stylegan2-trainingset2-gpus4-batch112-gamma10/network-snapshot-001512.pkl'...\n",
      "Using device: cuda\n",
      "Models G, D, and E loaded successfully.\n",
      "Using batch size: 1 for summary generation.\n",
      "\n",
      "--- Encoder (E) Summary ---\n",
      "\n",
      "Discriminator2       Parameters  Buffers  Output shape        Datatype\n",
      "---                  ---         ---      ---                 ---     \n",
      "b512.fromrgb         64          16       [1, 64, 512, 512]   float16 \n",
      "b512.skip            8192        16       [1, 128, 256, 256]  float16 \n",
      "b512.conv0           36864       16       [1, 64, 512, 512]   float16 \n",
      "b512.conv1           73728       16       [1, 128, 256, 256]  float16 \n",
      "b512                 -           16       [1, 128, 256, 256]  float16 \n",
      "b256.skip            32768       16       [1, 256, 128, 128]  float16 \n",
      "b256.conv0           147456      16       [1, 128, 256, 256]  float16 \n",
      "b256.conv1           294912      16       [1, 256, 128, 128]  float16 \n",
      "b256                 -           16       [1, 256, 128, 128]  float16 \n",
      "b128.skip            131072      16       [1, 512, 64, 64]    float16 \n",
      "b128.conv0           589824      16       [1, 256, 128, 128]  float16 \n",
      "b128.conv1           1179648     16       [1, 512, 64, 64]    float16 \n",
      "b128                 -           16       [1, 512, 64, 64]    float16 \n",
      "b64.skip             262144      16       [1, 512, 32, 32]    float16 \n",
      "b64.conv0            2359296     16       [1, 512, 64, 64]    float16 \n",
      "b64.conv1            2359296     16       [1, 512, 32, 32]    float16 \n",
      "b64                  -           16       [1, 512, 32, 32]    float16 \n",
      "b32.skip             262144      16       [1, 512, 16, 16]    float32 \n",
      "b32.conv0            2359296     16       [1, 512, 32, 32]    float32 \n",
      "b32.conv1            2359296     16       [1, 512, 16, 16]    float32 \n",
      "b32                  -           16       [1, 512, 16, 16]    float32 \n",
      "b16.skip             262144      16       [1, 512, 8, 8]      float32 \n",
      "b16.conv0            2359296     16       [1, 512, 16, 16]    float32 \n",
      "b16.conv1            2359296     16       [1, 512, 8, 8]      float32 \n",
      "b16                  -           16       [1, 512, 8, 8]      float32 \n",
      "b8.skip              262144      16       [1, 512, 4, 4]      float32 \n",
      "b8.conv0             2359296     16       [1, 512, 8, 8]      float32 \n",
      "b8.conv1             2359296     16       [1, 512, 4, 4]      float32 \n",
      "b8                   -           16       [1, 512, 4, 4]      float32 \n",
      "b4.get_mu.mbstd      -           -        [1, 513, 4, 4]      float32 \n",
      "b4.get_mu.conv       2363904     16       [1, 512, 4, 4]      float32 \n",
      "b4.get_mu.fc         4194816     -        [1, 512]            float32 \n",
      "b4.get_mu.out        6669        -        [1, 13]             float32 \n",
      "b4.get_logvar.mbstd  -           -        [1, 513, 4, 4]      float32 \n",
      "b4.get_logvar.conv   2363904     16       [1, 512, 4, 4]      float32 \n",
      "b4.get_logvar.fc     4194816     -        [1, 512]            float32 \n",
      "b4.get_logvar.out    6669        -        [1, 13]             float32 \n",
      "b4                   -           -        [1, 13]             float32 \n",
      "---                  ---         ---      ---                 ---     \n",
      "Total                35548250    496      -                   -       \n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Generator (G) Summary ---\n",
      "\n",
      "Generator             Parameters  Buffers  Output shape        Datatype\n",
      "---                   ---         ---      ---                 ---     \n",
      "mapping.fc0           7168        -        [1, 512]            float32 \n",
      "mapping.fc1           262656      -        [1, 512]            float32 \n",
      "mapping.fc2           262656      -        [1, 512]            float32 \n",
      "mapping.fc3           262656      -        [1, 512]            float32 \n",
      "mapping.fc4           262656      -        [1, 512]            float32 \n",
      "mapping.fc5           262656      -        [1, 512]            float32 \n",
      "mapping.fc6           262656      -        [1, 512]            float32 \n",
      "mapping.fc7           262656      -        [1, 512]            float32 \n",
      "mapping               -           512      [1, 16, 512]        float32 \n",
      "synthesis.b4.conv1    2622465     32       [1, 512, 4, 4]      float32 \n",
      "synthesis.b4.torgb    263169      -        [1, 1, 4, 4]        float32 \n",
      "synthesis.b4:0        8192        16       [1, 512, 4, 4]      float32 \n",
      "synthesis.b4:1        -           -        [1, 1, 4, 4]        float32 \n",
      "synthesis.b8.conv0    2622465     80       [1, 512, 8, 8]      float32 \n",
      "synthesis.b8.conv1    2622465     80       [1, 512, 8, 8]      float32 \n",
      "synthesis.b8.torgb    263169      -        [1, 1, 8, 8]        float32 \n",
      "synthesis.b8:0        -           16       [1, 512, 8, 8]      float32 \n",
      "synthesis.b8:1        -           -        [1, 1, 8, 8]        float32 \n",
      "synthesis.b16.conv0   2622465     272      [1, 512, 16, 16]    float32 \n",
      "synthesis.b16.conv1   2622465     272      [1, 512, 16, 16]    float32 \n",
      "synthesis.b16.torgb   263169      -        [1, 1, 16, 16]      float32 \n",
      "synthesis.b16:0       -           16       [1, 512, 16, 16]    float32 \n",
      "synthesis.b16:1       -           -        [1, 1, 16, 16]      float32 \n",
      "synthesis.b32.conv0   2622465     1040     [1, 512, 32, 32]    float32 \n",
      "synthesis.b32.conv1   2622465     1040     [1, 512, 32, 32]    float32 \n",
      "synthesis.b32.torgb   263169      -        [1, 1, 32, 32]      float32 \n",
      "synthesis.b32:0       -           16       [1, 512, 32, 32]    float32 \n",
      "synthesis.b32:1       -           -        [1, 1, 32, 32]      float32 \n",
      "synthesis.b64.conv0   2622465     4112     [1, 512, 64, 64]    float16 \n",
      "synthesis.b64.conv1   2622465     4112     [1, 512, 64, 64]    float16 \n",
      "synthesis.b64.torgb   263169      -        [1, 1, 64, 64]      float16 \n",
      "synthesis.b64:0       -           16       [1, 512, 64, 64]    float16 \n",
      "synthesis.b64:1       -           -        [1, 1, 64, 64]      float32 \n",
      "synthesis.b128.conv0  1442561     16400    [1, 256, 128, 128]  float16 \n",
      "synthesis.b128.conv1  721409      16400    [1, 256, 128, 128]  float16 \n",
      "synthesis.b128.torgb  131585      -        [1, 1, 128, 128]    float16 \n",
      "synthesis.b128:0      -           16       [1, 256, 128, 128]  float16 \n",
      "synthesis.b128:1      -           -        [1, 1, 128, 128]    float32 \n",
      "synthesis.b256.conv0  426369      65552    [1, 128, 256, 256]  float16 \n",
      "synthesis.b256.conv1  213249      65552    [1, 128, 256, 256]  float16 \n",
      "synthesis.b256.torgb  65793       -        [1, 1, 256, 256]    float16 \n",
      "synthesis.b256:0      -           16       [1, 128, 256, 256]  float16 \n",
      "synthesis.b256:1      -           -        [1, 1, 256, 256]    float32 \n",
      "synthesis.b512.conv0  139457      262160   [1, 64, 512, 512]   float16 \n",
      "synthesis.b512.conv1  69761       262160   [1, 64, 512, 512]   float16 \n",
      "synthesis.b512.torgb  32897       -        [1, 1, 512, 512]    float16 \n",
      "synthesis.b512:0      -           16       [1, 64, 512, 512]   float16 \n",
      "synthesis.b512:1      -           -        [1, 1, 512, 512]    float32 \n",
      "---                   ---         ---      ---                 ---     \n",
      "Total                 30015063    699904   -                   -       \n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Discriminator (D) Summary ---\n",
      "\n",
      "Discriminator2  Parameters  Buffers  Output shape        Datatype\n",
      "---             ---         ---      ---                 ---     \n",
      "b512.fromrgb    64          16       [1, 64, 512, 512]   float16 \n",
      "b512.skip       8192        16       [1, 128, 256, 256]  float16 \n",
      "b512.conv0      36864       16       [1, 64, 512, 512]   float16 \n",
      "b512.conv1      73728       16       [1, 128, 256, 256]  float16 \n",
      "b512            -           16       [1, 128, 256, 256]  float16 \n",
      "b512.fromrgb    -           -        [1, 64, 512, 512]   float16 \n",
      "b512.skip       -           -        [1, 128, 256, 256]  float16 \n",
      "b512.conv0      -           -        [1, 64, 512, 512]   float16 \n",
      "b512.conv1      -           -        [1, 128, 256, 256]  float16 \n",
      "b256.skip       32768       16       [1, 256, 128, 128]  float16 \n",
      "b256.conv0      147456      16       [1, 128, 256, 256]  float16 \n",
      "b256.conv1      294912      16       [1, 256, 128, 128]  float16 \n",
      "b256            -           16       [1, 256, 128, 128]  float16 \n",
      "b256.skip       -           -        [1, 256, 128, 128]  float16 \n",
      "b256.conv0      -           -        [1, 128, 256, 256]  float16 \n",
      "b256.conv1      -           -        [1, 256, 128, 128]  float16 \n",
      "b128.skip       131072      16       [1, 512, 64, 64]    float16 \n",
      "b128.conv0      589824      16       [1, 256, 128, 128]  float16 \n",
      "b128.conv1      1179648     16       [1, 512, 64, 64]    float16 \n",
      "b128            -           16       [1, 512, 64, 64]    float16 \n",
      "b128.skip       -           -        [1, 512, 64, 64]    float16 \n",
      "b128.conv0      -           -        [1, 256, 128, 128]  float16 \n",
      "b128.conv1      -           -        [1, 512, 64, 64]    float16 \n",
      "b64.skip        262144      16       [1, 512, 32, 32]    float16 \n",
      "b64.conv0       2359296     16       [1, 512, 64, 64]    float16 \n",
      "b64.conv1       2359296     16       [1, 512, 32, 32]    float16 \n",
      "b64             -           16       [1, 512, 32, 32]    float16 \n",
      "b64.skip        -           -        [1, 512, 32, 32]    float16 \n",
      "b64.conv0       -           -        [1, 512, 64, 64]    float16 \n",
      "b64.conv1       -           -        [1, 512, 32, 32]    float16 \n",
      "b32.skip        262144      16       [1, 512, 16, 16]    float32 \n",
      "b32.conv0       2359296     16       [1, 512, 32, 32]    float32 \n",
      "b32.conv1       2359296     16       [1, 512, 16, 16]    float32 \n",
      "b32             -           16       [1, 512, 16, 16]    float32 \n",
      "b32.skip        -           -        [1, 512, 16, 16]    float32 \n",
      "b32.conv0       -           -        [1, 512, 32, 32]    float32 \n",
      "b32.conv1       -           -        [1, 512, 16, 16]    float32 \n",
      "b16.skip        262144      16       [1, 512, 8, 8]      float32 \n",
      "b16.conv0       2359296     16       [1, 512, 16, 16]    float32 \n",
      "b16.conv1       2359296     16       [1, 512, 8, 8]      float32 \n",
      "b16             -           16       [1, 512, 8, 8]      float32 \n",
      "b16.skip        -           -        [1, 512, 8, 8]      float32 \n",
      "b16.conv0       -           -        [1, 512, 16, 16]    float32 \n",
      "b16.conv1       -           -        [1, 512, 8, 8]      float32 \n",
      "b8.skip         262144      16       [1, 512, 4, 4]      float32 \n",
      "b8.conv0        2359296     16       [1, 512, 8, 8]      float32 \n",
      "b8.conv1        2359296     16       [1, 512, 4, 4]      float32 \n",
      "b8              -           16       [1, 512, 4, 4]      float32 \n",
      "b8.skip         -           -        [1, 512, 4, 4]      float32 \n",
      "b8.conv0        -           -        [1, 512, 8, 8]      float32 \n",
      "b8.conv1        -           -        [1, 512, 4, 4]      float32 \n",
      "b4.mbstd        -           -        [1, 513, 4, 8]      float32 \n",
      "b4.conv         2363904     16       [1, 512, 4, 8]      float32 \n",
      "b4.fc           8389120     -        [1, 512]            float32 \n",
      "b4.out          513         -        [1, 1]              float32 \n",
      "---             ---         ---      ---                 ---     \n",
      "Total           33171009    480      -                   -       \n",
      "\n",
      "\n",
      "Summary generation complete.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Detailed print",
   "id": "6c4925651c3dd6f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T07:55:41.132583Z",
     "start_time": "2025-09-25T07:55:40.611813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Set compiler environment variables if needed for custom CUDA extensions\n",
    "os.environ['CC'] = \"/usr/bin/gcc-9\"\n",
    "os.environ['CXX'] = \"/usr/bin/g++-9\"\n",
    "\n",
    "\n",
    "def generate_publication_summary(model_name: str, model: torch.nn.Module, dummy_inputs: list,\n",
    "                                 output_format: str = 'text'):\n",
    "    \"\"\"\n",
    "    Generates a detailed, publication-ready architecture summary for a given model.\n",
    "\n",
    "    This function can output the summary as plain text or as a LaTeX table.\n",
    "    \"\"\"\n",
    "    if output_format == 'latex':\n",
    "        print(f\"% --- LaTeX Summary for {model_name} (Copy and paste into your .tex file) ---\")\n",
    "    else:\n",
    "        print(f\"--- {model_name} Summary ---\")\n",
    "\n",
    "    # 1. Use hooks to capture I/O shapes and datatypes\n",
    "    summary_data = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        def get_tensor_info(data):\n",
    "            if isinstance(data, torch.Tensor):\n",
    "                return list(data.shape), str(data.dtype).replace('torch.', '')\n",
    "            if isinstance(data, (list, tuple)) and data:\n",
    "                for item in data:\n",
    "                    shape, dtype = get_tensor_info(item)\n",
    "                    if shape is not None:\n",
    "                        return shape, dtype\n",
    "            return None, None\n",
    "\n",
    "        output_shape, output_dtype = get_tensor_info(output)\n",
    "        summary_data[module] = {\"output_shape\": output_shape, \"output_dtype\": output_dtype}\n",
    "\n",
    "    for module in model.modules():\n",
    "        hooks.append(module.register_forward_hook(hook_fn))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(*dummy_inputs)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # 2. Generate and print the summary in the chosen format\n",
    "    if output_format == 'latex':\n",
    "        generate_latex_table(model_name, model, summary_data)\n",
    "    else:\n",
    "        generate_text_table(model, summary_data)\n",
    "\n",
    "\n",
    "def generate_text_table(model, summary_data):\n",
    "    \"\"\"Prints the architecture summary as a plain text table.\"\"\"\n",
    "    header = (\n",
    "        f\"{'Layer (type)':<50} {'Output Shape':<25} {'Kernel/Stride':<15} \"\n",
    "        f\"{'Activation':<12} {'Datatype':<10} {'Parameters':<15}\"\n",
    "    )\n",
    "    print(header)\n",
    "    print(\"=\" * len(header))\n",
    "    total_params = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\": continue\n",
    "        params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "        total_params += params\n",
    "        io_info = summary_data.get(module)\n",
    "        output_shape_str = str(io_info['output_shape']) if io_info and io_info['output_shape'] else 'N/A'\n",
    "        output_dtype_str = str(io_info['output_dtype']) if io_info and io_info['output_dtype'] else 'N/A'\n",
    "        layer_type, details_str, activation_str = get_module_details(module)\n",
    "        indent = \"  \" * (name.count('.'))\n",
    "        name_str = indent + f\"{name.split('.')[-1]} ({layer_type})\"\n",
    "        row = (\n",
    "            f\"{name_str:<50} {output_shape_str:<25} {details_str:<15} \"\n",
    "            f\"{activation_str:<12} {output_dtype_str:<10} {params:<15,}\"\n",
    "        )\n",
    "        if params > 0 or \"Network\" in layer_type:  # Print parent networks even if they have no direct params\n",
    "            print(row)\n",
    "\n",
    "    print(\"=\" * len(header))\n",
    "    print(f\"Total Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "def generate_latex_table(model_name, model, summary_data):\n",
    "    \"\"\"Prints the architecture summary as a LaTeX table.\"\"\"\n",
    "    latex_output = [\n",
    "        r\"\\begin{table*}[ht]\",\n",
    "        r\"  \\centering\",\n",
    "        fr\"  \\caption{{Detailed architecture of the {model_name} network.}}\",\n",
    "        fr\"  \\label{{tab:arch_{model_name.lower().replace(' ', '_')}}}\",\n",
    "        r\"  \\begin{tabular}{l l l l l r}\",\n",
    "        r\"    \\hline\",\n",
    "        r\"    \\textbf{Layer (type)} & \\textbf{Output Shape} & \\textbf{Kernel/Stride} & \\textbf{Activation} & \\textbf{Datatype} & \\textbf{Parameters} \\\\\",\n",
    "        r\"    \\hline\"\n",
    "    ]\n",
    "    total_params = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"\": continue\n",
    "        params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "        total_params += params\n",
    "        layer_type, details_str, activation_str = get_module_details(module)\n",
    "\n",
    "        # Don't print rows for modules with no parameters, unless it's a major structural block\n",
    "        if params == 0 and \"Network\" not in layer_type:\n",
    "            continue\n",
    "\n",
    "        io_info = summary_data.get(module)\n",
    "        output_shape_str = str(io_info['output_shape']) if io_info and io_info['output_shape'] else 'N/A'\n",
    "        output_dtype_str = str(io_info['output_dtype']) if io_info and io_info['output_dtype'] else 'N/A'\n",
    "\n",
    "        def escape_latex(s):\n",
    "            return s.replace('_', r'\\_').replace('%', r'\\%').replace('$', r'\\$')\n",
    "\n",
    "        indent_level = name.count('.')\n",
    "        indent = r\"\\quad \" * indent_level\n",
    "\n",
    "        # Bold major network components\n",
    "        is_major_block = \"Network\" in layer_type\n",
    "        name_prefix = r\"\\textbf{\" if is_major_block else \"\"\n",
    "        name_suffix = \"}\" if is_major_block else \"\"\n",
    "\n",
    "        # For major blocks, don't show the layer type in parentheses\n",
    "        if is_major_block:\n",
    "            name_str = indent + name_prefix + escape_latex(f\"{name.split('.')[-1]}\") + name_suffix\n",
    "        else:\n",
    "            name_str = indent + name_prefix + escape_latex(f\"{name.split('.')[-1]} ({layer_type})\") + name_suffix\n",
    "\n",
    "        row_items = [\n",
    "            name_str,\n",
    "            escape_latex(output_shape_str),\n",
    "            escape_latex(details_str),\n",
    "            escape_latex(activation_str),\n",
    "            escape_latex(output_dtype_str),\n",
    "            f\"{params:,}\" if params > 0 else \"-\"\n",
    "        ]\n",
    "        latex_output.append(\"    \" + \" & \".join(row_items) + r\" \\\\\")\n",
    "\n",
    "    latex_output.extend([\n",
    "        r\"    \\hline\",\n",
    "        fr\"    \\textbf{{Total}} & & & & & \\textbf{{{total_params:,}}} \\\\\",\n",
    "        r\"    \\hline\",\n",
    "        r\"  \\end{tabular}\",\n",
    "        r\"\\end{table*}\"\n",
    "    ])\n",
    "    print(\"\\n\".join(latex_output))\n",
    "\n",
    "\n",
    "def get_module_details(module):\n",
    "    \"\"\"Helper function to extract details from a specific module.\"\"\"\n",
    "    layer_type = module.__class__.__name__\n",
    "    details_str = \"N/A\"\n",
    "    activation_str = \"N/A\"\n",
    "\n",
    "    if layer_type == 'SynthesisLayer':\n",
    "        k_size = tuple(getattr(module, 'weight', torch.zeros(0)).shape[2:])\n",
    "        stride_info = f\"{module.up} (up)\"\n",
    "        details_str = f\"{k_size[0]}x{k_size[1]} / {stride_info}\"\n",
    "        activation_str = getattr(module, 'activation', 'N/A')\n",
    "    elif layer_type == 'ToRGBLayer':\n",
    "        k_size = tuple(getattr(module, 'weight', torch.zeros(0)).shape[2:])\n",
    "        stride_info = \"1\"  # ToRGBLayer does not upsample.\n",
    "        details_str = f\"{k_size[0]}x{k_size[1]} / {stride_info}\"\n",
    "        activation_str = getattr(module, 'activation', 'N/A')\n",
    "    elif layer_type == 'Conv2dLayer':\n",
    "        k_size = tuple(module.weight.shape[2:])\n",
    "        stride_info = f\"{module.down} (down)\" if module.down > 1 else f\"{module.up} (up)\"\n",
    "        details_str = f\"{k_size[0]}x{k_size[1]} / {stride_info}\"\n",
    "        activation_str = getattr(module, 'activation', 'N/A')\n",
    "    elif layer_type == 'FullyConnectedLayer':\n",
    "        out_f, in_f = module.weight.shape\n",
    "        details_str = f\"({in_f}, {out_f})\"\n",
    "        activation_str = getattr(module, 'activation', 'N/A')\n",
    "    elif layer_type in ['MappingNetwork', 'SynthesisNetwork']:\n",
    "        # For major blocks, we don't show these details in the parent row\n",
    "        details_str, activation_str = \"\", \"\"\n",
    "\n",
    "    return layer_type, details_str, activation_str\n",
    "\n",
    "\n",
    "def analyze_full_model_architecture(network_pkl_path: str, output_format: str = 'latex'):\n",
    "    \"\"\"Loads G, D, and E and generates a detailed summary for each.\"\"\"\n",
    "    print(f\"Loading models from '{network_pkl_path}'...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        with dnnlib.util.open_url(network_pkl_path) as fp:\n",
    "            models = legacy.load_network_pkl(fp)\n",
    "            G = models['G_ema'].to(device).eval()\n",
    "            D = models['D'].to(device).eval()\n",
    "            E = models['E_ema'].to(device).eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Models G, D, and E loaded successfully. Generating {output_format} summaries...\\n\")\n",
    "\n",
    "    batch_gpu = 1\n",
    "    z = torch.empty([batch_gpu, G.z_dim], device=device)\n",
    "    c = torch.empty([batch_gpu, G.c_dim], device=device)\n",
    "    img = torch.empty([batch_gpu, G.img_channels, G.img_resolution, G.img_resolution], device=device)\n",
    "\n",
    "    generate_publication_summary(\"Encoder (E)\", E, [img, c], output_format)\n",
    "    print(\"\\n\")\n",
    "    generate_publication_summary(\"Generator (G)\", G, [z, c], output_format)\n",
    "    print(\"\\n\")\n",
    "    generate_publication_summary(\"Discriminator (D)\", D, [[img, img], [c, c]], output_format)\n",
    "\n",
    "    print(f\"\\nSummary generation complete. You can now copy the LaTeX code into your Overleaf project.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network_pkl = f\"{WORKING_DIR}/models/network-snapshot-001512.pkl\"\n",
    "    # To get plain text output instead, change to output_format='text'\n",
    "    analyze_full_model_architecture(network_pkl, output_format='latex')\n",
    "\n"
   ],
   "id": "aa8e2df7931117f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from '/home/xavier/PycharmProjects/training-runs/e13-post/from302kimgs/00001-stylegan2-trainingset2-gpus4-batch112-gamma10/network-snapshot-001512.pkl'...\n",
      "Using device: cuda\n",
      "Models G, D, and E loaded successfully. Generating latex summaries...\n",
      "\n",
      "% --- LaTeX Summary for Encoder (E) (Copy and paste into your .tex file) ---\n",
      "\\begin{table*}[ht]\n",
      "  \\centering\n",
      "  \\caption{Detailed architecture of the Encoder (E) network.}\n",
      "  \\label{tab:arch_encoder_(e)}\n",
      "  \\begin{tabular}{l l l l l r}\n",
      "    \\hline\n",
      "    \\textbf{Layer (type)} & \\textbf{Output Shape} & \\textbf{Kernel/Stride} & \\textbf{Activation} & \\textbf{Datatype} & \\textbf{Parameters} \\\\\n",
      "    \\hline\n",
      "    \\quad fromrgb (Conv2dLayer) & [1, 64, 512, 512] & 1x1 / 1 (up) & lrelu & float16 & 64 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 64, 512, 512] & 3x3 / 1 (up) & lrelu & float16 & 36,864 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 128, 256, 256] & 3x3 / 2 (down) & lrelu & float16 & 73,728 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 128, 256, 256] & 1x1 / 2 (down) & linear & float16 & 8,192 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 128, 256, 256] & 3x3 / 1 (up) & lrelu & float16 & 147,456 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 256, 128, 128] & 3x3 / 2 (down) & lrelu & float16 & 294,912 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 256, 128, 128] & 1x1 / 2 (down) & linear & float16 & 32,768 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 256, 128, 128] & 3x3 / 1 (up) & lrelu & float16 & 589,824 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 64, 64] & 3x3 / 2 (down) & lrelu & float16 & 1,179,648 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 64, 64] & 1x1 / 2 (down) & linear & float16 & 131,072 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 512, 64, 64] & 3x3 / 1 (up) & lrelu & float16 & 2,359,296 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 32, 32] & 3x3 / 2 (down) & lrelu & float16 & 2,359,296 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 32, 32] & 1x1 / 2 (down) & linear & float16 & 262,144 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 512, 32, 32] & 3x3 / 1 (up) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 16, 16] & 3x3 / 2 (down) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 16, 16] & 1x1 / 2 (down) & linear & float32 & 262,144 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 512, 16, 16] & 3x3 / 1 (up) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 8, 8] & 3x3 / 2 (down) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 8, 8] & 1x1 / 2 (down) & linear & float32 & 262,144 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 512, 8, 8] & 3x3 / 1 (up) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 4, 4] & 3x3 / 2 (down) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 4, 4] & 1x1 / 2 (down) & linear & float32 & 262,144 \\\\\n",
      "    \\quad \\quad conv (Conv2dLayer) & [1, 512, 4, 4] & 3x3 / 1 (up) & lrelu & float32 & 2,363,904 \\\\\n",
      "    \\quad \\quad fc (FullyConnectedLayer) & [1, 512] & (8192, 512) & lrelu & float32 & 4,194,816 \\\\\n",
      "    \\quad \\quad out (FullyConnectedLayer) & [1, 13] & (512, 13) & linear & float32 & 6,669 \\\\\n",
      "    \\quad \\quad conv (Conv2dLayer) & [1, 512, 4, 4] & 3x3 / 1 (up) & lrelu & float32 & 2,363,904 \\\\\n",
      "    \\quad \\quad fc (FullyConnectedLayer) & [1, 512] & (8192, 512) & lrelu & float32 & 4,194,816 \\\\\n",
      "    \\quad \\quad out (FullyConnectedLayer) & [1, 13] & (512, 13) & linear & float32 & 6,669 \\\\\n",
      "    \\hline\n",
      "    \\textbf{Total} & & & & & \\textbf{35,548,250} \\\\\n",
      "    \\hline\n",
      "  \\end{tabular}\n",
      "\\end{table*}\n",
      "\n",
      "\n",
      "% --- LaTeX Summary for Generator (G) (Copy and paste into your .tex file) ---\n",
      "\\begin{table*}[ht]\n",
      "  \\centering\n",
      "  \\caption{Detailed architecture of the Generator (G) network.}\n",
      "  \\label{tab:arch_generator_(g)}\n",
      "  \\begin{tabular}{l l l l l r}\n",
      "    \\hline\n",
      "    \\textbf{Layer (type)} & \\textbf{Output Shape} & \\textbf{Kernel/Stride} & \\textbf{Activation} & \\textbf{Datatype} & \\textbf{Parameters} \\\\\n",
      "    \\hline\n",
      "    \\textbf{synthesis} & [1, 1, 512, 512] &  &  & float32 & - \\\\\n",
      "    \\quad b4 (SynthesisBlock) & [1, 512, 4, 4] & N/A & N/A & float32 & 8,192 \\\\\n",
      "    \\quad \\quad conv1 (SynthesisLayer) & [1, 512, 4, 4] & 3x3 / 1 (up) & lrelu & float32 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad torgb (ToRGBLayer) & [1, 1, 4, 4] & 1x1 / 1 & N/A & float32 & 513 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv0 (SynthesisLayer) & [1, 512, 8, 8] & 3x3 / 2 (up) & lrelu & float32 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv1 (SynthesisLayer) & [1, 512, 8, 8] & 3x3 / 1 (up) & lrelu & float32 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad torgb (ToRGBLayer) & [1, 1, 8, 8] & 1x1 / 1 & N/A & float32 & 513 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv0 (SynthesisLayer) & [1, 512, 16, 16] & 3x3 / 2 (up) & lrelu & float32 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv1 (SynthesisLayer) & [1, 512, 16, 16] & 3x3 / 1 (up) & lrelu & float32 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad torgb (ToRGBLayer) & [1, 1, 16, 16] & 1x1 / 1 & N/A & float32 & 513 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv0 (SynthesisLayer) & [1, 512, 32, 32] & 3x3 / 2 (up) & lrelu & float32 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv1 (SynthesisLayer) & [1, 512, 32, 32] & 3x3 / 1 (up) & lrelu & float32 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad torgb (ToRGBLayer) & [1, 1, 32, 32] & 1x1 / 1 & N/A & float32 & 513 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv0 (SynthesisLayer) & [1, 512, 64, 64] & 3x3 / 2 (up) & lrelu & float16 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv1 (SynthesisLayer) & [1, 512, 64, 64] & 3x3 / 1 (up) & lrelu & float16 & 2,359,809 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad torgb (ToRGBLayer) & [1, 1, 64, 64] & 1x1 / 1 & N/A & float16 & 513 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv0 (SynthesisLayer) & [1, 256, 128, 128] & 3x3 / 2 (up) & lrelu & float16 & 1,179,905 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 512] & (512, 512) & linear & float32 & 262,656 \\\\\n",
      "    \\quad \\quad conv1 (SynthesisLayer) & [1, 256, 128, 128] & 3x3 / 1 (up) & lrelu & float16 & 590,081 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 256] & (512, 256) & linear & float32 & 131,328 \\\\\n",
      "    \\quad \\quad torgb (ToRGBLayer) & [1, 1, 128, 128] & 1x1 / 1 & N/A & float16 & 257 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 256] & (512, 256) & linear & float32 & 131,328 \\\\\n",
      "    \\quad \\quad conv0 (SynthesisLayer) & [1, 128, 256, 256] & 3x3 / 2 (up) & lrelu & float16 & 295,041 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 256] & (512, 256) & linear & float32 & 131,328 \\\\\n",
      "    \\quad \\quad conv1 (SynthesisLayer) & [1, 128, 256, 256] & 3x3 / 1 (up) & lrelu & float16 & 147,585 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 128] & (512, 128) & linear & float32 & 65,664 \\\\\n",
      "    \\quad \\quad torgb (ToRGBLayer) & [1, 1, 256, 256] & 1x1 / 1 & N/A & float16 & 129 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 128] & (512, 128) & linear & float32 & 65,664 \\\\\n",
      "    \\quad \\quad conv0 (SynthesisLayer) & [1, 64, 512, 512] & 3x3 / 2 (up) & lrelu & float16 & 73,793 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 128] & (512, 128) & linear & float32 & 65,664 \\\\\n",
      "    \\quad \\quad conv1 (SynthesisLayer) & [1, 64, 512, 512] & 3x3 / 1 (up) & lrelu & float16 & 36,929 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 64] & (512, 64) & linear & float32 & 32,832 \\\\\n",
      "    \\quad \\quad torgb (ToRGBLayer) & [1, 1, 512, 512] & 1x1 / 1 & N/A & float16 & 65 \\\\\n",
      "    \\quad \\quad \\quad affine (FullyConnectedLayer) & [1, 64] & (512, 64) & linear & float32 & 32,832 \\\\\n",
      "    \\textbf{mapping} & [1, 16, 512] &  &  & float32 & - \\\\\n",
      "    \\quad fc0 (FullyConnectedLayer) & [1, 512] & (13, 512) & lrelu & float32 & 7,168 \\\\\n",
      "    \\quad fc1 (FullyConnectedLayer) & [1, 512] & (512, 512) & lrelu & float32 & 262,656 \\\\\n",
      "    \\quad fc2 (FullyConnectedLayer) & [1, 512] & (512, 512) & lrelu & float32 & 262,656 \\\\\n",
      "    \\quad fc3 (FullyConnectedLayer) & [1, 512] & (512, 512) & lrelu & float32 & 262,656 \\\\\n",
      "    \\quad fc4 (FullyConnectedLayer) & [1, 512] & (512, 512) & lrelu & float32 & 262,656 \\\\\n",
      "    \\quad fc5 (FullyConnectedLayer) & [1, 512] & (512, 512) & lrelu & float32 & 262,656 \\\\\n",
      "    \\quad fc6 (FullyConnectedLayer) & [1, 512] & (512, 512) & lrelu & float32 & 262,656 \\\\\n",
      "    \\quad fc7 (FullyConnectedLayer) & [1, 512] & (512, 512) & lrelu & float32 & 262,656 \\\\\n",
      "    \\hline\n",
      "    \\textbf{Total} & & & & & \\textbf{30,015,063} \\\\\n",
      "    \\hline\n",
      "  \\end{tabular}\n",
      "\\end{table*}\n",
      "\n",
      "\n",
      "% --- LaTeX Summary for Discriminator (D) (Copy and paste into your .tex file) ---\n",
      "\\begin{table*}[ht]\n",
      "  \\centering\n",
      "  \\caption{Detailed architecture of the Discriminator (D) network.}\n",
      "  \\label{tab:arch_discriminator_(d)}\n",
      "  \\begin{tabular}{l l l l l r}\n",
      "    \\hline\n",
      "    \\textbf{Layer (type)} & \\textbf{Output Shape} & \\textbf{Kernel/Stride} & \\textbf{Activation} & \\textbf{Datatype} & \\textbf{Parameters} \\\\\n",
      "    \\hline\n",
      "    \\quad fromrgb (Conv2dLayer) & [1, 64, 512, 512] & 1x1 / 1 (up) & lrelu & float16 & 64 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 64, 512, 512] & 3x3 / 1 (up) & lrelu & float16 & 36,864 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 128, 256, 256] & 3x3 / 2 (down) & lrelu & float16 & 73,728 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 128, 256, 256] & 1x1 / 2 (down) & linear & float16 & 8,192 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 128, 256, 256] & 3x3 / 1 (up) & lrelu & float16 & 147,456 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 256, 128, 128] & 3x3 / 2 (down) & lrelu & float16 & 294,912 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 256, 128, 128] & 1x1 / 2 (down) & linear & float16 & 32,768 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 256, 128, 128] & 3x3 / 1 (up) & lrelu & float16 & 589,824 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 64, 64] & 3x3 / 2 (down) & lrelu & float16 & 1,179,648 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 64, 64] & 1x1 / 2 (down) & linear & float16 & 131,072 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 512, 64, 64] & 3x3 / 1 (up) & lrelu & float16 & 2,359,296 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 32, 32] & 3x3 / 2 (down) & lrelu & float16 & 2,359,296 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 32, 32] & 1x1 / 2 (down) & linear & float16 & 262,144 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 512, 32, 32] & 3x3 / 1 (up) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 16, 16] & 3x3 / 2 (down) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 16, 16] & 1x1 / 2 (down) & linear & float32 & 262,144 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 512, 16, 16] & 3x3 / 1 (up) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 8, 8] & 3x3 / 2 (down) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 8, 8] & 1x1 / 2 (down) & linear & float32 & 262,144 \\\\\n",
      "    \\quad conv0 (Conv2dLayer) & [1, 512, 8, 8] & 3x3 / 1 (up) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad conv1 (Conv2dLayer) & [1, 512, 4, 4] & 3x3 / 2 (down) & lrelu & float32 & 2,359,296 \\\\\n",
      "    \\quad skip (Conv2dLayer) & [1, 512, 4, 4] & 1x1 / 2 (down) & linear & float32 & 262,144 \\\\\n",
      "    \\quad conv (Conv2dLayer) & [1, 512, 4, 8] & 3x3 / 1 (up) & lrelu & float32 & 2,363,904 \\\\\n",
      "    \\quad fc (FullyConnectedLayer) & [1, 512] & (16384, 512) & lrelu & float32 & 8,389,120 \\\\\n",
      "    \\quad out (FullyConnectedLayer) & [1, 1] & (512, 1) & linear & float32 & 513 \\\\\n",
      "    \\hline\n",
      "    \\textbf{Total} & & & & & \\textbf{33,171,009} \\\\\n",
      "    \\hline\n",
      "  \\end{tabular}\n",
      "\\end{table*}\n",
      "\n",
      "Summary generation complete. You can now copy the LaTeX code into your Overleaf project.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d7d1ddc3b06e25a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
