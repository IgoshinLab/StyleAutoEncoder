{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c9ea69811c009a",
   "metadata": {},
   "source": [
    "# EBI File List Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0c84eaa8558cea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:12:11.359059Z",
     "start_time": "2025-10-01T06:12:11.289675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 Configuration loaded. Starting process...\n",
      "--- Stage 1: Starting File List Build ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/3: Collecting file metadata: 100%|██████████| 5/5 [00:00<00:00, 44810.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files that need MD5 hashing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/3: Hashing MD5 (parallel): 100%|██████████| 2/2 [00:00<00:00, 44.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3: Writing intermediate TSV file...\n",
      "Stage 1 Complete. Wrote 5 rows to /home/xavier/Documents/DAE_project/dataset/Roy_training/classification_model/file_list_generated.tsv\n",
      "\n",
      "Stage 1 finished in 0.05 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # EBI File List Builder (Jupyter Notebook)\n",
    "#\n",
    "# This notebook generates an EBI-compatible TSV file with columns: `Files`, `MD5`, `Type`, `Size`.\n",
    "#\n",
    "# ## Workflow\n",
    "#\n",
    "# This notebook is divided into two main stages:\n",
    "#\n",
    "# 1.  **Stage 1: File Discovery & Hashing**: Scans your directories, finds all files, and calculates their MD5 checksums, size, and type. It produces a raw TSV file.\n",
    "# 2.  **Stage 2: Finalize Output**: Reads the raw TSV file and writes the final output with the four required columns.\n",
    "#\n",
    "# ### Instructions\n",
    "# 1.  **Configure your settings** in the next cell (`Stage 1 User Configuration`). You **must** change the `ROOTS` variable to point to your data directory.\n",
    "# 2.  **Run all cells** in order (e.g., using \"Run All\" in the toolbar).\n",
    "# 3.  The final output will be saved to the file specified in the Stage 2 configuration.\n",
    "\n",
    "# %%\n",
    "# ------------------------------------\n",
    "# Stage 1) User Configuration Cell\n",
    "# ------------------------------------\n",
    "# Edit these variables for the initial file discovery and hashing.\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# --- REQUIRED SETTINGS ---\n",
    "# Add the full paths to the directories you want to scan.\n",
    "ROOTS: List[str] = [\n",
    "    \"/home/xavier/Documents/DAE_project/dataset/Roy_training/classification_model\",\n",
    "    # \"/path/to/your/second_data_folder\", # You can add more paths\n",
    "]\n",
    "\n",
    "# The base path to make all file paths relative to.\n",
    "RELATIVE_TO: str | None = \"/home/xavier/Documents/DAE_project/dataset/Roy_training/classification_model\"\n",
    "\n",
    "# --- OUTPUT AND CACHE (from Stage 1) ---\n",
    "# This is the intermediate file that Stage 2 will use as input.\n",
    "OUT_TSV: str = \"/home/xavier/Documents/DAE_project/dataset/Roy_training/classification_model/file_list_generated.tsv\"\n",
    "CACHE_JSON: str = \".ebi_md5_cache.json\"\n",
    "\n",
    "# --- PERFORMANCE ---\n",
    "WORKERS: int = min(8, (os.cpu_count() or 4))\n",
    "\n",
    "# --- RUNNING MODES (for Stage 1) ---\n",
    "# Set ONE of these to True for the initial run:\n",
    "FAST_MODE: bool = False  # Full build with MD5 hashes.\n",
    "# FAST_MODE: bool = True # Quick build without MD5 hashes.\n",
    "FILL_MD5_MODE: bool = False  # Fills missing MD5s from a previous FAST_MODE run.\n",
    "\n",
    "# --- FILTERS ---\n",
    "EXCLUDE_GLOBS: List[str] = [\"**/*.tmp\", \"**/.DS_Store\"]\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ## Stage 1 Helper Functions & Main Logic\n",
    "# *You generally do not need to edit the code in the cell below.*\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import hashlib\n",
    "import mimetypes\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Iterable\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Optional progress bar (Jupyter-friendly).\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(iterable=None, *args, **kwargs):\n",
    "        if iterable: print(\"tqdm not found. For a progress bar, run: pip install tqdm\")\n",
    "        return iterable if iterable is not None else range(0)\n",
    "\n",
    "mimetypes.add_type('video/x-msvideo', '.avi')\n",
    "MIME_MAP = {\n",
    "    '.zip': 'application/zip', '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "    '.csv': 'text/csv', '.bib': 'text/x-bibtex', '.avi': 'video/x-msvideo', '.pkl': 'application/octet-stream',\n",
    "    '.pt': 'application/octet-stream', '.pth': 'application/octet-stream', '.ckpt': 'application/octet-stream',\n",
    "}\n",
    "\n",
    "\n",
    "def is_tb_event(name: str) -> bool: return name.startswith('events.out.tfevents.')\n",
    "\n",
    "\n",
    "def guess_mime(p: Path) -> str:\n",
    "    sfx = p.suffix.lower()\n",
    "    if sfx in MIME_MAP: return MIME_MAP[sfx]\n",
    "    if is_tb_event(p.name): return 'application/octet-stream'\n",
    "    mime, _ = mimetypes.guess_type(str(p));\n",
    "    return mime or 'application/octet-stream'\n",
    "\n",
    "\n",
    "def file_sig(p: Path) -> tuple[int, float]: st = p.stat(); return st.st_size, st.st_mtime\n",
    "\n",
    "\n",
    "def md5sum(p: Path, blocksize: int = 4 * 1024 * 1024) -> str:\n",
    "    h = hashlib.md5()\n",
    "    try:\n",
    "        with p.open('rb') as f:\n",
    "            for chunk in iter(lambda: f.read(blocksize), b''): h.update(chunk)\n",
    "    except (IOError, PermissionError) as e:\n",
    "        print(f\"Error reading file for MD5: {p} ({e})\");\n",
    "        return \"\"\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def load_cache(cache_path: Path) -> Dict[str, Any]:\n",
    "    if cache_path.exists():\n",
    "        try:\n",
    "            return json.loads(cache_path.read_text(encoding='utf-8'))\n",
    "        except (json.JSONDecodeError, IOError):\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_cache(cache_path: Path, data: Dict[str, Any]) -> None:\n",
    "    tmp_path = cache_path.with_suffix(cache_path.suffix + '.tmp')\n",
    "    try:\n",
    "        tmp_path.write_text(json.dumps(data, indent=2), encoding='utf-8');\n",
    "        tmp_path.replace(cache_path)\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving cache: {e}\")\n",
    "\n",
    "\n",
    "def iter_files(roots: Iterable[Path], exclude_globs: List[str]) -> Iterable[Path]:\n",
    "    for root in roots:\n",
    "        if not root.is_dir(): print(f\"Warning: Root path is not a directory, skipping: {root}\"); continue\n",
    "        for p in root.rglob('*'):\n",
    "            if p.is_file() and not any(p.match(pat) for pat in exclude_globs): yield p\n",
    "\n",
    "\n",
    "def write_tsv(out_path: Path, rows: List[tuple[str, str, str, int]]) -> None:\n",
    "    header = ('Files', 'MD5', 'Type', 'Size')\n",
    "    with out_path.open('w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t');\n",
    "        writer.writerow(header);\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "def build_file_list(\n",
    "        roots: List[str], out_tsv: str, cache_json: str, relative_to: str | None, workers: int, fast: bool,\n",
    "        exclude_globs: List[str] | None\n",
    ") -> int:\n",
    "    print(\"--- Stage 1: Starting File List Build ---\");\n",
    "    roots_p = [Path(r).resolve() for r in roots]\n",
    "    rel_base = Path(relative_to).resolve() if relative_to else Path.cwd()\n",
    "    out_path, cache_path = Path(out_tsv), Path(cache_json)\n",
    "    exclude = exclude_globs or []\n",
    "    cache = load_cache(cache_path);\n",
    "    files = list(iter_files(roots_p, exclude))\n",
    "    rows_data: Dict[str, Dict[str, Any]] = {};\n",
    "    jobs: List[Path] = []\n",
    "    for p in tqdm(files, desc=\"1/3: Collecting file metadata\"):\n",
    "        key = str(p)\n",
    "        try:\n",
    "            rel_path = str(p.relative_to(rel_base));\n",
    "            size, mtime = file_sig(p)\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            print(f\"Skipping {p}: {e}\");\n",
    "            continue\n",
    "        cached_entry = cache.get(key)\n",
    "        is_valid = (cached_entry and cached_entry.get('size') == size and abs(\n",
    "            cached_entry.get('mtime', 0.0) - mtime) < 1e-6)\n",
    "        md5 = ''\n",
    "        if not fast:\n",
    "            if is_valid:\n",
    "                md5 = cached_entry.get('md5', '')\n",
    "            else:\n",
    "                jobs.append(p)\n",
    "        rows_data[key] = {'rel': rel_path, 'md5': md5, 'mime': guess_mime(p), 'size': size}\n",
    "    if jobs:\n",
    "        print(f\"Found {len(jobs)} files that need MD5 hashing.\")\n",
    "        with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "            futures = {ex.submit(md5sum, p): p for p in jobs}\n",
    "            for fut in tqdm(as_completed(futures), total=len(jobs), desc=\"2/3: Hashing MD5 (parallel)\"):\n",
    "                p = futures[fut];\n",
    "                key = str(p)\n",
    "                try:\n",
    "                    md5_result = fut.result()\n",
    "                    if md5_result:\n",
    "                        size, mtime = file_sig(p);\n",
    "                        rows_data[key]['md5'] = md5_result\n",
    "                        cache[key] = {'size': size, 'mtime': mtime, 'md5': md5_result}\n",
    "                except Exception as e:\n",
    "                    print(f\"Error hashing {p}: {e}\")\n",
    "        save_cache(cache_path, cache)\n",
    "    final_rows = [(d['rel'], d['md5'], d['mime'], d['size']) for d in rows_data.values()]\n",
    "    print(\"3/3: Writing intermediate TSV file...\");\n",
    "    write_tsv(out_path, final_rows)\n",
    "    print(f\"Stage 1 Complete. Wrote {len(final_rows)} rows to {out_path}\")\n",
    "    return len(final_rows)\n",
    "\n",
    "\n",
    "def fill_md5_for_existing(out_tsv: str, cache_json: str, relative_to: str | None, workers: int) -> int:\n",
    "    print(\"--- Stage 1: Starting to Fill Missing MD5s ---\");\n",
    "    out_path = Path(out_tsv)\n",
    "    if not out_path.exists(): print(f\"Error: TSV file not found at {out_path}. Run a full build first.\"); return 0\n",
    "    cache_path = Path(cache_json);\n",
    "    cache = load_cache(cache_path)\n",
    "    rel_base = Path(relative_to).resolve() if relative_to else Path.cwd()\n",
    "    jobs: List[Path] = [];\n",
    "    print(\"1/3: Scanning TSV for files needing MD5...\")\n",
    "    try:\n",
    "        with out_path.open('r', encoding='utf-8', newline='') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t');\n",
    "            next(reader)\n",
    "            for row in tqdm(reader, desc=\"Scanning rows\"):\n",
    "                if len(row) >= 2 and not row[1]:\n",
    "                    p = (rel_base / row[0]).resolve()\n",
    "                    if p.exists(): jobs.append(p)\n",
    "    except (IOError, StopIteration) as e:\n",
    "        print(f\"Could not read TSV file: {e}\");\n",
    "        return 0\n",
    "    if not jobs: print(\"No missing MD5s found. File is already complete.\"); return 0\n",
    "    print(f\"Found {len(jobs)} files that need MD5 hashing.\")\n",
    "    md5_results: Dict[str, str] = {}\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        futures = {ex.submit(md5sum, p): p for p in jobs}\n",
    "        for fut in tqdm(as_completed(futures), total=len(jobs), desc=\"2/3: Hashing MD5 (parallel)\"):\n",
    "            p = futures[fut];\n",
    "            key = str(p)\n",
    "            try:\n",
    "                md5_result = fut.result()\n",
    "                if md5_result:\n",
    "                    size, mtime = file_sig(p);\n",
    "                    md5_results[str(p.relative_to(rel_base))] = md5_result\n",
    "                    cache[key] = {'size': size, 'mtime': mtime, 'md5': md5_result}\n",
    "            except Exception as e:\n",
    "                print(f\"Error hashing {p}: {e}\")\n",
    "    save_cache(cache_path, cache)\n",
    "    print(\"3/3: Rewriting TSV with new MD5s...\");\n",
    "    tmp_path = out_path.with_suffix('.tmp')\n",
    "    with out_path.open('r', encoding='utf-8', newline='') as infile, tmp_path.open('w', encoding='utf-8',\n",
    "                                                                                   newline='') as outfile:\n",
    "        reader = csv.reader(infile, delimiter='\\t');\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        writer.writerow(next(reader))\n",
    "        for row in reader:\n",
    "            if len(row) >= 2 and not row[1] and row[0] in md5_results: row[1] = md5_results[row[0]]\n",
    "            writer.writerow(row)\n",
    "    tmp_path.replace(out_path)\n",
    "    print(f\"Fill complete. Updated {len(md5_results)} rows in {out_path}\")\n",
    "    return len(md5_results)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ## ▶️ Execute Stage 1: File Discovery\n",
    "# *This cell runs the first stage based on your configuration above.*\n",
    "\n",
    "# %%\n",
    "def run_stage1():\n",
    "    \"\"\"Checks configuration and runs the appropriate file discovery function.\"\"\"\n",
    "    if not ROOTS or \"/path/to/your/\" in ROOTS[0] or (RELATIVE_TO and \"/path/to/your/\" in RELATIVE_TO):\n",
    "        print(\"=\" * 60 + \"\\n!!! CONFIGURATION NEEDED !!!\\n\" +\n",
    "              \"Please edit the `ROOTS` and `RELATIVE_TO` variables in the first cell\\n\" +\n",
    "              \"to point to your actual data directories before running.\\n\" + \"=\" * 60)\n",
    "        return False\n",
    "    print(\"Stage 1 Configuration loaded. Starting process...\");\n",
    "    start_time = time.time()\n",
    "    if FILL_MD5_MODE:\n",
    "        fill_md5_for_existing(OUT_TSV, CACHE_JSON, RELATIVE_TO, WORKERS)\n",
    "    else:\n",
    "        build_file_list(ROOTS, OUT_TSV, CACHE_JSON, RELATIVE_TO, WORKERS,\n",
    "                        fast=FAST_MODE, exclude_globs=EXCLUDE_GLOBS)\n",
    "    end_time = time.time();\n",
    "    print(f\"\\nStage 1 finished in {end_time - start_time:.2f} seconds.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run the main process\n",
    "stage1_success = run_stage1()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st3-pure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
