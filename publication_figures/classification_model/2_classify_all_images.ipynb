{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model application\n",
    "This notebook will perform the following tasks:\n",
    "1. Split the movies into training set, validation set and test set.\n",
    "2. Apply the classification model to the movie dataset.\n",
    "\n",
    "# Part 1: Train-validation-test split"
   ],
   "id": "2767e12c46a5da5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T19:53:15.799446Z",
     "start_time": "2024-05-31T19:53:15.539196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "IMG_DIR = \"/home/xavier/Documents/dataset/Welch/trainingset2/trainingset2\"\n",
    "OUT_DIR = \"/home/xavier/Documents/dataset/Welch/classification-v2024/classification_models/240430-001/movie_classification\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "classify_df, name_df = [], []\n",
    "phenotype_dict = {}\n",
    "'''Build dataset'''\n",
    "for strain in os.listdir(IMG_DIR):\n",
    "    run_id = int(strain[-4:])\n",
    "    for scope in os.listdir(os.path.join(IMG_DIR, strain)):\n",
    "        scope_id = int(scope[-2:])\n",
    "        directory = f\"{strain}/{scope}\"\n",
    "        name_df.append((run_id, scope_id, directory))\n",
    "\n",
    "name_df = pd.DataFrame(name_df, columns=['run_id', 'scope_id', 'directory'])\n",
    "name_df"
   ],
   "id": "7db44f323b91802b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     run_id  scope_id                            directory\n",
       "0       636        38   CS5_78_0425_1%agar_Run0636/Scope38\n",
       "1       636        37   CS5_78_0425_1%agar_Run0636/Scope37\n",
       "2       636        39   CS5_78_0425_1%agar_Run0636/Scope39\n",
       "3       672         3   CS6_27_1253_1%agar_Run0672/Scope03\n",
       "4       672         2   CS6_27_1253_1%agar_Run0672/Scope02\n",
       "..      ...       ...                                  ...\n",
       "932     287        31  CS1_55_10536_1%agar_Run0287/Scope31\n",
       "933     287        32  CS1_55_10536_1%agar_Run0287/Scope32\n",
       "934     526        30   CS4_81_5257_1%agar_Run0526/Scope30\n",
       "935     526        29   CS4_81_5257_1%agar_Run0526/Scope29\n",
       "936     526        27   CS4_81_5257_1%agar_Run0526/Scope27\n",
       "\n",
       "[937 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>scope_id</th>\n",
       "      <th>directory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>636</td>\n",
       "      <td>38</td>\n",
       "      <td>CS5_78_0425_1%agar_Run0636/Scope38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>636</td>\n",
       "      <td>37</td>\n",
       "      <td>CS5_78_0425_1%agar_Run0636/Scope37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>636</td>\n",
       "      <td>39</td>\n",
       "      <td>CS5_78_0425_1%agar_Run0636/Scope39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>672</td>\n",
       "      <td>3</td>\n",
       "      <td>CS6_27_1253_1%agar_Run0672/Scope03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>672</td>\n",
       "      <td>2</td>\n",
       "      <td>CS6_27_1253_1%agar_Run0672/Scope02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>287</td>\n",
       "      <td>31</td>\n",
       "      <td>CS1_55_10536_1%agar_Run0287/Scope31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>287</td>\n",
       "      <td>32</td>\n",
       "      <td>CS1_55_10536_1%agar_Run0287/Scope32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>526</td>\n",
       "      <td>30</td>\n",
       "      <td>CS4_81_5257_1%agar_Run0526/Scope30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>526</td>\n",
       "      <td>29</td>\n",
       "      <td>CS4_81_5257_1%agar_Run0526/Scope29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>526</td>\n",
       "      <td>27</td>\n",
       "      <td>CS4_81_5257_1%agar_Run0526/Scope27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Split the dataset\n",
    "We choose 100 movies each for validation set and test set. The movies have distinct run_id in each set."
   ],
   "id": "cb1d646cbf1c0a80"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T21:38:19.976355Z",
     "start_time": "2024-04-30T21:38:19.891814Z"
    }
   },
   "source": [
    "VALIDATION_SIZE = 100\n",
    "\n",
    "unique_run_ids = name_df['run_id'].drop_duplicates().sample(n=VALIDATION_SIZE, random_state=70)\n",
    "test_set = name_df[name_df['run_id'].isin(unique_run_ids)].groupby('run_id').apply(\n",
    "    lambda x: x.sample(1, random_state=405)).reset_index(drop=True)\n",
    "\n",
    "mask = pd.merge(name_df, test_set, on=['run_id', 'scope_id'], how='left', indicator=True)\n",
    "remaining_set = mask[mask['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "unique_run_ids = remaining_set['run_id'].drop_duplicates().sample(n=VALIDATION_SIZE, random_state=44)\n",
    "validation_set = remaining_set[remaining_set['run_id'].isin(unique_run_ids)].groupby('run_id').apply(\n",
    "    lambda x: x.sample(1, random_state=1622)).reset_index(drop=True)\n",
    "\n",
    "mask = pd.merge(remaining_set, validation_set, on=['run_id', 'scope_id'], how='left', indicator=True)\n",
    "training_set = mask[mask['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "test_set['Category'] = 'Test'\n",
    "validation_set['Category'] = 'Validation'\n",
    "training_set['Category'] = 'Training'\n",
    "full_dataset = pd.concat([test_set, validation_set, training_set])\n",
    "full_dataset = full_dataset.drop_duplicates(subset=['run_id', 'scope_id'])\n",
    "full_dataset = full_dataset[['run_id', 'scope_id', 'Category']]\n",
    "full_dataset = pd.merge(full_dataset, name_df, on=['run_id', 'scope_id'], how='left')\n",
    "full_dataset.to_csv(f'{OUT_DIR}/generator_full_dataset.csv', index=False)\n",
    "\n",
    "full_dataset"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:38:22.075793Z",
     "start_time": "2024-04-30T21:38:19.977225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ans = 0\n",
    "for index, row in full_dataset.iterrows():\n",
    "    ans += len(os.listdir(os.path.join(IMG_DIR, row['directory'])))\n",
    "print(\"There are \" + str(ans) + \" samples in total.\")"
   ],
   "id": "e2ad407d3b1c8c35",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 2: Classify images\n",
    "We use the pre-trained inception-V3 network and center crop of images to perform classification."
   ],
   "id": "b296a9cc93bf5a20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:57:09.961473Z",
     "start_time": "2024-05-17T20:57:09.950985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dnnlib\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def resize_crop(img_dir, resize_by=1., resolution=512, brightness_norm=False, brightness_mean=107.2, use_rgb=True):\n",
    "    img = cv2.imread(img_dir, cv2.IMREAD_UNCHANGED)\n",
    "    if img.dtype != np.uint8:\n",
    "        img = np.uint8(img / 256)\n",
    "    img_shape = img.shape\n",
    "    resize_shape = np.array([img_shape[1] * resize_by, img_shape[0] * resize_by], dtype=int)\n",
    "    if resize_by != 1:\n",
    "        img = cv2.resize(img, resize_shape, cv2.INTER_LANCZOS4)\n",
    "\n",
    "    if use_rgb and len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    new_img = img[(resize_shape[1] - resolution) // 2:(resize_shape[1] + resolution) // 2,\n",
    "              (resize_shape[0] - resolution) // 2:(resize_shape[0] + resolution) // 2]\n",
    "    if brightness_norm:\n",
    "        obj_v = np.mean(new_img)\n",
    "        value = brightness_mean - obj_v\n",
    "        new_img = cv2.add(new_img, value)\n",
    "    return new_img\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_names):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_names = img_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = resize_crop(os.path.join(self.root_dir, self.img_names[idx]))\n",
    "        img = torch.tensor(img[:, :, :]).permute(2, 0, 1)\n",
    "        return img\n",
    "\n",
    "\n",
    "MODEL_DIR = \"/home/xavier/Documents/dataset/Welch/classification-v2024/classification_models/240430-001\"\n",
    "OUT_DIR = \"/home/xavier/Documents/dataset/Welch/classification-v2024/classification_models/240430-001/movie_classification\"\n",
    "best_epoch = 560\n",
    "BATCH_SIZE = 500\n",
    "TEST_WORKERS = 8\n",
    "checkpoint_interval = 10\n",
    "device = torch.device(\"cuda\")"
   ],
   "id": "74acd1bb32c0ea8",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T00:13:13.067730Z",
     "start_time": "2024-04-30T21:39:43.154028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEMP_DIR = f'{OUT_DIR}/temporary_pred_labels.npy'\n",
    "with dnnlib.util.open_url(os.path.join(MODEL_DIR, f\"model_epoch_{best_epoch:03d}.pkl\"), verbose=True) as f:\n",
    "    detector = pkl.load(f)\n",
    "detector.to(device)\n",
    "detector.eval()\n",
    "\n",
    "all_img_names = []\n",
    "categories = []\n",
    "for index, row in full_dataset.iterrows():\n",
    "    img_folder = f\"{IMG_DIR}/{row['directory']}\"\n",
    "    img_names = [f\"{row['directory']}/{img_name}\" for img_name in os.listdir(img_folder)]\n",
    "    all_img_names.extend(img_names)\n",
    "    categories.extend([row['Category']] * len(img_names))\n",
    "\n",
    "test_dataset = TestDataset(root_dir=IMG_DIR, img_names=all_img_names)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=TEST_WORKERS)\n",
    "\n",
    "if os.path.exists(TEMP_DIR):\n",
    "    pred_labels = list(np.load(TEMP_DIR))\n",
    "    start_batch = len(pred_labels) // BATCH_SIZE\n",
    "else:\n",
    "    pred_labels = []\n",
    "    start_batch = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, images in enumerate(tqdm(test_dataloader, desc=\"Processing images\")):\n",
    "        if i < start_batch:\n",
    "            continue\n",
    "        images = images.to(device)\n",
    "        outputs = detector(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "        if (i + 1) % checkpoint_interval == 0:\n",
    "            np.save(TEMP_DIR, np.array(pred_labels))\n",
    "\n",
    "final_labels = np.array(pred_labels)\n",
    "\n",
    "final_df = pd.DataFrame({\n",
    "    'img_name': all_img_names,\n",
    "    'label': final_labels,\n",
    "    'Category': categories\n",
    "})\n",
    "\n",
    "final_df.to_csv(f'{OUT_DIR}/classified_images.csv', index=False)"
   ],
   "id": "70d98c9fbd27b5e1",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:57:17.812614Z",
     "start_time": "2024-05-17T20:57:16.663698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_df = pd.read_csv(f\"{OUT_DIR}/classified_images.csv\")\n",
    "label_dict = pkl.load(open(f\"{MODEL_DIR}/label_dict.pkl\", \"rb\"))\n",
    "inv_label_dict = {v: k for k, v in label_dict.items()}\n",
    "# final_df['label'] = final_df['label'].map(inv_label_dict)\n",
    "label_counts = final_df.groupby(['label', 'Category']).size().unstack(fill_value=0)\n",
    "\n",
    "total_row = label_counts[['Training', 'Validation', 'Test']].sum().astype(int)\n",
    "total_row.name = 'Total'\n",
    "\n",
    "total_df = pd.DataFrame(total_row).transpose()\n",
    "label_counts = pd.concat([label_counts, total_df])\n",
    "label_counts['Total'] = label_counts[['Training', 'Validation', 'Test']].sum(axis=1).astype(int)\n",
    "label_counts.index = label_counts.index.map(inv_label_dict)\n",
    "label_counts"
   ],
   "id": "294a73b66c516e25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category      Test  Training  Validation    Total\n",
       "Blank        18192    104810       11990   134992\n",
       "Branched      1097      8868         586    10551\n",
       "Clusters      5771     30835        6884    43490\n",
       "Dense         7627     60905        7682    76214\n",
       "Incomplete   23979    181305       21813   227097\n",
       "LWT           7762     61657       11113    80532\n",
       "Large         2965     15941        3318    22224\n",
       "Long          3201     36308        5868    45377\n",
       "Malformed     6780     48926        7497    63203\n",
       "Small         8282     45295        5756    59333\n",
       "Sparse        8639     57608        5191    71438\n",
       "Spots         9843     76902        8695    95440\n",
       "Tiering      39942    331297       47685   418924\n",
       "NaN         144080   1060657      144078  1348815"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Test</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Blank</th>\n",
       "      <td>18192</td>\n",
       "      <td>104810</td>\n",
       "      <td>11990</td>\n",
       "      <td>134992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Branched</th>\n",
       "      <td>1097</td>\n",
       "      <td>8868</td>\n",
       "      <td>586</td>\n",
       "      <td>10551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clusters</th>\n",
       "      <td>5771</td>\n",
       "      <td>30835</td>\n",
       "      <td>6884</td>\n",
       "      <td>43490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense</th>\n",
       "      <td>7627</td>\n",
       "      <td>60905</td>\n",
       "      <td>7682</td>\n",
       "      <td>76214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Incomplete</th>\n",
       "      <td>23979</td>\n",
       "      <td>181305</td>\n",
       "      <td>21813</td>\n",
       "      <td>227097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LWT</th>\n",
       "      <td>7762</td>\n",
       "      <td>61657</td>\n",
       "      <td>11113</td>\n",
       "      <td>80532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Large</th>\n",
       "      <td>2965</td>\n",
       "      <td>15941</td>\n",
       "      <td>3318</td>\n",
       "      <td>22224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Long</th>\n",
       "      <td>3201</td>\n",
       "      <td>36308</td>\n",
       "      <td>5868</td>\n",
       "      <td>45377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malformed</th>\n",
       "      <td>6780</td>\n",
       "      <td>48926</td>\n",
       "      <td>7497</td>\n",
       "      <td>63203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small</th>\n",
       "      <td>8282</td>\n",
       "      <td>45295</td>\n",
       "      <td>5756</td>\n",
       "      <td>59333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sparse</th>\n",
       "      <td>8639</td>\n",
       "      <td>57608</td>\n",
       "      <td>5191</td>\n",
       "      <td>71438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spots</th>\n",
       "      <td>9843</td>\n",
       "      <td>76902</td>\n",
       "      <td>8695</td>\n",
       "      <td>95440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tiering</th>\n",
       "      <td>39942</td>\n",
       "      <td>331297</td>\n",
       "      <td>47685</td>\n",
       "      <td>418924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>144080</td>\n",
       "      <td>1060657</td>\n",
       "      <td>144078</td>\n",
       "      <td>1348815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check sample images",
   "id": "49cf8125c4ba97ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T03:28:24.274982Z",
     "start_time": "2024-05-01T03:28:23.000860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Select one image name for each label\n",
    "sample_images = final_df.groupby('label').first()['img_name']\n",
    "\n",
    "# Setup the plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(sample_images), figsize=(15, 5))\n",
    "\n",
    "# Check if there is only one label, and handle the axis accordingly\n",
    "if len(sample_images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Load and display each image\n",
    "for ax, (label, img_path) in zip(axes, sample_images.items()):\n",
    "    img = Image.open(os.path.join(IMG_DIR, img_path))\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(label)\n",
    "    ax.axis('off')  # Hide the axes ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "62e93657ae1d3c90",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare dictionary for training",
   "id": "a1e2ffaa70804ba0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:59:13.272565Z",
     "start_time": "2024-05-17T20:59:12.692725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dict = {}\n",
    "for phenotype, group in final_df[final_df['Category'] == 'Training'].groupby('label'):\n",
    "    names = group['img_name'].values\n",
    "    training_dict[phenotype] = [f\"{name}\" for name in names]\n",
    "pkl.dump(training_dict, open(os.path.join(OUT_DIR, \"training_dict.pkl\"), \"wb\"))\n",
    "\n",
    "# training_dict_1class = {0: []}\n",
    "# for phenotype, group in final_df[final_df['Category'] == 'Training'].groupby('label'):\n",
    "#     names = group['img_name'].values\n",
    "#     training_dict_1class[0].extend([f\"{name}\" for name in names])\n",
    "# pkl.dump(training_dict_1class, open(os.path.join(OUT_DIR, \"training_dict_1class.pkl\"), \"wb\"))\n",
    "\n",
    "validation_set = {}\n",
    "for phenotype, group in final_df[final_df['Category'] == 'Validation'].groupby('label'):\n",
    "    names = group['img_name'].values\n",
    "    validation_set[phenotype] = [f\"{name}\" for name in names]\n",
    "pkl.dump(validation_set, open(os.path.join(OUT_DIR, \"validation_dict.pkl\"), \"wb\"))\n",
    "\n",
    "test_dict = {}\n",
    "for phenotype, group in final_df[final_df['Category'] == 'Test'].groupby('label'):\n",
    "    names = group['img_name'].values\n",
    "    test_dict[phenotype] = [f\"{name}\" for name in names]\n",
    "pkl.dump(test_dict, open(os.path.join(OUT_DIR, \"test_dict.pkl\"), \"wb\"))"
   ],
   "id": "af28016f303ea0c9",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extra\n",
    "## Sample 100 images in each case to form a smaller validation set"
   ],
   "id": "2d614fd7a0a0e572"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T19:53:45.225591Z",
     "start_time": "2024-05-31T19:53:45.189301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "validation_set = pkl.load(open(os.path.join(OUT_DIR, \"validation_dict.pkl\"), \"rb\"))\n",
    "for item in validation_set:\n",
    "    validation_set[item] = random.sample(validation_set[item], 5)\n",
    "\n",
    "pkl.dump(validation_set, open(os.path.join(OUT_DIR, \"validation_tiny_dict.pkl\"), \"wb\"))"
   ],
   "id": "1612d8fd83330a12",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Merge validation set and test set",
   "id": "1784f50e63c35576"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T23:07:48.566Z",
     "start_time": "2024-05-30T23:07:48.432535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "validation_set = pkl.load(open(os.path.join(OUT_DIR, \"validation_dict.pkl\"), \"rb\"))\n",
    "test_set = pkl.load(open(os.path.join(OUT_DIR, \"test_dict.pkl\"), \"rb\"))\n",
    "for phenotype in validation_set:\n",
    "    validation_set[phenotype].extend(test_set[phenotype])\n",
    "pkl.dump(validation_set, open(os.path.join(OUT_DIR, \"merged_test_dict.pkl\"), \"wb\"))"
   ],
   "id": "b474969038822d68",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Collect Images to a new folder or .zip",
   "id": "59b0106a7a9d3632"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T23:10:02.445124Z",
     "start_time": "2024-05-28T23:06:39.163693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMG_DIR = \"/home/xavier/Documents/dataset/Welch/trainingset2/trainingset2\"\n",
    "OUT_DIR = \"/media/xavier/Storage/feature_extraction/Welch-validation.zip\"\n",
    "DICT_DIR = \"/home/xavier/Documents/dataset/Welch/classification-v2024/classification_models/240430-001/movie_classification/validation_dict.pkl\"\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "# Function to monitor memory and pause if threshold is exceeded\n",
    "def monitor_memory(threshold_mb=20971):  # Set threshold to 30 GB\n",
    "    while True:\n",
    "        usage_mb = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "        if usage_mb < threshold_mb:\n",
    "            return\n",
    "        print(f\"Current memory usage is {usage_mb:.2f} MB, pausing until memory drops below threshold...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file_path, base_folder, zipf, lock):\n",
    "    arcname = file_path[len(base_folder.rstrip(os.sep)) + 1:]\n",
    "    with lock:\n",
    "        zipf.write(file_path, arcname=arcname)\n",
    "\n",
    "\n",
    "# Function to process files in batches with indexed progress bars\n",
    "def process_files_in_batches(files, base_folder, zipf, lock, batch_size=2000):\n",
    "    # Process files in batches\n",
    "    total_batches = (len(files) + batch_size - 1) // batch_size\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_index = i // batch_size + 1\n",
    "        batch = files[i:i + batch_size]\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = {executor.submit(process_file, file, base_folder, zipf, lock): file for file in batch}\n",
    "            with tqdm(as_completed(futures), total=len(futures),\n",
    "                      desc=f\"Batch {batch_index}/{total_batches} - Zipping files\", miniters=1) as progress:\n",
    "                for future in progress:\n",
    "                    future.result()  # Wait for the future to complete\n",
    "        monitor_memory()  # Check memory after each batch\n",
    "\n",
    "\n",
    "# Main function to zip directory\n",
    "def zip_directory(base_folder, dictionary_dir, zip_filename):\n",
    "    files_to_zip = []\n",
    "    sampled_dict = pkl.load(open(dictionary_dir, \"rb\"))\n",
    "    for group in sampled_dict:\n",
    "        files_to_zip.extend([os.path.join(base_folder, img_name) for img_name in sampled_dict[group]])\n",
    "\n",
    "    with zipfile.ZipFile(zip_filename, 'w', compression=zipfile.ZIP_STORED) as zipf:\n",
    "        from threading import Lock\n",
    "        lock = Lock()  # Lock for thread-safe writing to the zipfile\n",
    "        process_files_in_batches(files_to_zip, base_folder, zipf, lock)\n",
    "\n",
    "\n",
    "zip_directory(IMG_DIR, DICT_DIR, OUT_DIR)"
   ],
   "id": "13b2f812a0b048f5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1386.38it/s]\n",
      "Batch 2/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1409.37it/s]\n",
      "Batch 3/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1213.96it/s]\n",
      "Batch 4/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1010.65it/s]\n",
      "Batch 5/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1277.49it/s]\n",
      "Batch 6/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 997.87it/s] \n",
      "Batch 7/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 681.37it/s]\n",
      "Batch 8/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 825.51it/s] \n",
      "Batch 9/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 844.57it/s] \n",
      "Batch 10/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 746.62it/s] \n",
      "Batch 11/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 515.49it/s]\n",
      "Batch 12/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 575.19it/s]\n",
      "Batch 13/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 542.03it/s]\n",
      "Batch 14/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 630.91it/s]\n",
      "Batch 15/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 623.85it/s]\n",
      "Batch 16/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 709.90it/s] \n",
      "Batch 17/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 792.84it/s]\n",
      "Batch 18/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1206.05it/s]\n",
      "Batch 19/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 781.20it/s]\n",
      "Batch 20/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 597.60it/s]\n",
      "Batch 21/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 604.96it/s]\n",
      "Batch 22/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 713.77it/s]\n",
      "Batch 23/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 625.15it/s] \n",
      "Batch 24/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 775.24it/s] \n",
      "Batch 25/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 711.54it/s]\n",
      "Batch 26/73 - Zipping files: 100%|██████████| 2000/2000 [00:04<00:00, 476.80it/s]\n",
      "Batch 27/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 539.82it/s]\n",
      "Batch 28/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 648.82it/s]\n",
      "Batch 29/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 735.30it/s]\n",
      "Batch 30/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 748.88it/s]\n",
      "Batch 31/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 802.48it/s]\n",
      "Batch 32/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 709.97it/s]\n",
      "Batch 33/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 753.17it/s]\n",
      "Batch 34/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 816.74it/s] \n",
      "Batch 35/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 681.53it/s]\n",
      "Batch 36/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 535.41it/s]\n",
      "Batch 37/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 664.14it/s]\n",
      "Batch 38/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 608.98it/s]\n",
      "Batch 39/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 609.87it/s]\n",
      "Batch 40/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 552.97it/s]\n",
      "Batch 41/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1091.57it/s]\n",
      "Batch 42/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 677.61it/s]\n",
      "Batch 43/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 636.04it/s]\n",
      "Batch 44/73 - Zipping files: 100%|██████████| 2000/2000 [00:04<00:00, 498.67it/s]\n",
      "Batch 45/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 599.75it/s]\n",
      "Batch 46/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 581.32it/s]\n",
      "Batch 47/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 835.71it/s] \n",
      "Batch 48/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 583.87it/s]\n",
      "Batch 49/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 575.15it/s]\n",
      "Batch 50/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 913.23it/s] \n",
      "Batch 51/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 648.99it/s]\n",
      "Batch 52/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 918.46it/s] \n",
      "Batch 53/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 899.73it/s] \n",
      "Batch 54/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1138.16it/s]\n",
      "Batch 55/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 708.48it/s]\n",
      "Batch 56/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 905.71it/s] \n",
      "Batch 57/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 804.07it/s] \n",
      "Batch 58/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 581.50it/s]\n",
      "Batch 59/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 671.68it/s]\n",
      "Batch 60/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 657.66it/s]\n",
      "Batch 61/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 773.06it/s]\n",
      "Batch 62/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 852.08it/s] \n",
      "Batch 63/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 546.86it/s]\n",
      "Batch 64/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 611.32it/s]\n",
      "Batch 65/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 818.71it/s]\n",
      "Batch 66/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1133.88it/s]\n",
      "Batch 67/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1071.30it/s]\n",
      "Batch 68/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 666.61it/s]\n",
      "Batch 69/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 814.16it/s] \n",
      "Batch 70/73 - Zipping files: 100%|██████████| 2000/2000 [00:03<00:00, 659.34it/s]\n",
      "Batch 71/73 - Zipping files: 100%|██████████| 2000/2000 [00:02<00:00, 817.61it/s] \n",
      "Batch 72/73 - Zipping files: 100%|██████████| 2000/2000 [00:01<00:00, 1035.49it/s]\n",
      "Batch 73/73 - Zipping files: 100%|██████████| 78/78 [00:00<00:00, 1052.34it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "340b933602fcf351"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
