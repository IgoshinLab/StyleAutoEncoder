{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:19:59.424607Z",
     "start_time": "2025-11-05T15:19:59.421842Z"
    }
   },
   "cell_type": "code",
   "source": "WORKING_DIR = \"/home/xavier/Documents/DAE_project\"",
   "id": "55eab237e2cb350a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Process tables",
   "id": "2bb31c3b05de2d0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Merge motility labels",
   "id": "be6715129a8bc643"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T22:38:30.589459Z",
     "start_time": "2025-10-12T22:38:27.214923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "WORKING_DIR = \"/home/xavier/Documents/DAE_project\"\n",
    "\n",
    "# --- File Paths ---\n",
    "EXPERIMENT_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/Caro_3d_9.7.22_2.20_new.xlsx'\n",
    "UPDATED_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/Updated_with_He_et_al__1994.xlsx'\n",
    "KAISER_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/Kaiser_strain_list_at_UCD.xlsx'\n",
    "IMAGES_BASE_PATH = f'{WORKING_DIR}/dataset/Roy_training/images/'\n",
    "\n",
    "OUTPUT_FULL_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/merged_strain_data_full.xlsx'\n",
    "OUTPUT_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/merged_strain_data.xlsx'\n",
    "\n",
    "# --- Data Loading ---\n",
    "try:\n",
    "    print(\"Loading data files...\")\n",
    "    experiment_df = pd.read_excel(EXPERIMENT_FILE_PATH)\n",
    "    updated_df = pd.read_excel(UPDATED_FILE_PATH)\n",
    "    kaiser_df = pd.read_excel(KAISER_FILE_PATH)\n",
    "    print(\"All files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure all files are in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Processing the 'Updated' Table ---\n",
    "print(\"Processing the 'Updated' table...\")\n",
    "\n",
    "# 1. Select and filter columns\n",
    "updated_df_processed = updated_df[\n",
    "    ['Run', 'Mutant #', 'Movies', 'Reference', 'Jiangguo', 'Source_labelled', 'Source', 'Bib']].copy()\n",
    "\n",
    "# 2. Keep rows where 'Run' has a value and remove duplicates\n",
    "updated_df_processed.dropna(subset=['Run'], inplace=True)\n",
    "updated_df_processed.drop_duplicates(inplace=True)\n",
    "print(f\"Filtered down to {len(updated_df_processed)} unique rows with 'Run' values.\")\n",
    "\n",
    "\n",
    "# 3. Clean the 'Mutant #' column to create 'Strain'\n",
    "def clean_mutant_id(mutant_id):\n",
    "    \"\"\"\n",
    "    Cleans the Mutant ID based on specified rules:\n",
    "    - If an entry has a letter-digit pattern (e.g., DK1622A), it keeps the pattern and removes the rest (-> DK1622).\n",
    "    - If an entry starts with a digit, it prepends 'DK'.\n",
    "    \"\"\"\n",
    "    if pd.isna(mutant_id):\n",
    "        return None\n",
    "\n",
    "    mutant_id_str = str(mutant_id)\n",
    "    parts = [part.strip() for part in mutant_id_str.split(',')]\n",
    "\n",
    "    for part in parts:\n",
    "        if not part:\n",
    "            continue\n",
    "        match = re.match(r'(DK\\d+)', part)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        # Check if the first character is a digit\n",
    "        if part and part[0].isdigit():\n",
    "            return f'DK{part}'\n",
    "    return parts[0] if parts else None\n",
    "\n",
    "\n",
    "updated_df_processed['Strain'] = updated_df_processed['Mutant #'].apply(clean_mutant_id)\n",
    "print(\"Cleaned 'Mutant #' column into 'Strain'.\")\n",
    "\n",
    "\n",
    "# --- Calculate Final Movie Count from Directories ---\n",
    "def calculate_final_movies(run_ids_str, images_path):\n",
    "    \"\"\"\n",
    "    Calculates the total number of movies by counting subdirectories for each run ID.\n",
    "    \"\"\"\n",
    "    if pd.isna(run_ids_str) or not os.path.isdir(images_path):\n",
    "        return 0\n",
    "\n",
    "    total_movies = 0\n",
    "    try:\n",
    "        run_ids = [int(float(run_id.strip())) for run_id in str(run_ids_str).split(',') if run_id.strip()]\n",
    "    except (ValueError, AttributeError):\n",
    "        return 0\n",
    "\n",
    "    # Get a list of all items in the images directory once for efficiency\n",
    "    all_image_folders = os.listdir(images_path)\n",
    "\n",
    "    for run_id in run_ids:\n",
    "        # Pad run_id to 4 digits to match folder naming convention, e.g., 1 -> 0001\n",
    "        run_id_str_padded = str(run_id).zfill(4)\n",
    "        for folder_name in all_image_folders:\n",
    "            if folder_name.endswith(run_id_str_padded):\n",
    "                run_folder_path = os.path.join(images_path, folder_name)\n",
    "                if os.path.isdir(run_folder_path):\n",
    "                    # Count subdirectories inside the run folder\n",
    "                    num_subfolders = sum(\n",
    "                        os.path.isdir(os.path.join(run_folder_path, item)) for item in os.listdir(run_folder_path))\n",
    "                    total_movies += num_subfolders\n",
    "                    break  # Move to the next run_id once the folder is found\n",
    "    return total_movies\n",
    "\n",
    "\n",
    "print(\"Calculating final movie counts from image directories...\")\n",
    "updated_df_processed['Final Movies'] = updated_df_processed['Run'].apply(\n",
    "    lambda run_ids: calculate_final_movies(run_ids, IMAGES_BASE_PATH)\n",
    ")\n",
    "print(\"Added 'Final Movies' column.\")\n",
    "\n",
    "# --- Retrieving Original Data from Experiment Table ---\n",
    "print(\"Looking up original data from the experiment file...\")\n",
    "experiment_df.dropna(subset=['Run'], inplace=True)\n",
    "experiment_df['Run'] = experiment_df['Run'].astype(int)\n",
    "\n",
    "\n",
    "def get_original_info(run_ids_str, exp_df):\n",
    "    \"\"\"\n",
    "    Looks up run IDs in the experiment_df and concatenates original mutant numbers and sources.\n",
    "    \"\"\"\n",
    "    if pd.isna(run_ids_str):\n",
    "        return pd.Series([None, None], index=['Original Mutant #', 'Original Source'])\n",
    "\n",
    "    try:\n",
    "        run_ids = [int(float(run_id.strip())) for run_id in str(run_ids_str).split(',') if run_id.strip()]\n",
    "    except (ValueError, AttributeError):\n",
    "        return pd.Series([None, None], index=['Original Mutant #', 'Original Source'])\n",
    "\n",
    "    matches = exp_df[exp_df['Run'].isin(run_ids)]\n",
    "    if matches.empty:\n",
    "        return pd.Series([None, None], index=['Original Mutant #', 'Original Source'])\n",
    "\n",
    "    original_mutants = ','.join(matches['Mutant #'].dropna().astype(str).unique())\n",
    "    original_sources = ','.join(matches['Source'].dropna().astype(str).unique())\n",
    "    return pd.Series([original_mutants, original_sources], index=['Original Mutant #', 'Original Source'])\n",
    "\n",
    "\n",
    "updated_df_processed[['Original Mutant #', 'Original Source']] = updated_df_processed['Run'].apply(\n",
    "    lambda run_ids: get_original_info(run_ids, experiment_df)\n",
    ")\n",
    "print(\"Added 'Original Mutant #' and 'Original Source' columns.\")\n",
    "\n",
    "# --- Processing the 'Kaiser' Table ---\n",
    "print(\"Processing the 'Kaiser' table...\")\n",
    "# To prevent row duplication during the merge, we must ensure 'DK#' is unique in the Kaiser table.\n",
    "# We will group by 'DK#' and aggregate the information from other columns.\n",
    "\n",
    "kaiser_df_processed = kaiser_df[['DK#', 'genotype', 'phenotype', 'References']].copy()\n",
    "# Drop rows where DK# is null as they can't be used for merging\n",
    "kaiser_df_processed.dropna(subset=['DK#'], inplace=True)\n",
    "\n",
    "# Convert all relevant columns to string to prevent aggregation errors with mixed types\n",
    "for col in ['genotype', 'phenotype', 'References']:\n",
    "    kaiser_df_processed[col] = kaiser_df_processed[col].astype(str)\n",
    "\n",
    "# Group by 'DK#' and aggregate the other columns by joining unique, non-null values\n",
    "kaiser_df_processed = kaiser_df_processed.groupby('DK#').agg({\n",
    "    'genotype': lambda x: ', '.join(x.replace('nan', '').dropna().unique()),\n",
    "    'phenotype': lambda x: ', '.join(x.replace('nan', '').dropna().unique()),\n",
    "    'References': lambda x: ', '.join(x.replace('nan', '').dropna().unique())\n",
    "}).reset_index()\n",
    "print(\"Aggregated Kaiser table to ensure unique DK# entries, preventing duplicates in final output.\")\n",
    "\n",
    "# --- Merging the Tables ---\n",
    "print(\"Joining the Updated and Kaiser tables...\")\n",
    "merged_df = pd.merge(\n",
    "    updated_df_processed,\n",
    "    kaiser_df_processed,\n",
    "    left_on='Strain',\n",
    "    right_on='DK#',\n",
    "    how='left'\n",
    ")\n",
    "print(\"Join complete.\")\n",
    "\n",
    "\n",
    "# --- Adding Motility Column ---\n",
    "def determine_motility(row):\n",
    "    \"\"\"Determines motility label based on 'Jiangguo' and 'phenotype' columns.\"\"\"\n",
    "    jiangguo_label = None\n",
    "    phenotype_label = None\n",
    "\n",
    "    if pd.notna(row['Jiangguo']):\n",
    "        jg_val = str(row['Jiangguo']).strip()\n",
    "        if jg_val in ['WT', 'A-S+', 'A+S-', 'A-S-']:\n",
    "            jiangguo_label = jg_val\n",
    "\n",
    "    if pd.notna(row['phenotype']):\n",
    "        ph_val = str(row['phenotype'])\n",
    "        if 'A-S+' in ph_val:\n",
    "            phenotype_label = 'A-S+'\n",
    "        elif 'A+S-' in ph_val:\n",
    "            phenotype_label = 'A+S-'\n",
    "        elif 'A-S-' in ph_val:\n",
    "            phenotype_label = 'A-S-'\n",
    "        elif 'A-' in ph_val:\n",
    "            phenotype_label = 'A-S+'\n",
    "        elif 'S-' in ph_val:\n",
    "            phenotype_label = 'A+S-'\n",
    "\n",
    "    final_label = jiangguo_label\n",
    "    if phenotype_label:\n",
    "        if jiangguo_label and jiangguo_label != phenotype_label:\n",
    "            final_label = f\"{phenotype_label}\"\n",
    "        else:\n",
    "            final_label = phenotype_label\n",
    "    return final_label\n",
    "\n",
    "\n",
    "print(\"Adding 'motility' column...\")\n",
    "merged_df['motility'] = merged_df.apply(determine_motility, axis=1)\n",
    "\n",
    "# --- Final Column Selection and Ordering ---\n",
    "print(\"Reordering and selecting final columns...\")\n",
    "final_columns = [\n",
    "    'Strain', 'Run', 'Movies', 'Final Movies',  # Use the new 'Final Movies' column\n",
    "    'Original Mutant #', 'Original Source',\n",
    "    'genotype', 'phenotype', 'Reference', 'motility',\n",
    "    'Source_labelled', 'Source', 'References', 'Bib'\n",
    "]\n",
    "final_columns_exist = [col for col in final_columns if col in merged_df.columns]\n",
    "final_df = merged_df[final_columns_exist]\n",
    "# Filter out rows where 'Final Movies' is 0\n",
    "final_df = final_df[final_df['Final Movies'] != 0].copy()\n",
    "\n",
    "# Sort the final DataFrame by 'Strain'\n",
    "print(\"Sorting final data by 'Strain'...\")\n",
    "final_df.sort_values(by='Strain', inplace=True)\n",
    "final_df[\"Bib\"] = final_df[\"Bib\"].fillna(\"caro2023myxococcus\")\n",
    "\n",
    "# --- Saving the Result ---\n",
    "final_df.to_excel(OUTPUT_FULL_FILE_PATH, index=False)\n",
    "final_df[['Strain', 'Run', 'Final Movies', 'motility', 'Bib']].to_excel(OUTPUT_FILE_PATH, index=False)\n",
    "print(f\"Successfully created the final output file: {OUTPUT_FILE_PATH}\")"
   ],
   "id": "622a72a20f343df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "All files loaded successfully.\n",
      "Processing the 'Updated' table...\n",
      "Filtered down to 439 unique rows with 'Run' values.\n",
      "Cleaned 'Mutant #' column into 'Strain'.\n",
      "Calculating final movie counts from image directories...\n",
      "Added 'Final Movies' column.\n",
      "Looking up original data from the experiment file...\n",
      "Added 'Original Mutant #' and 'Original Source' columns.\n",
      "Processing the 'Kaiser' table...\n",
      "Aggregated Kaiser table to ensure unique DK# entries, preventing duplicates in final output.\n",
      "Joining the Updated and Kaiser tables...\n",
      "Join complete.\n",
      "Adding 'motility' column...\n",
      "Reordering and selecting final columns...\n",
      "Sorting final data by 'Strain'...\n",
      "Successfully created the final output file: /home/xavier/Documents/DAE_project/dataset/Roy_training/merged_strain_data.xlsx\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert to Latex table",
   "id": "ef57f2a51be7a333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:09:26.949644Z",
     "start_time": "2025-10-13T04:09:26.897014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_bib_to_latex(bib_string):\n",
    "    \"\"\"\n",
    "    Formats a comma-separated string of references into a single\n",
    "    LaTeX \\\\cite{ref1, ref2} command.\n",
    "    \"\"\"\n",
    "    if not isinstance(bib_string, str):\n",
    "        return ''\n",
    "    # Split by comma, allowing for an optional space after it\n",
    "    references = re.split(r',\\s*', bib_string)\n",
    "    # Clean up and filter out any empty strings\n",
    "    cleaned_references = [ref.strip() for ref in references if ref.strip()]\n",
    "    # Join the references with a comma and space\n",
    "    joined_references = ', '.join(cleaned_references)\n",
    "    # If there are any references, wrap them in a single \\cite{} command\n",
    "    if joined_references:\n",
    "        return f\"\\\\cite{{{joined_references}}}\"\n",
    "    return ''\n",
    "\n",
    "\n",
    "# --- Start of Transformation ---\n",
    "\n",
    "# 2. Create a copy to avoid modifying the original DataFrame\n",
    "df = final_df[['Strain', 'Final Movies', 'motility', 'Bib']].copy()\n",
    "\n",
    "# 3. Sort the DataFrame by 'motility' in descending order\n",
    "# pandas will automatically place NaN values at the end of the sort\n",
    "df = df.sort_values(by='motility', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 4. Rename the 'Final Movies' column to 'Number of Movies'\n",
    "df = df.rename(columns={'Final Movies': 'Number of Movies'})\n",
    "\n",
    "# 5. Apply the formatting function to the 'Bib' column\n",
    "df['Bib'] = df['Bib'].apply(format_bib_to_latex)\n",
    "\n",
    "# 6. Replace any NaN/None motility values with a hyphen for display\n",
    "df['motility'] = df['motility'].fillna('-')\n",
    "\n",
    "# 7. Calculate the total number of movies for the summary\n",
    "total_movies = df['Number of Movies'].sum()\n",
    "\n",
    "# --- LaTeX Conversion ---\n",
    "\n",
    "# 8. Convert DataFrame to a standard LaTeX tabular string using the Styler.\n",
    "# We will manually parse this string and wrap it in a 'longtable' environment.\n",
    "styler = df.style.format(formatter={'Bib': lambda val: val})\n",
    "styler.hide(axis=\"index\")  # Use the hide() method to remove the index.\n",
    "tabular_string = styler.to_latex(hrules=True)  # Generate a standard table\n",
    "\n",
    "# 9. Manually parse the tabular string and build the longtable.\n",
    "lines = tabular_string.strip().split('\\n')\n",
    "try:\n",
    "    # Extract the column specification (e.g., {lccc})\n",
    "    col_spec = lines[0].replace('\\\\begin{tabular}', '').strip()\n",
    "    # Extract the header (from \\toprule to \\midrule)\n",
    "    header_end_index = lines.index('\\\\midrule')\n",
    "    table_header = '\\n'.join(lines[1:header_end_index + 1])\n",
    "    # Extract the table body (rows between \\midrule and \\bottomrule)\n",
    "    table_body = '\\n'.join(lines[header_end_index + 1:-2])\n",
    "except (ValueError, IndexError):\n",
    "    print(\"Error: Could not parse the generated LaTeX table structure.\")\n",
    "    final_latex_table = tabular_string  # Fallback to the original table\n",
    "else:\n",
    "    # Prepare the summary row for the table's final footer\n",
    "    summary_row = f\"\\\\midrule\\n\\\\textbf{{Total}} & {total_movies} & & \\\\\\\\\"\n",
    "\n",
    "    # Assemble the final longtable string. This structure defines the headers\n",
    "    # and footers that longtable will use automatically across pages.\n",
    "    longtable_parts = [\n",
    "        f\"\\\\begin{{longtable}}{col_spec}\",\n",
    "        \"\\\\caption{Strain Motility Summary}\\\\\\\\\",\n",
    "        table_header,\n",
    "        \"\\\\endfirsthead\",  # End of header for the first page\n",
    "        \"\\\\caption[]{-- continued from previous page}\\\\\\\\\",\n",
    "        table_header,\n",
    "        \"\\\\endhead\",  # End of header for all subsequent pages\n",
    "        \"\\\\bottomrule\\n\\\\multicolumn{4}{r}{{Continued on next page}} \\\\\\\\\",\n",
    "        \"\\\\endfoot\",  # Footer for all pages except the last\n",
    "        summary_row,\n",
    "        \"\\\\bottomrule\",\n",
    "        \"\\\\endlastfoot\",  # Footer for the very last page\n",
    "        table_body,  # The main data rows of the table\n",
    "        \"\\\\end{longtable}\"\n",
    "    ]\n",
    "    final_latex_table = '\\n'.join(longtable_parts)\n",
    "\n",
    "# 10. Wrap the entire table in a 'center' environment to center it on the page.\n",
    "final_latex_table = f\"\\\\begin{{center}}\\n{final_latex_table}\\n\\\\end{{center}}\"\n",
    "\n",
    "# 11. Print the final LaTeX table string\n",
    "print(\"--- Generated LaTeX Table ---\")\n",
    "print(final_latex_table)\n",
    "\n",
    "# --- Example of how to write to a file ---\n",
    "output_path = f'{WORKING_DIR}/motility_table.tex'\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(final_latex_table)\n",
    "print(f\"\\nSuccessfully saved to {output_path}\")\n",
    "\n"
   ],
   "id": "448ab15eb9a2f355",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated LaTeX Table ---\n",
      "\\begin{center}\n",
      "\\begin{longtable}{lrll}\n",
      "\\caption{Strain Motility Summary}\\\\\n",
      "\\toprule\n",
      "Strain & Number of Movies & motility & Bib \\\\\n",
      "\\midrule\n",
      "\\endfirsthead\n",
      "\\caption[]{-- continued from previous page}\\\\\n",
      "\\toprule\n",
      "Strain & Number of Movies & motility & Bib \\\\\n",
      "\\midrule\n",
      "\\endhead\n",
      "\\bottomrule\n",
      "\\multicolumn{4}{r}{{Continued on next page}} \\\\\n",
      "\\endfoot\n",
      "\\midrule\n",
      "\\textbf{Total} & 937 & & \\\\\n",
      "\\bottomrule\n",
      "\\endlastfoot\n",
      "DK101 & 9 & WT & \\cite{hodgkin1979genetics, cheng1989dsg} \\\\\n",
      "DK1622 & 13 & WT & \\cite{caro2023myxococcus} \\\\\n",
      "DK3119 & 3 & A-S- & \\cite{dana1993regulation} \\\\\n",
      "DK4160 & 3 & A-S- & \\cite{rodriguez1997tgl} \\\\\n",
      "DK2160 & 3 & A-S- & \\cite{wu1997myxococcus} \\\\\n",
      "DK1259 & 2 & A-S- & \\cite{hodgkin1979genetics, kaiser1979social} \\\\\n",
      "DK1409 & 3 & A-S- & \\cite{caro2023myxococcus} \\\\\n",
      "DK4050 & 2 & A-S- & \\cite{stephens1987genetics} \\\\\n",
      "DK4153 & 3 & A-S- & \\cite{stephens1987genetics, stephens1989gliding} \\\\\n",
      "DK2618 & 3 & A-S+ & \\cite{shimkets1982induction} \\\\\n",
      "DK2616 & 3 & A-S+ & \\cite{shimkets1986correlation} \\\\\n",
      "DK2614 & 2 & A-S+ & \\cite{dana1993regulation} \\\\\n",
      "DK2608 & 3 & A-S+ & \\cite{dana1993regulation} \\\\\n",
      "DK2232 & 3 & A-S+ & \\cite{fontes1999myxococcus} \\\\\n",
      "DK1213 & 3 & A-S+ & \\cite{hodgkin1979genetics} \\\\\n",
      "DK1215 & 3 & A-S+ & \\cite{hodgkin1979genetics} \\\\\n",
      "DK1217 & 3 & A-S+ & \\cite{hodgkin1979genetics} \\\\\n",
      "DK1410 & 3 & A-S+ & \\cite{caro2023myxococcus} \\\\\n",
      "DK7679 & 3 & A-S+ & \\cite{caro2023myxococcus} \\\\\n",
      "DK1227 & 3 & A-S+ & \\cite{hodgkin1979genetics} \\\\\n",
      "DK1224 & 3 & A-S+ & \\cite{caro2023myxococcus} \\\\\n",
      "DK1221 & 3 & A-S+ & \\cite{hodgkin1979genetics} \\\\\n",
      "DK1220 & 3 & A-S+ & \\cite{hodgkin1979genetics} \\\\\n",
      "DK1218 & 3 & A-S+ & \\cite{hodgkin1979genetics} \\\\\n",
      "DK3475 & 3 & A+S- & \\cite{dana1993regulation} \\\\\n",
      "DK8615 & 3 & A+S- & \\cite{wei2011heterologous} \\\\\n",
      "DK3481 & 3 & A+S- & \\cite{shimkets1986correlation} \\\\\n",
      "DK7558 & 2 & A+S- & \\cite{keseler1996sigma} \\\\\n",
      "DK2158 & 3 & A+S- & \\cite{wu1997myxococcus} \\\\\n",
      "DK3186 & 3 & A+S- & \\cite{chen1991physical} \\\\\n",
      "DK3068 & 3 & A+S- & \\cite{kuspa1989genes} \\\\\n",
      "DK2112 & 2 & A+S- & \\cite{wu1997myxococcus} \\\\\n",
      "DK1932 & 2 & A+S- & \\cite{rodriguez1999genetic} \\\\\n",
      "DK1680 & 3 & A+S- & \\cite{avery1983situ, ward1999motility} \\\\\n",
      "DK1670 & 3 & A+S- & \\cite{wu1997myxococcus} \\\\\n",
      "DK1300 & 6 & A+S- & \\cite{sun1999effect} \\\\\n",
      "DK1253 & 6 & A+S- & \\cite{kaiser1979social} \\\\\n",
      "DK8620 & 3 & A+S- & \\cite{caro2023myxococcus} \\\\\n",
      "ASX1 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK1013 & 2 & - & \\cite{shimkets1986correlation} \\\\\n",
      "DK1016 & 3 & - & \\cite{shimkets1986role} \\\\\n",
      "DK1031 & 2 & - & \\cite{shimkets1986correlation} \\\\\n",
      "DK10524 & 3 & - & \\cite{licking2000common} \\\\\n",
      "DK10527 & 3 & - & \\cite{licking2000common} \\\\\n",
      "DK10536 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK10546 & 3 & - & \\cite{asp2023mapping} \\\\\n",
      "DK10603 & 1 & - & \\cite{yoder2004mutational} \\\\\n",
      "DK10604 & 3 & - & \\cite{gronewold2007mutations} \\\\\n",
      "DK1061 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK11212 & 3 & - & \\cite{licking2000common} \\\\\n",
      "DK119 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK121 & 2 & - & \\cite{shi1993two} \\\\\n",
      "DK129 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK149 & 3 & - & \\cite{manoil1980guanosine} \\\\\n",
      "DK1525 & 3 & - & \\cite{shimkets1982induction} \\\\\n",
      "DK1529 & 3 & - & \\cite{larossa1983developmental} \\\\\n",
      "DK1627 & 2 & - & \\cite{wall1999myxococcus} \\\\\n",
      "DK1648 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK1666 & 3 & - & \\cite{wu1997myxococcus} \\\\\n",
      "DK1669 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK2109 & 2 & - & \\cite{troselj2020conditional} \\\\\n",
      "DK2110 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK2118 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK2134 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK2138 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK2143 & 1 & - & \\cite{wu1998pilh} \\\\\n",
      "DK2161 & 1 & - & \\cite{wu1997myxococcus} \\\\\n",
      "DK2210 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK2213 & 2 & - & \\cite{chen1991physical} \\\\\n",
      "DK2231 & 2 & - & \\cite{nudleman2004polar} \\\\\n",
      "DK233 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK2737 & 1 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK315 & 3 & - & \\cite{hodgkin1977cell} \\\\\n",
      "DK3151 & 3 & - & \\cite{chen1991physical} \\\\\n",
      "DK3250 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK3252 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK3258 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK3261 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK3354 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK3430 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK3468 & 2 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK3516 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK3517 & 3 & - & \\cite{chatwin1994molecular} \\\\\n",
      "DK3517 & 3 & - & \\cite{chatwin1994molecular} \\\\\n",
      "DK3518 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK3551 & 3 & - & \\cite{chatwin1994molecular} \\\\\n",
      "DK3619 & 3 & - & \\cite{stephens1987genetics} \\\\\n",
      "DK3955 & 5 & - & \\cite{rosenbluh1992effect} \\\\\n",
      "DK3959 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK406 & 3 & - & \\cite{mcgowan1993light} \\\\\n",
      "DK412 & 2 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK4141 & 3 & - & \\cite{kroos1988link} \\\\\n",
      "DK4162 & 3 & - & \\cite{rodriguez1997tgl} \\\\\n",
      "DK418 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK425 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK426 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK4285 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK429 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK4290 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK4292 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK4293 & 6 & - & \\cite{wall1999myxococcus} \\\\\n",
      "DK4296 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK4297 & 3 & - & \\cite{kroos1986global} \\\\\n",
      "DK4298 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK4299 & 3 & - & \\cite{kaiser1999intercellular} \\\\\n",
      "DK4300 & 5 & - & \\cite{rasmussen2003todk} \\\\\n",
      "DK4302 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4308 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4309 & 2 & - & \\cite{thony1993devrs} \\\\\n",
      "DK4310 & 3 & - & \\cite{bowden1996myxococcus} \\\\\n",
      "DK4312 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4313 & 2 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4316 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK4322 & 6 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4323 & 6 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4324 & 6 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4325 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4366 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4371 & 3 & - & \\cite{bowden1996myxococcus} \\\\\n",
      "DK4372 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4375 & 5 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4376 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4377 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4379 & 3 & - & \\cite{bowden1996myxococcus} \\\\\n",
      "DK4380 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4386 & 6 & - & \\cite{bowden1996myxococcus, kuspa1986intercellular} \\\\\n",
      "DK4387 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4389 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4396 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK4399 & 6 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK440 & 3 & - & \\cite{larossa1983developmental} \\\\\n",
      "DK4401 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4408 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4414 & 3 & - & \\cite{wall1999myxococcus} \\\\\n",
      "DK4435 & 3 & - & \\cite{li1988site} \\\\\n",
      "DK4442 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4445 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK4451 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK4459 & 3 & - & \\cite{chen1991physical} \\\\\n",
      "DK4464 & 6 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK4474 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK4481 & 6 & - & \\cite{kroos1986global} \\\\\n",
      "DK4492 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK4499 & 3 & - & \\cite{fisseha1999identification} \\\\\n",
      "DK4500 & 3 & - & \\cite{kroos1986global} \\\\\n",
      "DK4506 & 3 & - & \\cite{kroos1987expression, kroos1986global} \\\\\n",
      "DK4517 & 6 & - & \\cite{kroos1986global} \\\\\n",
      "DK4519 & 6 & - & \\cite{kashefi1995genetic} \\\\\n",
      "DK4521 & 12 & - & \\cite{kroos1987expression, keseler1996sigma} \\\\\n",
      "DK4529 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK4530 & 2 & - & \\cite{kroos1987expression} \\\\\n",
      "DK4535 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK4564 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK4566 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK4587 & 2 & - & \\cite{kuspa1989intercellular} \\\\\n",
      "DK4619 & 3 & - & \\cite{mayo1989asgb} \\\\\n",
      "DK4678 & 3 & - & \\cite{mayo1989asgb} \\\\\n",
      "DK4696 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK473 & 6 & - & \\cite{larossa1983developmental} \\\\\n",
      "DK4746 & 6 & - & \\cite{kaplan1991suppressors} \\\\\n",
      "DK4750 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK476 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "DK4764 & 5 & - & \\cite{mayo1989asgb} \\\\\n",
      "DK4765 & 3 & - & \\cite{mayo1989asgb} \\\\\n",
      "DK4778 & 3 & - & \\cite{mayo1989asgb} \\\\\n",
      "DK48 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK4861 & 8 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK4863 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK4870 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK495 & 3 & - & \\cite{hagen1978synergism} \\\\\n",
      "DK5055 & 1 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5056 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5057 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK50576 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK5058 & 5 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5061 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5063 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5066 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5067 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5068 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5069 & 2 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5070 & 3 & - & \\cite{kuspa1989intercellular} \\\\\n",
      "DK5073 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5074 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5090 & 2 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK510 & 3 & - & \\cite{hodgkin1979genetics} \\\\\n",
      "DK516 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK5200 & 5 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5203 & 3 & - & \\cite{kuspa1989intercellular} \\\\\n",
      "DK5204 & 3 & - & \\cite{li1988site} \\\\\n",
      "DK5205 & 3 & - & \\cite{kaplan1991suppressors} \\\\\n",
      "DK5206 & 9 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5208 & 6 & - & \\cite{licking2000common} \\\\\n",
      "DK5209 & 8 & - & \\cite{karamanos1996secretion} \\\\\n",
      "DK5216 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5217 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5225 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5246 & 5 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5247 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5251 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5257 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5270 & 2 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5274 & 3 & - & \\cite{kroos1990defects} \\\\\n",
      "DK5275 & 3 & - & \\cite{kroos1990defects} \\\\\n",
      "DK5276 & 2 & - & \\cite{kroos1990defects} \\\\\n",
      "DK5277 & 3 & - & \\cite{kroos1990defects} \\\\\n",
      "DK5279 & 5 & - & \\cite{kim1990cell} \\\\\n",
      "DK5280 & 3 & - & \\cite{keseler1996sigma} \\\\\n",
      "DK5281 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK5285 & 3 & - & \\cite{kroos1987expression} \\\\\n",
      "DK5510 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK5511 & 2 & - & \\cite{dana1993regulation} \\\\\n",
      "DK5870 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK590 & 6 & - & \\cite{manoil1980purine} \\\\\n",
      "DK6000 & 2 & - & \\cite{martinez1989genic} \\\\\n",
      "DK6001 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK6058 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK6206 & 3 & - & \\cite{hartzell1991function} \\\\\n",
      "DK653 & 3 & - & \\cite{hagen1978synergism} \\\\\n",
      "DK6601 & 2 & - & \\cite{kaplan1991suppressors} \\\\\n",
      "DK6607 & 3 & - & \\cite{kaplan1991suppressors} \\\\\n",
      "DK6620 & 3 & - & \\cite{gulati1995identification} \\\\\n",
      "DK6621 & 3 & - & \\cite{gulati1995identification} \\\\\n",
      "DK6643 & 3 & - & \\cite{kaplan1991suppressors} \\\\\n",
      "DK6665 & 3 & - & \\cite{asp2023mapping} \\\\\n",
      "DK6759 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK68 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7052 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7053 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7056 & 8 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7057 & 6 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7058 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7088 & 2 & - & \\cite{dana1993regulation} \\\\\n",
      "DK7090 & 5 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7091 & 6 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7154 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7160 & 3 & - & \\cite{gorski2000varsigma54} \\\\\n",
      "DK718 & 3 & - & \\cite{hagen1978synergism} \\\\\n",
      "DK7293 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7294 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7295 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK731 & 3 & - & \\cite{hagen1978synergism} \\\\\n",
      "DK739 & 3 & - & \\cite{hagen1978synergism} \\\\\n",
      "DK741 & 2 & - & \\cite{shimkets1982induction} \\\\\n",
      "DK7507 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7509 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7512 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7517 & 3 & - & \\cite{asp2023mapping} \\\\\n",
      "DK752 & 3 & - & \\cite{kuspa1989genes} \\\\\n",
      "DK7523 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7559 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK756 & 2 & - & \\cite{hagen1978synergism} \\\\\n",
      "DK7563 & 3 & - & \\cite{keseler1996sigma} \\\\\n",
      "DK7575 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7578 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7585 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK762 & 3 & - & \\cite{shimkets1982murein} \\\\\n",
      "DK7627 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7636 & 3 & - & \\cite{keseler1996sigma} \\\\\n",
      "DK7639 & 1 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7646 & 2 & - & \\cite{keseler1996sigma} \\\\\n",
      "DK7669 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7761 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7763 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK777 & 3 & - & \\cite{mayo1989asgb} \\\\\n",
      "DK7824 & 2 & - & \\cite{gorski1998targeted} \\\\\n",
      "DK7825 & 3 & - & \\cite{brenner2003role} \\\\\n",
      "DK7826 & 3 & - & \\cite{gorski1998targeted} \\\\\n",
      "DK7827 & 2 & - & \\cite{gorski2000varsigma54} \\\\\n",
      "DK785 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK7862 & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK8617 & 3 & - & \\cite{wall1999myxococcus} \\\\\n",
      "DK8624 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "DK9521 & 3 & - & \\cite{kashefi1995genetic} \\\\\n",
      "FruATc & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "MXAN1010 & 3 & - & \\cite{curtis2007metabolic} \\\\\n",
      "MXAN1909 & 2 & - & \\cite{caberoy2005coordinating} \\\\\n",
      "MXAN3442 & 3 & - & \\cite{giglio2011enhancer} \\\\\n",
      "MXAN3474 & 2 & - & \\cite{youderian2006transposon} \\\\\n",
      "MXAN3553 & 2 & - & \\cite{kim2009operon} \\\\\n",
      "MXAN3557 & 3 & - & \\cite{kim2009operon} \\\\\n",
      "MXAN4251 & 3 & - & \\cite{sarwar2012role} \\\\\n",
      "MXAN4398 & 6 & - & \\cite{treuner2010phosphatomes} \\\\\n",
      "MXAN4400 & 3 & - & \\cite{caro2023myxococcus} \\\\\n",
      "MXAN4494 & 1 & - & \\cite{kahnt2010profiling} \\\\\n",
      "MXAN7059 & 6 & - & \\cite{whitworth2008protein} \\\\\n",
      "MXAN7089 & 6 & - & \\cite{bhat2011pores} \\\\\n",
      "MXAN7164 & 3 & - & \\cite{caberoy2005coordinating} \\\\\n",
      "Omega4531 & 1 & - & \\cite{kroos1986global} \\\\\n",
      "esgWen & 2 & - & \\cite{caro2023myxococcus} \\\\\n",
      "omega4469 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "omega4473 & 3 & - & \\cite{kuspa1986intercellular} \\\\\n",
      "\\end{longtable}\n",
      "\\end{center}\n",
      "\n",
      "Successfully saved to /home/xavier/Documents/DAE_project/motility_table.tex\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T04:50:25.242548Z",
     "start_time": "2025-10-13T04:50:25.223052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Motility Group Statistics ---\n",
    "print(\"--- Motility Group Statistics ---\")\n",
    "# Group by the 'motility' column and calculate aggregates\n",
    "motility_groups = df.groupby('motility').agg(\n",
    "    strain_count=('Strain', 'count'),\n",
    "    movie_sum=('Number of Movies', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for a clearer output\n",
    "motility_groups = motility_groups.rename(columns={\n",
    "    'motility': 'Motility',\n",
    "    'strain_count': 'Number of Strains',\n",
    "    'movie_sum': 'Total Movies'\n",
    "})\n",
    "\n",
    "# Print the statistics to the console\n",
    "print(motility_groups.to_string(index=False))\n",
    "print(\"-\" * 35)  # Add a separator for clarity"
   ],
   "id": "bf0c0ad37fb1fa16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Motility Group Statistics ---\n",
      "Motility  Number of Strains  Total Movies\n",
      "       -                254           807\n",
      "    A+S-                 14            45\n",
      "    A-S+                 15            44\n",
      "    A-S-                  7            19\n",
      "      WT                  2            22\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot WT aggregating distribution",
   "id": "2cd414c1a243d281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "WT_LABEL_PATH = f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\"\n",
    "\n",
    "wt_df = pd.read_csv(WT_LABEL_PATH)\n",
    "# Group by run_id and count T/F in aggregates_formed\n",
    "counts = wt_df.groupby(\"run_id\")[\"aggregates_formed\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Create bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# F on top, T on bottom â†’ stack bars with T first, then F\n",
    "counts.plot(kind=\"bar\", stacked=True, ax=ax, color={\"T\": \"tab:blue\", \"F\": \"tab:orange\"})\n",
    "\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Counts of Aggregates Formed (T and F) by run_id\")\n",
    "ax.legend(title=\"Aggregates Formed\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2cab1996a746a7fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea07e0fd65e50bd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Perform AUC and PCA analysis for WT aggregate formation and motility",
   "id": "314e693c8294feae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:37:41.788875905Z",
     "start_time": "2025-09-28T06:29:51.209324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Integrated Script: Trajectory Distance, Predictive Analysis, Dimensionality Reduction, and Frame Extraction\n",
    "# ==============================================================================\n",
    "# Purpose:\n",
    "# 1. Load and process trajectory data, imputing NaN values with the last valid frame.\n",
    "# 2. Print a summary of sample and run counts for each class.\n",
    "# 3. Calculate pairwise distances and train SVM classifiers to evaluate predictive power (AUC).\n",
    "#    Includes a fallback from StratifiedGroupKFold to StratifiedKFold if splits are invalid.\n",
    "# 4. Generate bar plots of AUC scores and SAVE THE UNDERLYING DATA to a CSV file.\n",
    "# 5. Perform dimensionality reduction (PCA/UMAP) on the feature space at specified time points,\n",
    "#    and SAVE THE REDUCED COORDINATES to a CSV file.\n",
    "# 6. Optionally, visualize the SVM decision boundary on the dimensionality reduction plots,\n",
    "#    now including a color bar to indicate class probability.\n",
    "# 7. For specified time points, find the corresponding raw images, perform a center crop,\n",
    "#    and save the processed images to an output folder, organized by time and class.\n",
    "#\n",
    "# REVISIONS IN THIS VERSION:\n",
    "# - Optimized the SVM boundary visualization in `plot_dimensionality_reduction` for significant speed improvement.\n",
    "#   - Increased the `meshgrid` step size to reduce the number of prediction points.\n",
    "#   - Removed the unnecessary cross-validation loop for plotting; a single SVM is now trained on the\n",
    "#     sampled 2D data for a much faster, yet still representative, visualization.\n",
    "# - Maintained all advanced features: probability-based shading, color consistency, and the color bar.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.stats import sem\n",
    "from itertools import combinations, product\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "import warnings\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CONFIG = {\n",
    "    # --- Analysis Setup ---\n",
    "    \"analysis_type\": \"WT\",  # \"WT\" or \"motility\"\n",
    "    \"random_seed\": 42,  # Seed for all random operations to ensure reproducibility\n",
    "\n",
    "    # --- Data and Model Paths ---\n",
    "    \"features_base_dir_wt\": f\"{WORKING_DIR}/encoded_features/WT_features\",\n",
    "    \"labeling_csv_path_wt\": f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\",\n",
    "    \"WT_img_dir\": f\"{WORKING_DIR}/dataset/WT/images\",\n",
    "\n",
    "    \"features_base_dir_motility\": f\"{WORKING_DIR}/encoded_features/Roy_training_features\",\n",
    "    \"motility_csv_path\": f'{WORKING_DIR}/dataset/Roy_training/merged_strain_data.xlsx',\n",
    "    \"motility_img_dir\": f\"{WORKING_DIR}/dataset/Roy_training/images\",\n",
    "\n",
    "    # --- Output Configuration ---\n",
    "    \"analysis_output_dir\": f\"{WORKING_DIR}/images/figure6/%s_analysis\",\n",
    "    \"output_figure_name\": \"distance_and_prediction_summary.pdf\",\n",
    "    \"roc_figure_name\": \"roc_curves_combined.pdf\",\n",
    "    \"auc_barplot_name\": \"auc_barplot.pdf\",\n",
    "    \"pca_plot_name\": \"pca_plot.pdf\",\n",
    "    \"umap_plot_name\": \"umap_plot.pdf\",\n",
    "\n",
    "    # --- Rerun and Visualization Settings ---\n",
    "    \"force_rerun\": True,\n",
    "    \"show_movie_distance_analysis\": True,\n",
    "    \"n_splits\": 3,\n",
    "\n",
    "    # --- Frame Copying Settings ---\n",
    "    \"copy_frames\": False,\n",
    "    \"copied_frames_dir\": \"copied_frames\",\n",
    "\n",
    "    # --- Analysis Parameters ---\n",
    "    \"selected_time_points\": [1440, 0],\n",
    "    \"dist_method\": \"euclidean\",\n",
    "    \"tolerance\": 90,\n",
    "    \"num_workers\": max(1, cpu_count() - 2),\n",
    "    \"required_frames_motility\": 1441,\n",
    "\n",
    "    # --- Dimensionality Reduction Settings ---\n",
    "    \"dimensionality_reduction\": {\n",
    "        \"run\": True,\n",
    "        \"method\": \"PCA\",  # \"PCA\" or \"UMAP\"\n",
    "        \"sample_equal\": True,\n",
    "        \"plot_in_one_figure\": False,\n",
    "        \"show_svm_boundary\": False,\n",
    "        \"plot_mean_features_at_times\": []\n",
    "    },\n",
    "\n",
    "    # --- Motility Analysis Specific ---\n",
    "    \"motility_target_classes\": ['WT', 'A+S-', 'A-S+', 'A-S-'],\n",
    "    \"motility_comparison_pairs\": [\n",
    "        ('WT', 'A+S-'),\n",
    "        ('WT', 'A-S+'),\n",
    "        ('WT', 'A-S-'),\n",
    "        ('A+S-', 'A-S+'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Set the global random seed from the config for numpy operations\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "CONFIG[\"analysis_output_dir\"] = CONFIG[\"analysis_output_dir\"] % CONFIG[\"analysis_type\"]\n",
    "# Add paths for cached files\n",
    "CONFIG[\"cached_distances_path\"] = os.path.join(CONFIG['analysis_output_dir'], \"cached_movie_distances.npz\")\n",
    "CONFIG[\"cached_dist_matrix_path\"] = os.path.join(CONFIG['analysis_output_dir'], \"cached_dist_matrix.npz\")\n",
    "\n",
    "\n",
    "# --- Image Processing Helper Functions ---\n",
    "\n",
    "def resize_crop(img_dir, resize_by=1., resolution=512, brightness_norm=True, brightness_mean=107):\n",
    "    \"\"\"\n",
    "    Loads an image, resizes it, and takes a center crop.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_dir, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    if img.dtype != np.uint8:\n",
    "        img = (img / 256).astype(np.uint8)\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    img_shape = img.shape\n",
    "    resize_shape = (int(img_shape[1] * resize_by), int(img_shape[0] * resize_by))\n",
    "\n",
    "    if resize_by != 1:\n",
    "        img = cv2.resize(img, resize_shape, cv2.INTER_LANCZOS4)\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    start_y = max(0, (h - resolution) // 2)\n",
    "    start_x = max(0, (w - resolution) // 2)\n",
    "    new_img = img[start_y:start_y + resolution, start_x:start_x + resolution]\n",
    "\n",
    "    if brightness_norm:\n",
    "        obj_v = np.mean(new_img)\n",
    "        value = brightness_mean - obj_v\n",
    "        value_array = np.full(new_img.shape, value, dtype=np.float64)\n",
    "        new_img = np.clip(new_img.astype(np.float64) + value_array, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return new_img\n",
    "\n",
    "\n",
    "def copy_and_process_frames(run_id, scope_id, class_name, time_points, base_img_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Finds, processes, and saves specific frames for a given sample.\n",
    "    Finds the closest available frame if the exact frame is not found.\n",
    "    \"\"\"\n",
    "    for t in time_points:\n",
    "        try:\n",
    "            frame_output_dir = os.path.join(output_dir, f\"{t}min\", class_name)\n",
    "            os.makedirs(frame_output_dir, exist_ok=True)\n",
    "\n",
    "            run_dir_pattern = os.path.join(base_img_dir, f\"*Run{run_id:04d}*\")\n",
    "            matching_run_dirs = glob.glob(run_dir_pattern)\n",
    "\n",
    "            scope_dir_path = \"\"\n",
    "            if matching_run_dirs:\n",
    "                run_dir = matching_run_dirs[0]\n",
    "                scope_dir_path = os.path.join(run_dir, f\"Scope{scope_id:02d}\")\n",
    "            else:\n",
    "                scope_dir_path = os.path.join(base_img_dir, f\"Run{run_id:04d}\", f\"Scope{scope_id:02d}\")\n",
    "\n",
    "            if not os.path.isdir(scope_dir_path):\n",
    "                print(\n",
    "                    f\"  Warning: Scope directory not found for Run {run_id}, Scope {scope_id}. Path: {scope_dir_path}\")\n",
    "                continue\n",
    "\n",
    "            all_images = glob.glob(os.path.join(scope_dir_path, \"*.jpg\"))\n",
    "            if not all_images:\n",
    "                print(f\"  Warning: No JPG images found for Run {run_id}, Scope {scope_id} in {scope_dir_path}\")\n",
    "                continue\n",
    "\n",
    "            target_frame_idx = t + 1\n",
    "            best_match_path = None\n",
    "            min_diff = float('inf')\n",
    "            frame_number_pattern = re.compile(r'_(\\d+)\\.jpg$')\n",
    "\n",
    "            for img_path in all_images:\n",
    "                match = frame_number_pattern.search(os.path.basename(img_path))\n",
    "                if match:\n",
    "                    frame_num = int(match.group(1))\n",
    "                    diff = abs(frame_num - target_frame_idx)\n",
    "                    if diff < min_diff:\n",
    "                        min_diff = diff\n",
    "                        best_match_path = img_path\n",
    "\n",
    "            if best_match_path:\n",
    "                source_path = best_match_path\n",
    "                found_frame_num_match = frame_number_pattern.search(os.path.basename(source_path))\n",
    "                if found_frame_num_match:\n",
    "                    found_frame_num = int(found_frame_num_match.group(1))\n",
    "                    if found_frame_num != target_frame_idx:\n",
    "                        print(\n",
    "                            f\"  Info: For Run {run_id}, Scope {scope_id}, Time {t} min, using closest frame {found_frame_num}.\")\n",
    "\n",
    "                processed_img = resize_crop(source_path)\n",
    "                if processed_img is not None:\n",
    "                    dest_filename = f\"{class_name}_Run{run_id:04d}_Scope{scope_id:02d}.jpg\"\n",
    "                    dest_path = os.path.join(frame_output_dir, dest_filename)\n",
    "                    cv2.imwrite(dest_path, processed_img)\n",
    "                else:\n",
    "                    print(f\"  Warning: Failed to process image: {source_path}\")\n",
    "            else:\n",
    "                print(f\"  Warning: Image not found for Run {run_id}, Scope {scope_id}, Time {t} min.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing frame for Run {run_id}, Scope {scope_id} at time {t}: {e}\")\n",
    "\n",
    "\n",
    "# --- Analysis Helper Functions ---\n",
    "\n",
    "def impute_nans_with_previous_frame(trajectory):\n",
    "    for i in range(1, trajectory.shape[0]):\n",
    "        if np.isnan(trajectory[i]).any():\n",
    "            trajectory[i] = trajectory[i - 1]\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def get_closest_frame_index(requested_frame, total_frames):\n",
    "    if total_frames == 0:\n",
    "        raise ValueError(\"Cannot get frame index from a trajectory with zero frames.\")\n",
    "    max_index = total_frames - 1\n",
    "    return min(requested_frame, max_index)\n",
    "\n",
    "\n",
    "def train_and_get_roc_data(features, labels, groups, use_group_kfold, analysis_name, precomputed_kernel=False):\n",
    "    n_splits = CONFIG['n_splits']\n",
    "    random_seed = CONFIG['random_seed']\n",
    "\n",
    "    if use_group_kfold:\n",
    "        cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "        split_iterator = cv.split(features, labels, groups)\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "        split_iterator = cv.split(features, labels)\n",
    "\n",
    "    tprs, aucs = [], []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    try:\n",
    "        for train_idx, test_idx in split_iterator:\n",
    "            if len(np.unique(labels[test_idx])) < 2:\n",
    "                print(f\"  Skipping invalid fold in CV for {analysis_name}: test set contains only one class.\")\n",
    "                continue\n",
    "\n",
    "            if precomputed_kernel:\n",
    "                model = SVC(kernel='precomputed', class_weight='balanced', probability=True, random_state=random_seed)\n",
    "                model.fit(features[np.ix_(train_idx, train_idx)], labels[train_idx])\n",
    "                probas_ = model.predict_proba(features[np.ix_(test_idx, train_idx)])\n",
    "            else:\n",
    "                pipeline = make_pipeline(StandardScaler(), SVC(kernel='rbf', class_weight='balanced', probability=True,\n",
    "                                                               random_state=random_seed))\n",
    "                pipeline.fit(features[train_idx], labels[train_idx])\n",
    "                probas_ = pipeline.predict_proba(features[test_idx])\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(labels[test_idx], probas_[:, 1])\n",
    "            tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            aucs.append(auc(fpr, tpr))\n",
    "    except Exception as e:\n",
    "        print(f\"  CV failed for {analysis_name} with error: {e}. Cannot generate ROC data.\")\n",
    "        return None\n",
    "\n",
    "    if not tprs:\n",
    "        print(f\"  Could not generate any valid CV folds for {analysis_name}. Cannot generate ROC data.\")\n",
    "        return None\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "\n",
    "    return mean_fpr, mean_tpr, std_tpr, mean_auc, std_auc\n",
    "\n",
    "\n",
    "# --- Plotting Functions ---\n",
    "\n",
    "def plot_auc_barplot(auc_scores, time_points, output_path, title_prefix=\"\", force_rerun=False):\n",
    "    if not force_rerun and os.path.exists(output_path):\n",
    "        print(f\"Skipping existing AUC bar plot: {os.path.basename(output_path)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Generating AUC bar plot: {os.path.basename(output_path)}\")\n",
    "    labels = [f'{t // 60} h' for t in time_points]\n",
    "    auc_means = [auc_scores.get(f'Time {t} min', (np.nan, np.nan))[0] for t in time_points]\n",
    "    auc_stds = [auc_scores.get(f'Time {t} min', (np.nan, np.nan))[1] for t in time_points]\n",
    "\n",
    "    y_err_lower, y_err_upper = [], []\n",
    "    for mean, std in zip(auc_means, auc_stds):\n",
    "        if np.isnan(mean) or np.isnan(std):\n",
    "            y_err_lower.append(0)\n",
    "            y_err_upper.append(0)\n",
    "            continue\n",
    "        margin = 1.96 * std\n",
    "        y_err_upper.append(min(mean + margin, 1.0) - mean)\n",
    "        y_err_lower.append(mean - max(mean - margin, 0.0))\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(x, auc_means, yerr=[y_err_lower, y_err_upper], capsize=5, color='skyblue', ecolor='gray',\n",
    "           label='Mean AUC (95% CI)')\n",
    "    ax.set_ylabel('Mean AUC Score')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylim(0.45, 1.05)\n",
    "    ax.axhline(y=0.5, color='r', linestyle='--', label='Random Chance (AUC=0.5)')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(output_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_dimensionality_reduction(features, labels, time_points, method='PCA', sample_equal=True,\n",
    "                                  plot_in_one_figure=True, output_path='dim_red.pdf', title_prefix=\"\",\n",
    "                                  legend_map=None, show_svm_boundary=False, force_rerun=False,\n",
    "                                  plot_mean_features_at_times=None):\n",
    "    \"\"\"\n",
    "    Samples data, performs PCA/UMAP, plots results, saves the data to a CSV,\n",
    "    and optionally shows a shaded SVM decision boundary with a color bar.\n",
    "    OPTIMIZED for performance.\n",
    "    \"\"\"\n",
    "    if not force_rerun and os.path.exists(output_path) and plot_in_one_figure:\n",
    "        print(f\"Skipping existing dimensionality reduction plot: {os.path.basename(output_path)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Generating {method} plot(s): {os.path.basename(output_path)}\")\n",
    "    print(f\"  Initial number of samples for plotting: {features.shape[0]}\")\n",
    "\n",
    "    random_seed = CONFIG['random_seed']\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "\n",
    "    if plot_in_one_figure:\n",
    "        fig, axes = plt.subplots(1, len(time_points), figsize=(5.5 * len(time_points), 5), sharex=False, sharey=False)\n",
    "        if len(time_points) == 1: axes = [axes]\n",
    "    else:\n",
    "        fig, axes = None, None\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2:\n",
    "        print(\"Warning: Only one class present. Skipping dimensionality reduction plot.\")\n",
    "        return\n",
    "\n",
    "    total_frames = features.shape[1]\n",
    "    contour_object = None\n",
    "\n",
    "    color_class_0 = '#3b75af'\n",
    "    color_class_1 = '#d1495b'\n",
    "    custom_palette_dict = None\n",
    "    if legend_map:\n",
    "        sorted_keys = sorted(legend_map.keys())\n",
    "        if len(sorted_keys) == 2:\n",
    "            custom_palette_dict = {\n",
    "                legend_map[sorted_keys[0]]: color_class_0,\n",
    "                legend_map[sorted_keys[1]]: color_class_1\n",
    "            }\n",
    "\n",
    "    for i, t_req in enumerate(time_points):\n",
    "        t = get_closest_frame_index(t_req, total_frames)\n",
    "        individual_output_path = output_path.replace('.pdf', f'_{t_req}min.pdf')\n",
    "        if not plot_in_one_figure and not force_rerun and os.path.exists(individual_output_path):\n",
    "            print(f\"Skipping existing plot: {os.path.basename(individual_output_path)}\")\n",
    "            continue\n",
    "\n",
    "        features_at_t = features[:, t, :]\n",
    "\n",
    "        if sample_equal:\n",
    "            min_samples = min(np.sum(labels == unique_labels[0]), np.sum(labels == unique_labels[1]))\n",
    "            indices_0 = rng.choice(np.where(labels == unique_labels[0])[0], min_samples, replace=False)\n",
    "            indices_1 = rng.choice(np.where(labels == unique_labels[1])[0], min_samples, replace=False)\n",
    "            sampled_indices = np.concatenate([indices_0, indices_1])\n",
    "            features_for_dim_red = features_at_t[sampled_indices]\n",
    "            labels_for_dim_red = labels[sampled_indices]\n",
    "        else:\n",
    "            features_for_dim_red = features_at_t\n",
    "            labels_for_dim_red = labels\n",
    "\n",
    "        reducer = PCA(n_components=2, random_state=random_seed) if method == 'PCA' else UMAP(n_components=2,\n",
    "                                                                                             random_state=random_seed)\n",
    "\n",
    "        try:\n",
    "            transformed_features = reducer.fit_transform(features_for_dim_red)\n",
    "        except ValueError as e:\n",
    "            print(f\"  ERROR: Could not perform {method} at time {t_req} min. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        plot_labels = pd.Series(labels_for_dim_red).map(legend_map) if legend_map else labels_for_dim_red\n",
    "\n",
    "        base_name = os.path.splitext(os.path.basename(output_path))[0]\n",
    "        data_filename = f\"{base_name}_{t_req}min_data.csv\"\n",
    "        data_output_path = os.path.join(os.path.dirname(output_path), data_filename)\n",
    "        if not os.path.exists(data_output_path) or force_rerun:\n",
    "            print(f\"  Saving {method} data for time {t_req} min to {os.path.basename(data_output_path)}...\")\n",
    "            pd.DataFrame({\n",
    "                f'{method} Component 1': transformed_features[:, 0],\n",
    "                f'{method} Component 2': transformed_features[:, 1],\n",
    "                'Group': plot_labels\n",
    "            }).to_csv(data_output_path, index=False, float_format='%.4f')\n",
    "        else:\n",
    "            print(f\"  Skipping existing {method} data file: {os.path.basename(data_output_path)}\")\n",
    "\n",
    "        ax = axes[i] if plot_in_one_figure else plt.subplots(figsize=(7, 6))[1]\n",
    "        if not plot_in_one_figure: fig_single = ax.get_figure()\n",
    "\n",
    "        if show_svm_boundary:\n",
    "            print(f\"  Visualizing SVM boundaries for time {t_req} min...\")\n",
    "            x_min, x_max = transformed_features[:, 0].min() - 1, transformed_features[:, 0].max() + 1\n",
    "            y_min, y_max = transformed_features[:, 1].min() - 1, transformed_features[:, 1].max() + 1\n",
    "\n",
    "            # OPTIMIZATION: Increased meshgrid step size from 0.02 to 0.1 for a >20x speedup.\n",
    "            # This creates a coarser grid for visualization without significant loss of quality.\n",
    "            step_size = 0.1\n",
    "            xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n",
    "\n",
    "            # OPTIMIZATION: Removed CV loop for visualization. A single SVM trained on the\n",
    "            # sampled 2D data is sufficient and much faster for plotting a representative boundary.\n",
    "            svm_2d = SVC(kernel='rbf', gamma='auto', probability=True, random_state=random_seed)\n",
    "            svm_2d.fit(transformed_features, labels_for_dim_red)\n",
    "            Z = svm_2d.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "            Z = Z.reshape(xx.shape)\n",
    "\n",
    "            contour = ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.5, levels=np.linspace(0, 1, 21))\n",
    "            contour_object = contour\n",
    "\n",
    "            if not plot_in_one_figure:\n",
    "                cbar = fig_single.colorbar(contour, ax=ax)\n",
    "                cbar.set_label('Class Probability', rotation=270, labelpad=15)\n",
    "                cbar.set_ticks([0, 0.5, 1])\n",
    "                if legend_map:\n",
    "                    class0_label, class1_label = legend_map.get(0, 'Class 0'), legend_map.get(1, 'Class 1')\n",
    "                    cbar.ax.set_yticklabels([class0_label, 'Boundary', class1_label], fontsize=8, rotation=90,\n",
    "                                            va='center')\n",
    "\n",
    "        sns.scatterplot(x=transformed_features[:, 0], y=transformed_features[:, 1], hue=plot_labels,\n",
    "                        palette=custom_palette_dict if custom_palette_dict else 'Set2',\n",
    "                        ax=ax, alpha=0.8, edgecolor='k')\n",
    "        ax.set_title(f'Time {t_req // 60} h')\n",
    "        ax.set_xlabel(f'{method} 1')\n",
    "        ax.set_ylabel(f'{method} 2')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        if plot_mean_features_at_times and t_req in plot_mean_features_at_times:\n",
    "            print(f\"  Calculating and plotting mean features for time {t_req} min...\")\n",
    "            mean_feature_c0 = np.mean(features_at_t[labels == unique_labels[0]], axis=0)\n",
    "            mean_feature_c1 = np.mean(features_at_t[labels == unique_labels[1]], axis=0)\n",
    "\n",
    "            transformed_mean_c0 = reducer.transform(mean_feature_c0.reshape(1, -1))\n",
    "            transformed_mean_c1 = reducer.transform(mean_feature_c1.reshape(1, -1))\n",
    "            mean_of_means = (transformed_mean_c0 + transformed_mean_c1) / 2.0\n",
    "            ax.scatter(transformed_mean_c0[:, 0], transformed_mean_c0[:, 1], marker='*', s=300, c=color_class_0,\n",
    "                       edgecolor='white', zorder=10)\n",
    "            ax.scatter(transformed_mean_c1[:, 0], transformed_mean_c1[:, 1], marker='*', s=300, c=color_class_1,\n",
    "                       edgecolor='white', zorder=10)\n",
    "            ax.scatter(mean_of_means[:, 0], mean_of_means[:, 1], marker='*', s=300, c='yellow', edgecolor='black',\n",
    "                       zorder=10)\n",
    "\n",
    "        if not plot_in_one_figure:\n",
    "            plt.tight_layout()\n",
    "            fig_single.savefig(individual_output_path, dpi=300)\n",
    "            plt.close(fig_single)\n",
    "\n",
    "    if plot_in_one_figure:\n",
    "        fig.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "        if contour_object:\n",
    "            cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "            cbar = fig.colorbar(contour_object, cax=cbar_ax)\n",
    "            cbar.set_label('Class Probability', rotation=270, labelpad=15)\n",
    "            cbar.set_ticks([0, 0.5, 1])\n",
    "            if legend_map:\n",
    "                class0_label, class1_label = legend_map.get(0, 'Class 0'), legend_map.get(1, 'Class 1')\n",
    "                cbar.ax.set_yticklabels([class0_label, 'Boundary', class1_label], fontsize=8, rotation=90, va='center')\n",
    "        fig.savefig(output_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# --- Main Analysis Functions ---\n",
    "\n",
    "def run_wt_analysis():\n",
    "    print(\"\\n[STAGE 1/4] Loading and preparing WT data...\")\n",
    "    try:\n",
    "        labels_df = pd.read_csv(CONFIG['labeling_csv_path_wt'])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Labeling sheet not found at '{CONFIG['labeling_csv_path_wt']}'.\")\n",
    "        return\n",
    "\n",
    "    labels_df = labels_df.dropna(subset=['aggregates_formed'])\n",
    "    labels_df = labels_df[labels_df['aggregates_formed'].isin(['T', 'F'])]\n",
    "    labels_df['label'] = labels_df['aggregates_formed'].map({'T': 1, 'F': 0})\n",
    "\n",
    "    print(\"\\n--- Data Summary ---\")\n",
    "    for class_label, count in labels_df['aggregates_formed'].value_counts().items():\n",
    "        class_name = 'Aggregate' if class_label == 'T' else 'No Aggregate'\n",
    "        num_runs = labels_df[labels_df['aggregates_formed'] == class_label]['run_id'].nunique()\n",
    "        print(f\"  Class '{class_name}': {count} samples from {num_runs} unique runs.\")\n",
    "    print(\"--------------------\\n\")\n",
    "\n",
    "    all_features = {}\n",
    "    if CONFIG['copy_frames']:\n",
    "        print(\"[STAGE 1.5/4] Copying and processing selected frames for WT analysis...\")\n",
    "        copied_frames_output_dir = os.path.join(CONFIG['analysis_output_dir'], CONFIG['copied_frames_dir'], 'WT')\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        run_id, scope_id = row['run_id'], row['scope_id']\n",
    "        key = f\"R{run_id:04d}_S{scope_id:02d}\"\n",
    "        npz_path = os.path.join(CONFIG['features_base_dir_wt'], f\"Run{run_id:04d}\", f\"Scope{scope_id:02d}\",\n",
    "                                \"features.npz\")\n",
    "        if not os.path.exists(npz_path):\n",
    "            print(f\"Warning: Feature file not found for {key}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        features_data = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "        all_features[key] = {'label': row['label'], 'run_id': row['run_id'], 'features': features_data}\n",
    "\n",
    "        if CONFIG['copy_frames']:\n",
    "            class_name = 'Aggregate' if row['label'] == 1 else 'No_Aggregate'\n",
    "            copy_and_process_frames(run_id, scope_id, class_name, CONFIG['selected_time_points'], CONFIG['WT_img_dir'],\n",
    "                                    copied_frames_output_dir)\n",
    "\n",
    "    if not all_features: print(\"ERROR: No valid data could be loaded.\"); return\n",
    "\n",
    "    min_frames = min(v['features'].shape[0] for v in all_features.values())\n",
    "    all_trajectories = np.array([v['features'][:min_frames] for v in all_features.values()])\n",
    "    all_labels = np.array([v['label'] for v in all_features.values()])\n",
    "    all_groups = np.array([v['run_id'] for v in all_features.values()])\n",
    "\n",
    "    if len(np.unique(all_labels)) < 2: print(\"Warning: Insufficient data for one or both classes.\"); return\n",
    "\n",
    "    print(\"\\n[STAGE 2/4] Analyzing predictive power of features...\")\n",
    "    auc_scores = {}\n",
    "    n_splits = CONFIG['n_splits']\n",
    "    use_group_kfold = len(np.unique(all_groups[all_labels == 0])) >= n_splits and len(\n",
    "        np.unique(all_groups[all_labels == 1])) >= n_splits\n",
    "\n",
    "    for t in CONFIG['selected_time_points']:\n",
    "        features_at_t = all_trajectories[:, get_closest_frame_index(t, min_frames), :]\n",
    "        analysis_name = f\"Time {t} min\"\n",
    "        roc_data = train_and_get_roc_data(features_at_t, all_labels, all_groups, use_group_kfold, analysis_name)\n",
    "        if roc_data is None and use_group_kfold:\n",
    "            print(f\"  Warning: StratifiedGroupKFold failed for {analysis_name}. Retrying with StratifiedKFold.\")\n",
    "            roc_data = train_and_get_roc_data(features_at_t, all_labels, all_groups, False, analysis_name)\n",
    "        if roc_data: auc_scores[analysis_name] = (roc_data[3], roc_data[4])\n",
    "\n",
    "    print(\"\\n[STAGE 3/4] Saving result data...\")\n",
    "    if auc_scores:\n",
    "        auc_data_path = os.path.join(CONFIG['analysis_output_dir'], \"WT_auc_scores.csv\")\n",
    "        if not os.path.exists(auc_data_path) or CONFIG['force_rerun']:\n",
    "            auc_data = [\n",
    "                {'Time (min)': int(re.search(r'(\\d+)', name).group(1)), 'Mean AUC': mean_auc, 'Std Dev AUC': std_auc}\n",
    "                for name, (mean_auc, std_auc) in auc_scores.items()]\n",
    "            pd.DataFrame(auc_data).sort_values('Time (min)').to_csv(auc_data_path, index=False, float_format='%.4f')\n",
    "            print(f\"  Saved WT AUC scores to: {os.path.basename(auc_data_path)}\")\n",
    "        else:\n",
    "            print(f\"  Skipping existing WT AUC scores file: {os.path.basename(auc_data_path)}\")\n",
    "\n",
    "    print(\"\\n[STAGE 4/4] Generating analysis plots...\")\n",
    "    plot_auc_barplot(auc_scores, CONFIG['selected_time_points'],\n",
    "                     os.path.join(CONFIG['analysis_output_dir'], f\"WT_{CONFIG['auc_barplot_name']}\"), \"WT Analysis: \",\n",
    "                     CONFIG['force_rerun'])\n",
    "\n",
    "    if CONFIG['dimensionality_reduction']['run']:\n",
    "        dr_config = CONFIG['dimensionality_reduction']\n",
    "        output_name = f\"WT_{dr_config['method']}_plot.pdf\"\n",
    "        plot_dimensionality_reduction(all_trajectories, all_labels, time_points=CONFIG['selected_time_points'],\n",
    "                                      method=dr_config['method'], sample_equal=dr_config['sample_equal'],\n",
    "                                      plot_in_one_figure=dr_config['plot_in_one_figure'],\n",
    "                                      output_path=os.path.join(CONFIG['analysis_output_dir'], output_name),\n",
    "                                      legend_map={0: 'No Aggregate', 1: 'Aggregate'},\n",
    "                                      show_svm_boundary=dr_config['show_svm_boundary'],\n",
    "                                      force_rerun=CONFIG['force_rerun'],\n",
    "                                      plot_mean_features_at_times=dr_config.get('plot_mean_features_at_times'))\n",
    "\n",
    "\n",
    "def run_motility_analysis():\n",
    "    print(\"\\n[STAGE 1/4] Loading and preparing motility data...\")\n",
    "    try:\n",
    "        labels_df = pd.read_excel(CONFIG['motility_csv_path'])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Could not load motility data xlsx. Check path: {CONFIG['motility_csv_path']}\")\n",
    "        return\n",
    "\n",
    "    labels_df = labels_df.dropna(subset=['motility', 'Strain'])\n",
    "    labels_df = labels_df[labels_df['motility'].isin(CONFIG['motility_target_classes'])]\n",
    "    strain_to_label_map = pd.Series(labels_df.motility.values, index=labels_df.Strain).to_dict()\n",
    "\n",
    "    all_samples_dict = {class_name: [] for class_name in CONFIG['motility_target_classes']}\n",
    "    dir_pattern = re.compile(r'Run(\\d+)_Mutant(\\d+)')\n",
    "    scope_pattern = re.compile(r'Scope(\\d+)')\n",
    "\n",
    "    for dir_name in os.listdir(CONFIG['features_base_dir_motility']):\n",
    "        match = dir_pattern.match(dir_name)\n",
    "        if not match: continue\n",
    "        run_id, mutant_num = int(match.group(1)), int(match.group(2))\n",
    "        strain_id = f\"DK{mutant_num}\"\n",
    "        if strain_id in strain_to_label_map:\n",
    "            label = strain_to_label_map[strain_id]\n",
    "            for scope_dir_name in os.listdir(os.path.join(CONFIG['features_base_dir_motility'], dir_name)):\n",
    "                scope_match = scope_pattern.match(scope_dir_name)\n",
    "                if scope_match:\n",
    "                    scope_id = int(scope_match.group(1))\n",
    "                    npz_path = os.path.join(CONFIG['features_base_dir_motility'], dir_name, scope_dir_name,\n",
    "                                            \"features.npz\")\n",
    "                    if os.path.exists(npz_path):\n",
    "                        features = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "                        all_samples_dict[label].append({'features': features, 'run_id': run_id, 'scope_id': scope_id})\n",
    "\n",
    "    print(\"\\n--- Data Summary ---\")\n",
    "    for class_name, samples in all_samples_dict.items():\n",
    "        print(\n",
    "            f\"  Class '{class_name}': {len(samples)} samples from {len(np.unique([s['run_id'] for s in samples]))} unique runs.\")\n",
    "    print(\"--------------------\\n\")\n",
    "\n",
    "    if CONFIG['copy_frames']:\n",
    "        print(\"[STAGE 1.5/4] Copying and processing selected frames for motility analysis...\")\n",
    "        copied_frames_output_dir = os.path.join(CONFIG['analysis_output_dir'], CONFIG['copied_frames_dir'], 'motility')\n",
    "        for class_name, samples in all_samples_dict.items():\n",
    "            for sample in samples:\n",
    "                copy_and_process_frames(sample['run_id'], sample['scope_id'], class_name,\n",
    "                                        CONFIG['selected_time_points'], CONFIG['motility_img_dir'],\n",
    "                                        copied_frames_output_dir)\n",
    "\n",
    "    all_trajectories_dict = {k: [s['features'] for s in v] for k, v in all_samples_dict.items()}\n",
    "    all_runs_dict = {k: [s['run_id'] for s in v] for k, v in all_samples_dict.items()}\n",
    "    required_frames = CONFIG['required_frames_motility']\n",
    "    for class_name, trajs in all_trajectories_dict.items():\n",
    "        processed = [\n",
    "            np.vstack([t, np.repeat(t[-1:], required_frames - len(t), axis=0)]) if len(t) < required_frames else t[\n",
    "                :required_frames] for t in trajs]\n",
    "        all_trajectories_dict[class_name] = np.array(processed) if processed else np.array([])\n",
    "\n",
    "    for class1_name, class2_name in CONFIG['motility_comparison_pairs']:\n",
    "        print(f\"\\n--- Comparing '{class1_name}' vs. '{class2_name}' ---\")\n",
    "        trajs1, trajs2 = all_trajectories_dict.get(class1_name), all_trajectories_dict.get(class2_name)\n",
    "        if trajs1 is None or len(trajs1) == 0 or trajs2 is None or len(trajs2) == 0:\n",
    "            print(f\"  Warning: Insufficient data. Skipping pair.\");\n",
    "            continue\n",
    "\n",
    "        runs1, runs2 = all_runs_dict.get(class1_name), all_runs_dict.get(class2_name)\n",
    "        pair_trajs = np.concatenate([trajs1, trajs2])\n",
    "        pair_labels = np.array([0] * len(trajs1) + [1] * len(trajs2))\n",
    "        pair_groups = np.concatenate([runs1, runs2])\n",
    "\n",
    "        print(\"  [Step 1/3] Analyzing predictive power...\")\n",
    "        auc_scores = {}\n",
    "        n_splits = CONFIG['n_splits']\n",
    "        use_group_kfold = len(np.unique(runs1)) >= n_splits and len(np.unique(runs2)) >= n_splits\n",
    "\n",
    "        for t in CONFIG['selected_time_points']:\n",
    "            features_at_t = pair_trajs[:, get_closest_frame_index(t, required_frames), :]\n",
    "            analysis_name = f\"Time {t} min\"\n",
    "            roc_data = train_and_get_roc_data(features_at_t, pair_labels, pair_groups, use_group_kfold, analysis_name)\n",
    "            if roc_data is None and use_group_kfold:\n",
    "                print(f\"    Warning: StratifiedGroupKFold failed. Retrying with StratifiedKFold.\")\n",
    "                roc_data = train_and_get_roc_data(features_at_t, pair_labels, pair_groups, False, analysis_name)\n",
    "            if roc_data: auc_scores[analysis_name] = (roc_data[3], roc_data[4])\n",
    "\n",
    "        print(\"  [Step 2/3] Saving result data...\")\n",
    "        if auc_scores:\n",
    "            auc_data_path = os.path.join(CONFIG['analysis_output_dir'],\n",
    "                                         f\"motility_{class1_name}_vs_{class2_name}_auc_scores.csv\")\n",
    "            if not os.path.exists(auc_data_path) or CONFIG['force_rerun']:\n",
    "                auc_data = [{'Time (min)': int(re.search(r'(\\d+)', name).group(1)), 'Mean AUC': mean_auc,\n",
    "                             'Std Dev AUC': std_auc} for name, (mean_auc, std_auc) in auc_scores.items()]\n",
    "                pd.DataFrame(auc_data).sort_values('Time (min)').to_csv(auc_data_path, index=False, float_format='%.4f')\n",
    "                print(f\"    Saved AUC scores to: {os.path.basename(auc_data_path)}\")\n",
    "            else:\n",
    "                print(f\"    Skipping existing AUC scores file: {os.path.basename(auc_data_path)}\")\n",
    "\n",
    "        print(\"  [Step 3/3] Generating analysis plots...\")\n",
    "        title_prefix = f\"{class1_name}_vs_{class2_name}: \"\n",
    "        plot_auc_barplot(auc_scores, CONFIG['selected_time_points'], os.path.join(CONFIG['analysis_output_dir'],\n",
    "                                                                                  f\"motility_{class1_name}_vs_{class2_name}_{CONFIG['auc_barplot_name']}\"),\n",
    "                         title_prefix, CONFIG['force_rerun'])\n",
    "\n",
    "        if CONFIG['dimensionality_reduction']['run']:\n",
    "            dr_config = CONFIG['dimensionality_reduction']\n",
    "            output_name = f\"motility_{class1_name}_vs_{class2_name}_{dr_config['method']}_plot.pdf\"\n",
    "            plot_dimensionality_reduction(pair_trajs, pair_labels, time_points=CONFIG['selected_time_points'],\n",
    "                                          method=dr_config['method'], sample_equal=dr_config['sample_equal'],\n",
    "                                          plot_in_one_figure=dr_config['plot_in_one_figure'],\n",
    "                                          output_path=os.path.join(CONFIG['analysis_output_dir'], output_name),\n",
    "                                          legend_map={0: class1_name, 1: class2_name},\n",
    "                                          show_svm_boundary=dr_config['show_svm_boundary'],\n",
    "                                          force_rerun=CONFIG['force_rerun'],\n",
    "                                          plot_mean_features_at_times=dr_config.get('plot_mean_features_at_times'))\n",
    "\n",
    "\n",
    "def main():\n",
    "    output_dir = CONFIG['analysis_output_dir']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    analysis_type = CONFIG['analysis_type']\n",
    "    print(f\"--- Starting Analysis Script for '{analysis_type}' ---\")\n",
    "\n",
    "    if analysis_type == 'WT':\n",
    "        run_wt_analysis()\n",
    "    elif analysis_type == 'motility':\n",
    "        run_motility_analysis()\n",
    "    else:\n",
    "        print(f\"ERROR: Unknown analysis type '{analysis_type}'. Choose \\'WT\\' or \\'motility\\'.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Script execution complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Before running, ensure the WORKING_DIR at the top of the script is set correctly.\n",
    "    if WORKING_DIR == \".\":\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"!!! WARNING: `WORKING_DIR` is not set. Please update it to  !!!\")\n",
    "        print(\"!!! your project's base directory before running this script. !!!\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    main()\n"
   ],
   "id": "99572da4faf1a542",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Analysis Script for 'WT' ---\n",
      "\n",
      "[STAGE 1/4] Loading and preparing WT data...\n",
      "\n",
      "--- Data Summary ---\n",
      "  Class 'Aggregate': 220 samples from 34 unique runs.\n",
      "  Class 'No Aggregate': 132 samples from 38 unique runs.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[STAGE 2/4] Analyzing predictive power of features...\n",
      "\n",
      "[STAGE 3/4] Saving result data...\n",
      "  Saved WT AUC scores to: WT_auc_scores.csv\n",
      "\n",
      "[STAGE 4/4] Generating analysis plots...\n",
      "Generating AUC bar plot: WT_auc_barplot.pdf\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Select False Positives",
   "id": "f2be5514bb2a7d4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:30:00.062443Z",
     "start_time": "2025-11-05T15:29:58.490218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "# ----------------------------------------------------\n",
    "\n",
    "CONFIG = {\n",
    "    # --- Analysis Setup ---\n",
    "    \"random_seed\": 42,\n",
    "\n",
    "    # --- Data and Model Paths (from your script) ---\n",
    "    \"features_base_dir_wt\": f\"{WORKING_DIR}/encoded_features/WT_features\",\n",
    "    \"labeling_csv_path_wt\": f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\",\n",
    "    \"WT_img_dir\": f\"{WORKING_DIR}/dataset/WT/images\",\n",
    "\n",
    "    # --- Output Configuration ---\n",
    "    \"analysis_output_dir\": f\"{WORKING_DIR}/images/figure6/WT_analysis\",\n",
    "\n",
    "    # --- New Configuration for this Script ---\n",
    "    \"misclassified_output_dir\": f\"{WORKING_DIR}/images/figure6/WT_analysis/misclassified_samples\",\n",
    "    \"time_point_to_analyze\": 1440,  # Time point (in min) to get features from\n",
    "    \"num_samples_to_find\": 10,  # Number of FPs and FNs to save\n",
    "}\n",
    "\n",
    "# Set the global random seed\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "\n",
    "# --- Helper Functions (from your script) ---\n",
    "\n",
    "def impute_nans_with_previous_frame(trajectory):\n",
    "    \"\"\"Fills NaN values with the value from the previous frame.\"\"\"\n",
    "    for i in range(1, trajectory.shape[0]):\n",
    "        if np.isnan(trajectory[i]).any():\n",
    "            trajectory[i] = trajectory[i - 1]\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def get_closest_frame_index(requested_frame, total_frames):\n",
    "    \"\"\"Gets the closest valid frame index.\"\"\"\n",
    "    if total_frames == 0:\n",
    "        raise ValueError(\"Cannot get frame index from a trajectory with zero frames.\")\n",
    "    max_index = total_frames - 1\n",
    "    return min(requested_frame, max_index)\n",
    "\n",
    "\n",
    "def resize_crop(img_dir, resolution=512):\n",
    "    \"\"\"\n",
    "    Loads an image, and takes a center crop.\n",
    "    (Simplified from your original for this task)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_dir, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    if img.dtype != np.uint8:\n",
    "        img = (img / 256).astype(np.uint8)\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    start_y = max(0, (h - resolution) // 2)\n",
    "    start_x = max(0, (w - resolution) // 2)\n",
    "    new_img = img[start_y:start_y + resolution, start_x:start_x + resolution]\n",
    "\n",
    "    return new_img\n",
    "\n",
    "\n",
    "def find_and_save_frame(run_id, scope_id, class_name, time_point, prob_str, output_path):\n",
    "    \"\"\"\n",
    "    Finds the closest frame, processes it, and saves it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the specific scope directory\n",
    "        run_dir_pattern = os.path.join(CONFIG['WT_img_dir'], f\"*Run{run_id:04d}*\")\n",
    "        matching_run_dirs = glob.glob(run_dir_pattern)\n",
    "        scope_dir_path = \"\"\n",
    "\n",
    "        if matching_run_dirs:\n",
    "            run_dir = matching_run_dirs[0]\n",
    "            scope_dir_path = os.path.join(run_dir, f\"Scope{scope_id:02d}\")\n",
    "        else:\n",
    "            scope_dir_path = os.path.join(CONFIG['WT_img_dir'], f\"Run{run_id:04d}\", f\"Scope{scope_id:02d}\")\n",
    "\n",
    "        if not os.path.isdir(scope_dir_path):\n",
    "            print(f\"  [Warning] Scope directory not found: {scope_dir_path}\")\n",
    "            return\n",
    "\n",
    "        all_images = glob.glob(os.path.join(scope_dir_path, \"*.jpg\"))\n",
    "        if not all_images:\n",
    "            print(f\"  [Warning] No JPG images found in: {scope_dir_path}\")\n",
    "            return\n",
    "\n",
    "        # Find the best matching frame\n",
    "        target_frame_idx = time_point + 1\n",
    "        best_match_path = None\n",
    "        min_diff = float('inf')\n",
    "        frame_number_pattern = re.compile(r'_(\\d+)\\.jpg$')\n",
    "\n",
    "        for img_path in all_images:\n",
    "            match = frame_number_pattern.search(os.path.basename(img_path))\n",
    "            if match:\n",
    "                frame_num = int(match.group(1))\n",
    "                diff = abs(frame_num - target_frame_idx)\n",
    "                if diff < min_diff:\n",
    "                    min_diff = diff\n",
    "                    best_match_path = img_path\n",
    "\n",
    "        if best_match_path:\n",
    "            processed_img = resize_crop(best_match_path)\n",
    "            if processed_img is not None:\n",
    "                # Save the image\n",
    "                dest_filename = f\"{class_name}_Prob{prob_str}_Run{run_id:04d}_Scope{scope_id:02d}.jpg\"\n",
    "                dest_path = os.path.join(output_path, dest_filename)\n",
    "                cv2.imwrite(dest_path, processed_img)\n",
    "                print(f\"  Saved image: {dest_filename}\")\n",
    "            else:\n",
    "                print(f\"  [Warning] Failed to process image: {best_match_path}\")\n",
    "        else:\n",
    "            print(f\"  [Warning] No matching frame found for Run {run_id}, Scope {scope_id} near time {time_point}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Failed to save frame for Run {run_id}, Scope {scope_id}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    if WORKING_DIR == \".\":\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"!!! WARNING: `WORKING_DIR` is not set. Please update it to  !!!\")\n",
    "        print(\"!!! your project's base directory before running this script. !!!\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        return\n",
    "\n",
    "    print(\"--- Finding Representative Misclassified Samples ---\")\n",
    "\n",
    "    # --- 1. Load Data (WT Analysis) ---\n",
    "    print(f\"[1/5] Loading WT data from '{CONFIG['labeling_csv_path_wt']}'...\")\n",
    "    try:\n",
    "        labels_df = pd.read_csv(CONFIG['labeling_csv_path_wt'])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Labeling sheet not found. Check WORKING_DIR and CONFIG paths.\")\n",
    "        return\n",
    "\n",
    "    labels_df = labels_df.dropna(subset=['aggregates_formed'])\n",
    "    labels_df = labels_df[labels_df['aggregates_formed'].isin(['T', 'F'])]\n",
    "    labels_df['label'] = labels_df['aggregates_formed'].map({'T': 1, 'F': 0})\n",
    "\n",
    "    sample_info_list = []\n",
    "    features_list = []\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        run_id, scope_id = row['run_id'], row['scope_id']\n",
    "        npz_path = os.path.join(CONFIG['features_base_dir_wt'], f\"Run{run_id:04d}\", f\"Scope{scope_id:02d}\",\n",
    "                                \"features.npz\")\n",
    "        if not os.path.exists(npz_path):\n",
    "            continue\n",
    "\n",
    "        features_data = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "        features_list.append(features_data)\n",
    "        sample_info_list.append({\n",
    "            'run_id': run_id,\n",
    "            'scope_id': scope_id,\n",
    "            'actual_label': row['label'],\n",
    "            'actual_class': row['aggregates_formed']\n",
    "        })\n",
    "\n",
    "    if not features_list:\n",
    "        print(\"ERROR: No feature files were found. Check CONFIG path.\")\n",
    "        return\n",
    "\n",
    "    # Standardize all trajectories to the same length\n",
    "    min_frames = min(f.shape[0] for f in features_list)\n",
    "    all_trajectories = np.array([f[:min_frames] for f in features_list])\n",
    "\n",
    "    # Re-create labels and info in the same order as trajectories\n",
    "    all_labels = np.array([info['actual_label'] for info in sample_info_list])\n",
    "\n",
    "    print(f\"  Loaded {len(all_labels)} samples. Using min_frames: {min_frames}\")\n",
    "\n",
    "    # --- 2. Get Features at Specific Time ---\n",
    "    time_point = CONFIG['time_point_to_analyze']\n",
    "    t_index = get_closest_frame_index(time_point, min_frames)\n",
    "    features_at_t = all_trajectories[:, t_index, :]\n",
    "    print(f\"[2/5] Extracted features for {features_at_t.shape[0]} samples at time {time_point} min (index {t_index}).\")\n",
    "\n",
    "    # --- 3. Train SVM and Get Probabilities ---\n",
    "    print(\"[3/5] Training SVM on full-dimensional data...\")\n",
    "    pipeline = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=CONFIG[\"random_seed\"])\n",
    "    )\n",
    "    pipeline.fit(features_at_t, all_labels)\n",
    "    all_probas = pipeline.predict_proba(features_at_t)\n",
    "\n",
    "    # Add probabilities to our sample info\n",
    "    for i, info in enumerate(sample_info_list):\n",
    "        info['proba_F'] = all_probas[i, 0]  # Probability of class 0 ('F')\n",
    "        info['proba_T'] = all_probas[i, 1]  # Probability of class 1 ('T')\n",
    "\n",
    "    results_df = pd.DataFrame(sample_info_list)\n",
    "\n",
    "    # --- 4. Identify Misclassified Samples ---\n",
    "    print(\"[4/5] Identifying most confident misclassifications...\")\n",
    "\n",
    "    # Find False Positives (FP): Actual='F'(0), but model predicted 'T'(1) with high confidence\n",
    "    # We sort by 'proba_T' descending (model was *most* sure it was 'T')\n",
    "    fp_df = results_df[results_df['actual_label'] == 0].sort_values(by='proba_T', ascending=False)\n",
    "    top_fps = fp_df.head(CONFIG['num_samples_to_find'])\n",
    "\n",
    "    # Find False Negatives (FN): Actual='T'(1), but model predicted 'F'(0) with high confidence\n",
    "    # We sort by 'proba_T' ascending (model was *least* sure it was 'T')\n",
    "    fn_df = results_df[results_df['actual_label'] == 1].sort_values(by='proba_T', ascending=True)\n",
    "    top_fns = fn_df.head(CONFIG['num_samples_to_find'])\n",
    "\n",
    "    print(\"\\n--- Top False Positives (Actual: F, Predicted: T) ---\")\n",
    "    print(top_fps[['run_id', 'scope_id', 'actual_class', 'proba_T', 'proba_F']])\n",
    "\n",
    "    print(\"\\n--- Top False Negatives (Actual: T, Predicted: F) ---\")\n",
    "    print(top_fns[['run_id', 'scope_id', 'actual_class', 'proba_T', 'proba_F']])\n",
    "\n",
    "    # --- 5. Save Images ---\n",
    "    print(\"\\n[5/5] Saving images for misclassified samples...\")\n",
    "\n",
    "    # Create output directories\n",
    "    fp_output_dir = os.path.join(CONFIG['misclassified_output_dir'], 'False_Positives')\n",
    "    fn_output_dir = os.path.join(CONFIG['misclassified_output_dir'], 'False_Negatives')\n",
    "    os.makedirs(fp_output_dir, exist_ok=True)\n",
    "    os.makedirs(fn_output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nSaving False Positives to: {fp_output_dir}\")\n",
    "    for _, row in top_fps.iterrows():\n",
    "        prob_str = f\"T_{row['proba_T']:.2f}\"\n",
    "        find_and_save_frame(row['run_id'], row['scope_id'], \"Actual_F_Pred\", time_point, prob_str, fp_output_dir)\n",
    "\n",
    "    print(f\"\\nSaving False Negatives to: {fn_output_dir}\")\n",
    "    for _, row in top_fns.iterrows():\n",
    "        prob_str = f\"T_{row['proba_T']:.2f}\"\n",
    "        find_and_save_frame(row['run_id'], row['scope_id'], \"Actual_T_Pred\", time_point, prob_str, fn_output_dir)\n",
    "\n",
    "    print(\"\\n--- Analysis complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "e5194911dc56b392",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding Representative Misclassified Samples ---\n",
      "[1/5] Loading WT data from '/home/xavier/Documents/DAE_project/dataset/WT/labeling_sheet.csv'...\n",
      "  Loaded 352 samples. Using min_frames: 1441\n",
      "[2/5] Extracted features for 352 samples at time 1440 min (index 1440).\n",
      "[3/5] Training SVM on full-dimensional data...\n",
      "[4/5] Identifying most confident misclassifications...\n",
      "\n",
      "--- Top False Positives (Actual: F, Predicted: T) ---\n",
      "     run_id  scope_id actual_class   proba_T   proba_F\n",
      "250     894        39            F  0.857688  0.142312\n",
      "351     925        84            F  0.808554  0.191446\n",
      "237     887        11            F  0.791535  0.208465\n",
      "314     906        26            F  0.724779  0.275221\n",
      "325     913        69            F  0.519442  0.480558\n",
      "231     883         3            F  0.490978  0.509022\n",
      "235     886         9            F  0.455266  0.544734\n",
      "233     884         5            F  0.228781  0.771219\n",
      "262     900        26            F  0.167878  0.832122\n",
      "323     906        35            F  0.147829  0.852171\n",
      "\n",
      "--- Top False Negatives (Actual: T, Predicted: F) ---\n",
      "     run_id  scope_id actual_class   proba_T   proba_F\n",
      "248     893        43            T  0.054122  0.945878\n",
      "198     797        30            T  0.082561  0.917439\n",
      "186     796        44            T  0.092798  0.907202\n",
      "300     904        26            T  0.130058  0.869942\n",
      "280     903        63            T  0.139170  0.860830\n",
      "327     913        71            T  0.175227  0.824773\n",
      "259     897         4            T  0.601145  0.398855\n",
      "208     840         2            T  0.625378  0.374622\n",
      "93      199        20            T  0.641826  0.358174\n",
      "194     797        23            T  0.661524  0.338476\n",
      "\n",
      "[5/5] Saving images for misclassified samples...\n",
      "\n",
      "Saving False Positives to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/misclassified_samples/False_Positives\n",
      "  Saved image: Actual_F_Pred_ProbT_0.86_Run0894_Scope39.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.81_Run0925_Scope84.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.79_Run0887_Scope11.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.72_Run0906_Scope26.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.52_Run0913_Scope69.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.49_Run0883_Scope03.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.46_Run0886_Scope09.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.23_Run0884_Scope05.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.17_Run0900_Scope26.jpg\n",
      "  Saved image: Actual_F_Pred_ProbT_0.15_Run0906_Scope35.jpg\n",
      "\n",
      "Saving False Negatives to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/misclassified_samples/False_Negatives\n",
      "  Saved image: Actual_T_Pred_ProbT_0.05_Run0893_Scope43.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.08_Run0797_Scope30.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.09_Run0796_Scope44.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.13_Run0904_Scope26.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.14_Run0903_Scope63.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.18_Run0913_Scope71.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.60_Run0897_Scope04.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.63_Run0840_Scope02.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.64_Run0199_Scope20.jpg\n",
      "  Saved image: Actual_T_Pred_ProbT_0.66_Run0797_Scope23.jpg\n",
      "\n",
      "--- Analysis complete ---\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combine bar plot",
   "id": "dc78c6906d9e7022"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:40:27.596011Z",
     "start_time": "2025-09-27T23:40:27.223190Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Combined AUC Plot Generation ---\n",
      "Loading motility data from: D:/Projects/DAE_project/images/figure6/motility_analysis/motility_A+S-_vs_A-S+_auc_scores.csv\n",
      "Loading WT data from: D:/Projects/DAE_project/images/figure6/WT_analysis/WT_auc_scores.csv\n",
      "Plotting data for labels: ['A+S- vs A-S+\\n24 h', 'A+S- vs A-S+\\n0 h', 'WT Agg. vs No Agg.\\n24 h', 'WT Agg. vs No Agg.\\n0 h']\n",
      "Generating the bar plot...\n",
      "Successfully saved plot to: D:/Projects/DAE_project/images/figure6/motility_analysis/combined_auc_barplot.pdf\n",
      "--- Script finished ---\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration\n",
    "# ==============================================================================\n",
    "# --- File Paths ---\n",
    "# Assumes the CSV files are in the same directory as the script.\n",
    "# If not, provide the full path to the files.\n",
    "motility_csv_path = f'{WORKING_DIR}/images/figure6/motility_analysis/motility_A+S-_vs_A-S+_auc_scores.csv'\n",
    "wt_csv_path = f'{WORKING_DIR}/images/figure6/WT_analysis/WT_auc_scores.csv'\n",
    "output_plot_path = f'{WORKING_DIR}/images/figure6/motility_analysis/combined_auc_barplot.pdf'\n",
    "\n",
    "# --- Plotting Parameters ---\n",
    "time_points_to_plot = [1440, 0]  # The order of time points for each group\n",
    "bar_colors = ['#812db3', '#812db3', '#51ab4f', '#51ab4f']\n",
    "figure_size = (12, 6)\n",
    "y_axis_limit = [0.45, 1.05]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Script\n",
    "# ==============================================================================\n",
    "\n",
    "def create_combined_auc_plot():\n",
    "    \"\"\"\n",
    "    Loads AUC score data from two CSV files and plots them on a single\n",
    "    bar chart in a specified order.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Combined AUC Plot Generation ---\")\n",
    "\n",
    "    # --- 1. Load and Validate Data ---\n",
    "    if not os.path.exists(motility_csv_path):\n",
    "        print(f\"ERROR: Motility data file not found at '{motility_csv_path}'\")\n",
    "        return\n",
    "    if not os.path.exists(wt_csv_path):\n",
    "        print(f\"ERROR: WT data file not found at '{wt_csv_path}'\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading motility data from: {motility_csv_path}\")\n",
    "    motility_df = pd.read_csv(motility_csv_path)\n",
    "\n",
    "    print(f\"Loading WT data from: {wt_csv_path}\")\n",
    "    wt_df = pd.read_csv(wt_csv_path)\n",
    "\n",
    "    # --- 2. Prepare Data for Plotting ---\n",
    "    plot_data = {\n",
    "        'labels': [],\n",
    "        'means': [],\n",
    "        'stds': []\n",
    "    }\n",
    "\n",
    "    # Extract Motility data in the specified order\n",
    "    for time in time_points_to_plot:\n",
    "        row = motility_df[motility_df['Time (min)'] == time]\n",
    "        if not row.empty:\n",
    "            plot_data['labels'].append(f'A+S- vs A-S+\\n{time // 60} h')\n",
    "            plot_data['means'].append(row['Mean AUC'].iloc[0])\n",
    "            plot_data['stds'].append(row['Std Dev AUC'].iloc[0])\n",
    "        else:\n",
    "            print(f\"Warning: Time point {time} min not found in motility data.\")\n",
    "\n",
    "    # Extract WT data in the specified order\n",
    "    for time in time_points_to_plot:\n",
    "        row = wt_df[wt_df['Time (min)'] == time]\n",
    "        if not row.empty:\n",
    "            plot_data['labels'].append(f'WT Agg. vs No Agg.\\n{time // 60} h')\n",
    "            plot_data['means'].append(row['Mean AUC'].iloc[0])\n",
    "            plot_data['stds'].append(row['Std Dev AUC'].iloc[0])\n",
    "        else:\n",
    "            print(f\"Warning: Time point {time} min not found in WT data.\")\n",
    "\n",
    "    if not plot_data['means']:\n",
    "        print(\"ERROR: No data was extracted for plotting. Please check CSV files and time points.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Plotting data for labels: {plot_data['labels']}\")\n",
    "\n",
    "    # --- 3. Calculate Asymmetric 95% Confidence Intervals ---\n",
    "    y_err_lower = []\n",
    "    y_err_upper = []\n",
    "    for mean, std in zip(plot_data['means'], plot_data['stds']):\n",
    "        margin = 1.96 * std  # 95% CI margin\n",
    "        upper_error = min(mean + margin, 1.0) - mean\n",
    "        lower_error = mean - max(mean - margin, 0.0)\n",
    "        y_err_upper.append(upper_error)\n",
    "        y_err_lower.append(lower_error)\n",
    "\n",
    "    asymmetric_error = [y_err_lower, y_err_upper]\n",
    "\n",
    "    # --- 4. Generate the Plot ---\n",
    "    print(\"Generating the bar plot...\")\n",
    "    fig, ax = plt.subplots(figsize=figure_size)\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    x_pos = np.arange(len(plot_data['labels']))\n",
    "\n",
    "    ax.bar(x_pos, plot_data['means'], yerr=asymmetric_error,\n",
    "           color=bar_colors, capsize=5, ecolor='gray', zorder=2)\n",
    "\n",
    "    ax.set_ylabel('Mean AUC Score')\n",
    "    # ax.set_title('Comparison of Predictive Power')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(plot_data['labels'])\n",
    "    ax.set_ylim(y_axis_limit)\n",
    "\n",
    "    # Add a line for random chance\n",
    "    ax.axhline(y=0.5, color='r', linestyle='--', label='Random Chance (AUC=0.5)')\n",
    "\n",
    "    # Create custom legend handles\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=bar_colors[0], edgecolor=bar_colors[0], label='A+S- vs A-S+'),\n",
    "        Patch(facecolor=bar_colors[len(time_points_to_plot)], edgecolor=bar_colors[len(time_points_to_plot)],\n",
    "              label='WT Agg. vs No Agg.')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- 5. Save the Plot ---\n",
    "    try:\n",
    "        fig.savefig(output_plot_path, dpi=300)\n",
    "        print(f\"Successfully saved plot to: {output_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save the plot. Reason: {e}\")\n",
    "\n",
    "    plt.close(fig)\n",
    "    print(\"--- Script finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_combined_auc_plot()\n"
   ],
   "id": "c3b1b3642007a5bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:47:43.750822Z",
     "start_time": "2025-09-28T06:47:40.555034Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting WT Analysis for Reconstruction ---\n",
      "Comparing classes: 'No_Aggregate' (F) vs. 'Aggregate' (T)\n",
      "\n",
      "--- Starting Image Reconstruction ---\n",
      "Calculating mean feature vectors...\n",
      "  - Calculated mean for 132 'No_Aggregate' samples.\n",
      "  - Calculated mean for 220 'Aggregate' samples.\n",
      "  - Calculated midpoint vector.\n",
      "Using device: cuda\n",
      "Loading generator from '/home/xavier/Documents/DAE_project/models/network-snapshot-001512-patched.pkl'...\n",
      "Synthesizing images from mean vectors...\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "Saving images to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/centers/WT_T0_No_Aggregate_vs_Aggregate\n",
      "  - Saved: reconstruction_No_Aggregate.png\n",
      "  - Saved: reconstruction_Aggregate.png\n",
      "  - Saved: reconstruction_Midpoint.png\n",
      "--- Reconstruction complete! ---\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "# ==============================================================================\n",
    "# Standalone Script for Image Reconstruction from Mean Features\n",
    "# ==============================================================================\n",
    "# Purpose:\n",
    "# This script loads encoded features for specified classes, calculates the\n",
    "# mean feature vector for each class and their midpoint, and then uses a\n",
    "# pre-trained generator network (e.g., StyleGAN) to synthesize representative\n",
    "# images from these vectors.\n",
    "#\n",
    "# It is designed to be independent of the main analysis pipeline.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "CONFIG = {\n",
    "    # --- Analysis Target ---\n",
    "    \"analysis_type\": \"WT\",  # \"WT\" or \"motility\"\n",
    "    \"time_point_to_reconstruct\": 0,  # Time in minutes (e.g., 0, 720, 1440)\n",
    "\n",
    "    # --- Paths ---\n",
    "    \"network_pkl_path\": f\"{WORKING_DIR}/models/network-snapshot-001512-patched.pkl\",\n",
    "    \"output_dir\": f\"{WORKING_DIR}/images/figure6/WT_analysis/centers\",\n",
    "\n",
    "    # WT analysis paths\n",
    "    \"features_base_dir_wt\": f\"{WORKING_DIR}/encoded_features/WT_features\",\n",
    "    \"labeling_csv_path_wt\": f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\",\n",
    "\n",
    "    # Motility analysis paths\n",
    "    \"features_base_dir_motility\": f\"{WORKING_DIR}/encoded_features/Roy_training_features\",\n",
    "    \"motility_csv_path\": f'{WORKING_DIR}/dataset/Roy_training/merged_strain_data.xlsx',\n",
    "\n",
    "    # --- Class Selection ---\n",
    "    # For \"WT\" analysis_type\n",
    "    \"wt_classes_to_compare\": ('F', 'T'),  # ('F' = No Aggregate, 'T' = Aggregate)\n",
    "\n",
    "    # For \"motility\" analysis_type\n",
    "    \"motility_classes_to_compare\": ('WT', 'A-S-'),  # e.g., ('WT', 'A+S-'), ('A-S+', 'A-S-')\n",
    "}\n",
    "\n",
    "os.environ['CC'] = \"/usr/bin/gcc-9\"\n",
    "os.environ['CXX'] = \"/usr/bin/g++-9\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ---                          HELPER FUNCTIONS                            ---\n",
    "# ==============================================================================\n",
    "\n",
    "def impute_nans_with_previous_frame(trajectory):\n",
    "    \"\"\"Fills NaN values in a trajectory with the values from the last valid frame.\"\"\"\n",
    "    for i in range(1, trajectory.shape[0]):\n",
    "        if np.isnan(trajectory[i]).any():\n",
    "            trajectory[i] = trajectory[i - 1]\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def get_closest_frame_index(requested_frame, total_frames):\n",
    "    \"\"\"Finds the valid index for a requested frame number.\"\"\"\n",
    "    if total_frames == 0:\n",
    "        raise ValueError(\"Cannot get frame index from a trajectory with zero frames.\")\n",
    "    max_index = total_frames - 1\n",
    "    return min(requested_frame, max_index)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ---                        CORE RECONSTRUCTION LOGIC                     ---\n",
    "# ==============================================================================\n",
    "\n",
    "def reconstruct_images(features_c0, features_c1, class_name_c0, class_name_c1, config):\n",
    "    \"\"\"\n",
    "    Calculates mean vectors and generates images using the StyleGAN generator.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Image Reconstruction ---\")\n",
    "\n",
    "    if len(features_c0) == 0 or len(features_c1) == 0:\n",
    "        print(f\"Error: Not enough data for one or both classes. \"\n",
    "              f\"Found {len(features_c0)} samples for '{class_name_c0}' and \"\n",
    "              f\"{len(features_c1)} for '{class_name_c1}'. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # 1. Calculate Mean Feature Vectors\n",
    "    print(\"Calculating mean feature vectors...\")\n",
    "    mean_feature_c0 = np.mean(features_c0, axis=0)\n",
    "    mean_feature_c1 = np.mean(features_c1, axis=0)\n",
    "    mean_of_means = (mean_feature_c0 + mean_feature_c1) / 2.0\n",
    "    print(f\"  - Calculated mean for {len(features_c0)} '{class_name_c0}' samples.\")\n",
    "    print(f\"  - Calculated mean for {len(features_c1)} '{class_name_c1}' samples.\")\n",
    "    print(\"  - Calculated midpoint vector.\")\n",
    "\n",
    "    # 2. Setup Device and Load Generator\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(f\"Loading generator from '{config['network_pkl_path']}'...\")\n",
    "    try:\n",
    "        with dnnlib.util.open_url(config['network_pkl_path']) as fp:\n",
    "            models = legacy.load_network_pkl(fp)\n",
    "            G = models['G_ema'].to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not load the network PKL file. Check the path. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Generate Images\n",
    "    print(\"Synthesizing images from mean vectors...\")\n",
    "    batch_zs = np.vstack([mean_feature_c0, mean_feature_c1, mean_of_means])\n",
    "    batch_zs_tensor = torch.from_numpy(batch_zs).to(device)\n",
    "\n",
    "    # Assumes an unconditional generator (class labels `c` is None)\n",
    "    synth_images = G(batch_zs_tensor, None, noise_mode=\"const\")\n",
    "\n",
    "    # 4. Post-process and Save Images\n",
    "    synth_images = (synth_images + 1) * 127.5  # Denormalize from [-1, 1] to [0, 255]\n",
    "    synth_images = synth_images.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "\n",
    "    # Create a unique sub-directory for this reconstruction run\n",
    "    output_subdir_name = f\"{config['analysis_type']}_T{config['time_point_to_reconstruct']}_{class_name_c0}_vs_{class_name_c1}\"\n",
    "    final_output_dir = os.path.join(config['output_dir'], output_subdir_name)\n",
    "    os.makedirs(final_output_dir, exist_ok=True)\n",
    "    print(f\"Saving images to: {final_output_dir}\")\n",
    "\n",
    "    # Define filenames and save\n",
    "    paths = {\n",
    "        \"class0\": os.path.join(final_output_dir, f\"reconstruction_{class_name_c0}.png\"),\n",
    "        \"class1\": os.path.join(final_output_dir, f\"reconstruction_{class_name_c1}.png\"),\n",
    "        \"midpoint\": os.path.join(final_output_dir, f\"reconstruction_Midpoint.png\"),\n",
    "    }\n",
    "\n",
    "    # OpenCV expects BGR format, so convert from RGB\n",
    "    cv2.imwrite(paths[\"class0\"], cv2.cvtColor(synth_images[0], cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(paths[\"class1\"], cv2.cvtColor(synth_images[1], cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(paths[\"midpoint\"], cv2.cvtColor(synth_images[2], cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"  - Saved: {os.path.basename(paths['class0'])}\")\n",
    "    print(f\"  - Saved: {os.path.basename(paths['class1'])}\")\n",
    "    print(f\"  - Saved: {os.path.basename(paths['midpoint'])}\")\n",
    "    print(\"--- Reconstruction complete! ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ---                            MAIN EXECUTION                            ---\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load data and orchestrate the reconstruction.\"\"\"\n",
    "    cfg = CONFIG\n",
    "\n",
    "    # --- Data Loading Logic ---\n",
    "    if cfg['analysis_type'] == 'WT':\n",
    "        print(f\"--- Starting WT Analysis for Reconstruction ---\")\n",
    "        class0_label, class1_label = cfg['wt_classes_to_compare']\n",
    "        class_name_map = {'F': 'No_Aggregate', 'T': 'Aggregate'}\n",
    "        class_name_c0, class_name_c1 = class_name_map[class0_label], class_name_map[class1_label]\n",
    "\n",
    "        print(f\"Comparing classes: '{class_name_c0}' ({class0_label}) vs. '{class_name_c1}' ({class1_label})\")\n",
    "\n",
    "        try:\n",
    "            labels_df = pd.read_csv(cfg['labeling_csv_path_wt'])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Labeling sheet not found at '{cfg['labeling_csv_path_wt']}'.\")\n",
    "            return\n",
    "\n",
    "        labels_df = labels_df[labels_df['aggregates_formed'].isin([class0_label, class1_label])]\n",
    "\n",
    "        features_c0, features_c1 = [], []\n",
    "\n",
    "        for _, row in labels_df.iterrows():\n",
    "            run_id, scope_id = row['run_id'], row['scope_id']\n",
    "            npz_path = os.path.join(cfg['features_base_dir_wt'], f\"Run{run_id:04d}\", f\"Scope{scope_id:02d}\",\n",
    "                                    \"features.npz\")\n",
    "\n",
    "            if not os.path.exists(npz_path): continue\n",
    "\n",
    "            features_data = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "            frame_idx = get_closest_frame_index(cfg['time_point_to_reconstruct'], features_data.shape[0])\n",
    "            feature_vec = features_data[frame_idx]\n",
    "\n",
    "            if row['aggregates_formed'] == class0_label:\n",
    "                features_c0.append(feature_vec)\n",
    "            else:\n",
    "                features_c1.append(feature_vec)\n",
    "\n",
    "        reconstruct_images(features_c0, features_c1, class_name_c0, class_name_c1, cfg)\n",
    "\n",
    "    elif cfg['analysis_type'] == 'motility':\n",
    "        print(f\"--- Starting Motility Analysis for Reconstruction ---\")\n",
    "        class_name_c0, class_name_c1 = cfg['motility_classes_to_compare']\n",
    "        print(f\"Comparing classes: '{class_name_c0}' vs. '{class_name_c1}'\")\n",
    "\n",
    "        try:\n",
    "            labels_df = pd.read_excel(cfg['motility_csv_path'])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Motility data not found at '{cfg['motility_csv_path']}'.\")\n",
    "            return\n",
    "\n",
    "        strain_to_label_map = pd.Series(labels_df.motility.values, index=labels_df.Strain).to_dict()\n",
    "        dir_pattern = re.compile(r'Run(\\d+)_Mutant(\\d+)')\n",
    "\n",
    "        features_c0, features_c1 = [], []\n",
    "\n",
    "        for dir_name in os.listdir(cfg['features_base_dir_motility']):\n",
    "            match = dir_pattern.match(dir_name)\n",
    "            if not match: continue\n",
    "\n",
    "            mutant_num = int(match.group(2))\n",
    "            strain_id = f\"DK{mutant_num}\"\n",
    "\n",
    "            if strain_id in strain_to_label_map:\n",
    "                label = strain_to_label_map[strain_id]\n",
    "                if label not in [class_name_c0, class_name_c1]: continue\n",
    "\n",
    "                # Find all scope directories within this run/mutant folder\n",
    "                run_mutant_path = os.path.join(cfg['features_base_dir_motility'], dir_name)\n",
    "                for scope_dir_name in os.listdir(run_mutant_path):\n",
    "                    npz_path = os.path.join(run_mutant_path, scope_dir_name, \"features.npz\")\n",
    "                    if os.path.exists(npz_path):\n",
    "                        features_data = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "                        frame_idx = get_closest_frame_index(cfg['time_point_to_reconstruct'], features_data.shape[0])\n",
    "                        feature_vec = features_data[frame_idx]\n",
    "\n",
    "                        if label == class_name_c0:\n",
    "                            features_c0.append(feature_vec)\n",
    "                        else:\n",
    "                            features_c1.append(feature_vec)\n",
    "\n",
    "        reconstruct_images(features_c0, features_c1, class_name_c0, class_name_c1, cfg)\n",
    "\n",
    "    else:\n",
    "        print(f\"ERROR: Unknown analysis type '{cfg['analysis_type']}'. Choose 'WT' or 'motility'.\")\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(CONFIG[\"network_pkl_path\"]):\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"!!! WARNING: `network_pkl_path` is not set or file not found.   !!!\")\n",
    "        print(\"!!! Please update the CONFIG section before running this script.  !!!\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    else:\n",
    "        main()\n"
   ],
   "id": "37c6e5c78805a5f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optional",
   "id": "7ba977b1d7ab00cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reconstruct label centers",
   "id": "7e72ccae922facfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sample images",
   "id": "6b3f36392452be3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:58:39.886035Z",
     "start_time": "2025-09-28T06:58:39.715062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Script: Image Sampler and Processor\n",
    "# ==============================================================================\n",
    "# Purpose:\n",
    "# This script processes image files by first sampling from a labeling sheet.\n",
    "# 1. Loads a CSV file that classifies experiments (by run_id and scope_id)\n",
    "#    into different classes (e.g., aggregates formed 'T' or 'F').\n",
    "# 2. Randomly samples a specified number of experiments from each class.\n",
    "# 3. For each sampled experiment, it processes a list of specified frame numbers.\n",
    "# 4. For each frame, it locates the image, resizes, crops, normalizes brightness,\n",
    "#    and saves the result as a PNG file.\n",
    "#\n",
    "# Instructions:\n",
    "# - Update the CONFIG dictionary with your specific parameters.\n",
    "# - 'labeling_csv_path': Path to the CSV file with run/scope classifications.\n",
    "# - 'samples_per_class': How many experiments to randomly sample from each class.\n",
    "# - 'target_frames_to_process': A list of frame numbers to process for each\n",
    "#   sampled experiment.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from scikit-image about low contrast images\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='skimage.io')\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    \"base_image_dir\": f\"{WORKING_DIR}/dataset/WT/images\",\n",
    "    \"output_dir\": f\"{WORKING_DIR}/images/figure6/WT_analysis/sampled\",\n",
    "    \"labeling_csv_path\": f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\",\n",
    "\n",
    "    # --- Sampling Parameters ---\n",
    "    \"samples_per_class\": 3,  # Number of experiments to sample from each class\n",
    "    \"target_frames_to_process\": [1, 1441],  # Frames to process for each sampled experiment\n",
    "    \"random_seed\": 42,  # Seed for reproducible random sampling\n",
    "\n",
    "    # --- Image Processing Parameters ---\n",
    "    \"resize_by\": 1.0,\n",
    "    \"resolution\": 512,\n",
    "    \"brightness_norm\": True,\n",
    "    \"brightness_mean\": 107.2,\n",
    "    \"locations\": [\"center\"],  # Can be a list, e.g., [\"left\", \"center\", \"right\"]\n",
    "    \"crop_offset\": 128,\n",
    "}\n",
    "\n",
    "\n",
    "def resize_crop(img_name, strain_dir, resize_by=1.0, resolution=512, brightness_norm=True, brightness_mean=107.2,\n",
    "                locations=None, crop_offset=128):\n",
    "    \"\"\"\n",
    "    Loads, resizes, and crops an image from multiple locations.\n",
    "    \"\"\"\n",
    "    if locations is None:\n",
    "        locations = [\"center\"]\n",
    "    img_path = os.path.join(strain_dir, img_name)\n",
    "    if not os.path.exists(img_path): return None\n",
    "\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None: return None\n",
    "    if img.dtype != np.uint8: img = np.uint8(img / 256)\n",
    "\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    resize_w, resize_h = int(img_w * resize_by), int(img_h * resize_by)\n",
    "\n",
    "    if resize_by != 1.0:\n",
    "        img = cv2.resize(img, (resize_w, resize_h), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    cropped_imgs = []\n",
    "    for location in locations:\n",
    "        y_start = (resize_h - resolution) // 2\n",
    "        y_end = y_start + resolution\n",
    "\n",
    "        if location == \"left\":\n",
    "            x_start = crop_offset\n",
    "        elif location == \"right\":\n",
    "            x_start = resize_w - crop_offset - resolution\n",
    "        else:  # \"center\" or default\n",
    "            x_start = (resize_w - resolution) // 2\n",
    "\n",
    "        x_end = x_start + resolution\n",
    "        new_img = img[y_start:y_end, x_start:x_end]\n",
    "\n",
    "        if brightness_norm:\n",
    "            obj_v = np.mean(new_img)\n",
    "            value = brightness_mean - obj_v\n",
    "            new_img = cv2.add(new_img, value)\n",
    "        cropped_imgs.append((new_img, location))  # Return image and its location\n",
    "    return cropped_imgs\n",
    "\n",
    "\n",
    "def process_and_save_frame(run_id, scope_id, frame_num, label, base_dir, output_dir, processing_params):\n",
    "    \"\"\"\n",
    "    Loads, processes, and saves a single image frame.\n",
    "\n",
    "    Args:\n",
    "        run_id (int): The run identifier.\n",
    "        scope_id (int): The scope identifier.\n",
    "        frame_num (int): The specific frame number to process.\n",
    "        label (str): The class label for the experiment (e.g., 'T' or 'F').\n",
    "        base_dir (str): The root directory containing the image data.\n",
    "        output_dir (str): The directory where the output .png files will be saved.\n",
    "        processing_params (dict): A dictionary with image processing settings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. Find the correct run folder and construct the image path ---\n",
    "        run_suffix = f\"Run{run_id:04d}\"\n",
    "        run_folder_name = None\n",
    "\n",
    "        try:\n",
    "            run_folder_name = next(\n",
    "                d for d in os.listdir(base_dir) if d.endswith(run_suffix) and os.path.isdir(os.path.join(base_dir, d)))\n",
    "        except StopIteration:\n",
    "            print(f\"Warning: No folder found ending with '{run_suffix}' in '{base_dir}'. Skipping frame {frame_num}.\")\n",
    "            return\n",
    "\n",
    "        scope_dir = os.path.join(base_dir, run_folder_name, f\"Scope{scope_id:02d}\")\n",
    "        image_filename = f\"Run{run_id:04d}_scope{scope_id:d}-00_{frame_num:04d}.jpg\"\n",
    "\n",
    "        # --- 2. Process the image using the resize_crop function ---\n",
    "        cropped_results = resize_crop(\n",
    "            img_name=image_filename,\n",
    "            strain_dir=scope_dir,\n",
    "            **processing_params\n",
    "        )\n",
    "\n",
    "        if not cropped_results:\n",
    "            print(\n",
    "                f\"Warning: Cropping failed for run {run_id}, scope {scope_id}, frame {frame_num}. Image might not exist or be invalid.\")\n",
    "            return\n",
    "\n",
    "        # --- 3. Save the resulting image(s) ---\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for img, location in cropped_results:\n",
    "            # Construct a descriptive output filename including the class label\n",
    "            output_filename = f\"run{run_id:04d}_scope{scope_id:02d}_class{label}_{frame_num:04d}_{location}.png\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "            cv2.imwrite(output_path, img)\n",
    "            print(f\"Successfully processed frame {frame_num} ({location}). Saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing run {run_id}, scope {scope_id}, frame {frame_num}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to sample experiments and process frames.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Image Sampler and Processor Script ---\")\n",
    "\n",
    "    # --- 1. Load and prepare the labeling data ---\n",
    "    try:\n",
    "        labels_df = pd.read_csv(CONFIG['labeling_csv_path'])\n",
    "        labels_df = labels_df.dropna(subset=['aggregates_formed', 'run_id', 'scope_id'])\n",
    "        labels_df = labels_df[labels_df['aggregates_formed'].isin(['T', 'F'])]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Labeling sheet not found at '{CONFIG['labeling_csv_path']}'. Exiting.\")\n",
    "        return\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: The CSV file is missing a required column: {e}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Randomly sample from each class ---\n",
    "    samples_per_class = CONFIG['samples_per_class']\n",
    "    class_T_df = labels_df[labels_df['aggregates_formed'] == 'T']\n",
    "    class_F_df = labels_df[labels_df['aggregates_formed'] == 'F']\n",
    "\n",
    "    if len(class_T_df) < samples_per_class or len(class_F_df) < samples_per_class:\n",
    "        print(\"Warning: Not enough samples in the CSV for the requested number.\")\n",
    "        print(f\"  - Class 'T' has {len(class_T_df)} samples.\")\n",
    "        print(f\"  - Class 'F' has {len(class_F_df)} samples.\")\n",
    "        print(f\"  - Requested {samples_per_class} samples per class.\")\n",
    "        # Adjusting sample count to the minimum available\n",
    "        samples_per_class = min(len(class_T_df), len(class_F_df))\n",
    "        if samples_per_class == 0:\n",
    "            print(\"ERROR: Cannot proceed with 0 samples in one of the classes. Exiting.\")\n",
    "            return\n",
    "        print(f\"  Proceeding with {samples_per_class} samples per class.\")\n",
    "\n",
    "    sampled_T = class_T_df.sample(n=samples_per_class, random_state=CONFIG['random_seed'])\n",
    "    sampled_F = class_F_df.sample(n=samples_per_class, random_state=CONFIG['random_seed'])\n",
    "\n",
    "    combined_samples = pd.concat([sampled_T, sampled_F])\n",
    "    print(f\"\\nSuccessfully sampled {len(combined_samples)} total experiments.\")\n",
    "\n",
    "    # --- 3. Process the sampled frames ---\n",
    "    base_dir = CONFIG[\"base_image_dir\"]\n",
    "    output_dir = CONFIG[\"output_dir\"]\n",
    "    target_frames = CONFIG[\"target_frames_to_process\"]\n",
    "\n",
    "    processing_params = {\n",
    "        \"resize_by\": CONFIG[\"resize_by\"],\n",
    "        \"resolution\": CONFIG[\"resolution\"],\n",
    "        \"brightness_norm\": CONFIG[\"brightness_norm\"],\n",
    "        \"brightness_mean\": CONFIG[\"brightness_mean\"],\n",
    "        \"locations\": CONFIG[\"locations\"],\n",
    "        \"crop_offset\": CONFIG[\"crop_offset\"],\n",
    "    }\n",
    "\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Base Directory: {base_dir}\")\n",
    "    print(f\"  - Output Directory: {output_dir}\")\n",
    "    print(f\"  - Frames to process per sample: {target_frames}\")\n",
    "\n",
    "    # Loop through each sampled experiment\n",
    "    for index, row in combined_samples.iterrows():\n",
    "        run_id = int(row['run_id'])\n",
    "        scope_id = int(row['scope_id'])\n",
    "        label = row['aggregates_formed']\n",
    "\n",
    "        print(f\"\\nProcessing sampled experiment: Run {run_id}, Scope {scope_id} (Class: {label})\")\n",
    "\n",
    "        # Loop through each specified frame number for that experiment\n",
    "        for frame in target_frames:\n",
    "            process_and_save_frame(run_id, scope_id, frame, label, base_dir, output_dir, processing_params)\n",
    "\n",
    "    print(\"\\n--- Script execution complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "123d14a12d5bc30e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Image Sampler and Processor Script ---\n",
      "\n",
      "Successfully sampled 6 total experiments.\n",
      "Configuration:\n",
      "  - Base Directory: /home/xavier/Documents/DAE_project/dataset/WT/images\n",
      "  - Output Directory: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis\n",
      "  - Frames to process per sample: [1, 1441]\n",
      "\n",
      "Processing sampled experiment: Run 220, Scope 46 (Class: T)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0220_scope46_classT_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0220_scope46_classT_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 373, Scope 40 (Class: T)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0373_scope40_classT_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0373_scope40_classT_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 199, Scope 21 (Class: T)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0199_scope21_classT_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0199_scope21_classT_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 900, Scope 27 (Class: F)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0900_scope27_classF_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0900_scope27_classF_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 903, Scope 77 (Class: F)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0903_scope77_classF_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0903_scope77_classF_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 825, Scope 22 (Class: F)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0825_scope22_classF_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0825_scope22_classF_1441_center.png\n",
      "\n",
      "--- Script execution complete ---\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b8eb14e52303743"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
