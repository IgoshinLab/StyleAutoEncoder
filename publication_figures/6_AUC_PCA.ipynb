{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T19:57:16.302248Z",
     "start_time": "2025-09-28T19:57:16.300129Z"
    }
   },
   "cell_type": "code",
   "source": "WORKING_DIR = \"/home/xavier/Documents/DAE_project\"",
   "id": "55eab237e2cb350a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Process tables",
   "id": "2bb31c3b05de2d0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Merge motility labels",
   "id": "be6715129a8bc643"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T00:51:58.304420Z",
     "start_time": "2025-09-29T00:51:55.087373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "WORKING_DIR = \"/home/xavier/Documents/DAE_project\"\n",
    "\n",
    "# --- File Paths ---\n",
    "EXPERIMENT_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/Caro 3d 9.7.22_2.20_new.xlsx'\n",
    "UPDATED_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/Updated_with_He_et_al__1994.xlsx'\n",
    "KAISER_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/Kaiser strain list at UCD.xls'\n",
    "IMAGES_BASE_PATH = f'{WORKING_DIR}/dataset/Roy_training/images/'\n",
    "\n",
    "OUTPUT_FULL_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/merged_strain_data_full.xlsx'\n",
    "OUTPUT_FILE_PATH = f'{WORKING_DIR}/dataset/Roy_training/merged_strain_data.xlsx'\n",
    "\n",
    "# --- Data Loading ---\n",
    "try:\n",
    "    print(\"Loading data files...\")\n",
    "    experiment_df = pd.read_excel(EXPERIMENT_FILE_PATH)\n",
    "    updated_df = pd.read_excel(UPDATED_FILE_PATH)\n",
    "    kaiser_df = pd.read_excel(KAISER_FILE_PATH)\n",
    "    print(\"All files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure all files are in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Processing the 'Updated' Table ---\n",
    "print(\"Processing the 'Updated' table...\")\n",
    "\n",
    "# 1. Select and filter columns\n",
    "updated_df_processed = updated_df[\n",
    "    ['Run', 'Mutant #', 'Movies', 'Reference', 'Jiangguo', 'Source_labelled', 'Source', 'Bib']].copy()\n",
    "\n",
    "# 2. Keep rows where 'Run' has a value and remove duplicates\n",
    "updated_df_processed.dropna(subset=['Run'], inplace=True)\n",
    "updated_df_processed.drop_duplicates(inplace=True)\n",
    "print(f\"Filtered down to {len(updated_df_processed)} unique rows with 'Run' values.\")\n",
    "\n",
    "\n",
    "# 3. Clean the 'Mutant #' column to create 'Strain'\n",
    "def clean_mutant_id(mutant_id):\n",
    "    \"\"\"\n",
    "    Cleans the Mutant ID based on specified rules:\n",
    "    - If an entry has a letter-digit pattern (e.g., DK1622A), it keeps the pattern and removes the rest (-> DK1622).\n",
    "    - If an entry starts with a digit, it prepends 'DK'.\n",
    "    \"\"\"\n",
    "    if pd.isna(mutant_id):\n",
    "        return None\n",
    "\n",
    "    mutant_id_str = str(mutant_id)\n",
    "    parts = [part.strip() for part in mutant_id_str.split(',')]\n",
    "\n",
    "    for part in parts:\n",
    "        if not part:\n",
    "            continue\n",
    "        match = re.match(r'(DK\\d+)', part)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        # Check if the first character is a digit\n",
    "        if part and part[0].isdigit():\n",
    "            return f'DK{part}'\n",
    "    return parts[0] if parts else None\n",
    "\n",
    "\n",
    "updated_df_processed['Strain'] = updated_df_processed['Mutant #'].apply(clean_mutant_id)\n",
    "print(\"Cleaned 'Mutant #' column into 'Strain'.\")\n",
    "\n",
    "\n",
    "# --- Calculate Final Movie Count from Directories ---\n",
    "def calculate_final_movies(run_ids_str, images_path):\n",
    "    \"\"\"\n",
    "    Calculates the total number of movies by counting subdirectories for each run ID.\n",
    "    \"\"\"\n",
    "    if pd.isna(run_ids_str) or not os.path.isdir(images_path):\n",
    "        return 0\n",
    "\n",
    "    total_movies = 0\n",
    "    try:\n",
    "        run_ids = [int(float(run_id.strip())) for run_id in str(run_ids_str).split(',') if run_id.strip()]\n",
    "    except (ValueError, AttributeError):\n",
    "        return 0\n",
    "\n",
    "    # Get a list of all items in the images directory once for efficiency\n",
    "    all_image_folders = os.listdir(images_path)\n",
    "\n",
    "    for run_id in run_ids:\n",
    "        # Pad run_id to 4 digits to match folder naming convention, e.g., 1 -> 0001\n",
    "        run_id_str_padded = str(run_id).zfill(4)\n",
    "        for folder_name in all_image_folders:\n",
    "            if folder_name.endswith(run_id_str_padded):\n",
    "                run_folder_path = os.path.join(images_path, folder_name)\n",
    "                if os.path.isdir(run_folder_path):\n",
    "                    # Count subdirectories inside the run folder\n",
    "                    num_subfolders = sum(\n",
    "                        os.path.isdir(os.path.join(run_folder_path, item)) for item in os.listdir(run_folder_path))\n",
    "                    total_movies += num_subfolders\n",
    "                    break  # Move to the next run_id once the folder is found\n",
    "    return total_movies\n",
    "\n",
    "\n",
    "print(\"Calculating final movie counts from image directories...\")\n",
    "updated_df_processed['Final Movies'] = updated_df_processed['Run'].apply(\n",
    "    lambda run_ids: calculate_final_movies(run_ids, IMAGES_BASE_PATH)\n",
    ")\n",
    "print(\"Added 'Final Movies' column.\")\n",
    "\n",
    "# --- Retrieving Original Data from Experiment Table ---\n",
    "print(\"Looking up original data from the experiment file...\")\n",
    "experiment_df.dropna(subset=['Run'], inplace=True)\n",
    "experiment_df['Run'] = experiment_df['Run'].astype(int)\n",
    "\n",
    "\n",
    "def get_original_info(run_ids_str, exp_df):\n",
    "    \"\"\"\n",
    "    Looks up run IDs in the experiment_df and concatenates original mutant numbers and sources.\n",
    "    \"\"\"\n",
    "    if pd.isna(run_ids_str):\n",
    "        return pd.Series([None, None], index=['Original Mutant #', 'Original Source'])\n",
    "\n",
    "    try:\n",
    "        run_ids = [int(float(run_id.strip())) for run_id in str(run_ids_str).split(',') if run_id.strip()]\n",
    "    except (ValueError, AttributeError):\n",
    "        return pd.Series([None, None], index=['Original Mutant #', 'Original Source'])\n",
    "\n",
    "    matches = exp_df[exp_df['Run'].isin(run_ids)]\n",
    "    if matches.empty:\n",
    "        return pd.Series([None, None], index=['Original Mutant #', 'Original Source'])\n",
    "\n",
    "    original_mutants = ','.join(matches['Mutant #'].dropna().astype(str).unique())\n",
    "    original_sources = ','.join(matches['Source'].dropna().astype(str).unique())\n",
    "    return pd.Series([original_mutants, original_sources], index=['Original Mutant #', 'Original Source'])\n",
    "\n",
    "\n",
    "updated_df_processed[['Original Mutant #', 'Original Source']] = updated_df_processed['Run'].apply(\n",
    "    lambda run_ids: get_original_info(run_ids, experiment_df)\n",
    ")\n",
    "print(\"Added 'Original Mutant #' and 'Original Source' columns.\")\n",
    "\n",
    "# --- Processing the 'Kaiser' Table ---\n",
    "print(\"Processing the 'Kaiser' table...\")\n",
    "# To prevent row duplication during the merge, we must ensure 'DK#' is unique in the Kaiser table.\n",
    "# We will group by 'DK#' and aggregate the information from other columns.\n",
    "\n",
    "kaiser_df_processed = kaiser_df[['DK#', 'genotype', 'phenotype', 'References']].copy()\n",
    "# Drop rows where DK# is null as they can't be used for merging\n",
    "kaiser_df_processed.dropna(subset=['DK#'], inplace=True)\n",
    "\n",
    "# Convert all relevant columns to string to prevent aggregation errors with mixed types\n",
    "for col in ['genotype', 'phenotype', 'References']:\n",
    "    kaiser_df_processed[col] = kaiser_df_processed[col].astype(str)\n",
    "\n",
    "# Group by 'DK#' and aggregate the other columns by joining unique, non-null values\n",
    "kaiser_df_processed = kaiser_df_processed.groupby('DK#').agg({\n",
    "    'genotype': lambda x: ', '.join(x.replace('nan', '').dropna().unique()),\n",
    "    'phenotype': lambda x: ', '.join(x.replace('nan', '').dropna().unique()),\n",
    "    'References': lambda x: ', '.join(x.replace('nan', '').dropna().unique())\n",
    "}).reset_index()\n",
    "print(\"Aggregated Kaiser table to ensure unique DK# entries, preventing duplicates in final output.\")\n",
    "\n",
    "# --- Merging the Tables ---\n",
    "print(\"Joining the Updated and Kaiser tables...\")\n",
    "merged_df = pd.merge(\n",
    "    updated_df_processed,\n",
    "    kaiser_df_processed,\n",
    "    left_on='Strain',\n",
    "    right_on='DK#',\n",
    "    how='left'\n",
    ")\n",
    "print(\"Join complete.\")\n",
    "\n",
    "\n",
    "# --- Adding Motility Column ---\n",
    "def determine_motility(row):\n",
    "    \"\"\"Determines motility label based on 'Jiangguo' and 'phenotype' columns.\"\"\"\n",
    "    jiangguo_label = None\n",
    "    phenotype_label = None\n",
    "\n",
    "    if pd.notna(row['Jiangguo']):\n",
    "        jg_val = str(row['Jiangguo']).strip()\n",
    "        if jg_val in ['WT', 'A-S+', 'A+S-', 'A-S-']:\n",
    "            jiangguo_label = jg_val\n",
    "\n",
    "    if pd.notna(row['phenotype']):\n",
    "        ph_val = str(row['phenotype'])\n",
    "        if 'A-S+' in ph_val:\n",
    "            phenotype_label = 'A-S+'\n",
    "        elif 'A+S-' in ph_val:\n",
    "            phenotype_label = 'A+S-'\n",
    "        elif 'A-S-' in ph_val:\n",
    "            phenotype_label = 'A-S-'\n",
    "        elif 'A-' in ph_val:\n",
    "            phenotype_label = 'A-S+'\n",
    "        elif 'S-' in ph_val:\n",
    "            phenotype_label = 'A+S-'\n",
    "\n",
    "    final_label = jiangguo_label\n",
    "    if phenotype_label:\n",
    "        if jiangguo_label and jiangguo_label != phenotype_label:\n",
    "            final_label = f\"{phenotype_label}\"\n",
    "        else:\n",
    "            final_label = phenotype_label\n",
    "    return final_label\n",
    "\n",
    "\n",
    "print(\"Adding 'motility' column...\")\n",
    "merged_df['motility'] = merged_df.apply(determine_motility, axis=1)\n",
    "\n",
    "# --- Final Column Selection and Ordering ---\n",
    "print(\"Reordering and selecting final columns...\")\n",
    "final_columns = [\n",
    "    'Strain', 'Run', 'Movies', 'Final Movies',  # Use the new 'Final Movies' column\n",
    "    'Original Mutant #', 'Original Source',\n",
    "    'genotype', 'phenotype', 'Reference', 'motility',\n",
    "    'Source_labelled', 'Source', 'References', 'Bib'\n",
    "]\n",
    "final_columns_exist = [col for col in final_columns if col in merged_df.columns]\n",
    "final_df = merged_df[final_columns_exist]\n",
    "\n",
    "# Filter out rows where 'Final Movies' is 0\n",
    "final_df = final_df[final_df['Final Movies'] != 0].copy()\n",
    "\n",
    "# Sort the final DataFrame by 'Strain'\n",
    "print(\"Sorting final data by 'Strain'...\")\n",
    "final_df.sort_values(by='Strain', inplace=True)\n",
    "\n",
    "# --- Saving the Result ---\n",
    "\n",
    "final_df.to_excel(OUTPUT_FULL_FILE_PATH, index=False)\n",
    "final_df[['Strain', 'Run', 'Final Movies', 'motility', 'Bib']].to_excel(OUTPUT_FILE_PATH, index=False)\n",
    "print(f\"Successfully created the final output file: {OUTPUT_FILE_PATH}\")\n",
    "\n"
   ],
   "id": "622a72a20f343df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "All files loaded successfully.\n",
      "Processing the 'Updated' table...\n",
      "Filtered down to 439 unique rows with 'Run' values.\n",
      "Cleaned 'Mutant #' column into 'Strain'.\n",
      "Calculating final movie counts from image directories...\n",
      "Added 'Final Movies' column.\n",
      "Looking up original data from the experiment file...\n",
      "Added 'Original Mutant #' and 'Original Source' columns.\n",
      "Processing the 'Kaiser' table...\n",
      "Aggregated Kaiser table to ensure unique DK# entries, preventing duplicates in final output.\n",
      "Joining the Updated and Kaiser tables...\n",
      "Join complete.\n",
      "Adding 'motility' column...\n",
      "Reordering and selecting final columns...\n",
      "Sorting final data by 'Strain'...\n",
      "Successfully created the final output file: /home/xavier/Documents/DAE_project/dataset/Roy_training/merged_strain_data.xlsx\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T00:40:31.747568Z",
     "start_time": "2025-09-29T00:40:31.728117Z"
    }
   },
   "cell_type": "code",
   "source": "final_df",
   "id": "448ab15eb9a2f355",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Strain                 Run  Movies  Final Movies Original Mutant #  \\\n",
       "137       ASX1                 306     3.0             3              ASX1   \n",
       "138      DK101  407, 620, 660, 767    12.0             9      DK101 ,DK101   \n",
       "139     DK1013                 341     3.0             2            DK1013   \n",
       "140     DK1016                 342     3.0             3            DK1016   \n",
       "141     DK1031                 338     3.0             2            DK1031   \n",
       "..         ...                 ...     ...           ...               ...   \n",
       "476   MXAN7164                 570     3.0             3          MXAN7164   \n",
       "479  Omega4531                 626     3.0             1         Omega4531   \n",
       "410     esgWen                 227     3.0             2            esgWen   \n",
       "477  omega4469                 685     3.0             3         omega4469   \n",
       "478  omega4473                 658     3.0             3         omega4473   \n",
       "\n",
       "                                       Original Source genotype  \\\n",
       "137  M Fontes, D Kaiser - … of the National Academy...      NaN   \n",
       "138                 J Hodgkin and D Kaiser, 1977, PNAS      NaN   \n",
       "139  LJ Shimkets - Journal of bacteriology, 1986 - ...      NaN   \n",
       "140  LJ Shimkets - Journal of bacteriology, 1986 - ...      NaN   \n",
       "141  LJ Shimkets - Journal of bacteriology, 1986 - ...      NaN   \n",
       "..                                                 ...      ...   \n",
       "476                                                         NaN   \n",
       "479  L Kroos, A Kuspa, D Kaiser - Developmental bio...      NaN   \n",
       "410                                                         NaN   \n",
       "477  L Kroos, A Kuspa, D Kaiser - Developmental bio...      NaN   \n",
       "478  L Kroos, A Kuspa, D Kaiser - Developmental bio...      NaN   \n",
       "\n",
       "                                             phenotype  \\\n",
       "137                                                NaN   \n",
       "138                                                NaN   \n",
       "139  Mx1R, Mx4R, Mx8S, Mx8Cp2R, Mx9R, non-fruiting ...   \n",
       "140  Mx1R, Mx4R, Mx8S, Mx8Cp2S, non-fruiting in col...   \n",
       "141  Mx1R, Mx4R, Mx8S, Mx8Cp2S, Mx9R, non-fruiting ...   \n",
       "..                                                 ...   \n",
       "476                                                NaN   \n",
       "479                                                NaN   \n",
       "410                                                NaN   \n",
       "477                                                NaN   \n",
       "478                                                NaN   \n",
       "\n",
       "                                             Reference motility  \\\n",
       "137                                                NaN     None   \n",
       "138  https://link.springer.com/article/10.1007/bf00...       WT   \n",
       "139  https://journals.asm.org/doi/abs/10.1128/jb.16...     None   \n",
       "140  https://journals.asm.org/doi/epdf/10.1128/jb.1...     None   \n",
       "141  https://journals.asm.org/doi/abs/10.1128/jb.16...     None   \n",
       "..                                                 ...      ...   \n",
       "476  https://www.proquest.com/docview/305383337?pq-...     None   \n",
       "479  https://www.sciencedirect.com/science/article/...     None   \n",
       "410                                                NaN     None   \n",
       "477  https://www.sciencedirect.com/science/article/...     None   \n",
       "478  https://www.sciencedirect.com/science/article/...     None   \n",
       "\n",
       "                               Source_labelled  \\\n",
       "137                                        NaN   \n",
       "138                      Killeen & Nelson 1988   \n",
       "139                                        NaN   \n",
       "140                                        NaN   \n",
       "141                                        NaN   \n",
       "..                                         ...   \n",
       "476                                        NaN   \n",
       "479                              Shimkets 1998   \n",
       "410                                        NaN   \n",
       "477                              Shimkets 1998   \n",
       "478  Kroos, Kuspa & Kaiser 1986, Shimkets 1998   \n",
       "\n",
       "                                                Source References  \\\n",
       "137  M Fontes, D Kaiser - … of the National Academy...        NaN   \n",
       "138                 J Hodgkin and D Kaiser, 1977, PNAS        NaN   \n",
       "139  LJ Shimkets - Journal of bacteriology, 1986 - ...        NaN   \n",
       "140  LJ Shimkets - Journal of bacteriology, 1986 - ...        NaN   \n",
       "141  LJ Shimkets - Journal of bacteriology, 1986 - ...        NaN   \n",
       "..                                                 ...        ...   \n",
       "476                                                NaN        NaN   \n",
       "479  L Kroos, A Kuspa, D Kaiser - Developmental bio...        NaN   \n",
       "410                                                NaN        NaN   \n",
       "477  L Kroos, A Kuspa, D Kaiser - Developmental bio...        NaN   \n",
       "478  L Kroos, A Kuspa, D Kaiser - Developmental bio...        NaN   \n",
       "\n",
       "                                   Bib  \n",
       "137                                NaN  \n",
       "138  hodgkin1979genetics, cheng1989dsg  \n",
       "139            shimkets1986correlation  \n",
       "140                   shimkets1986role  \n",
       "141            shimkets1986correlation  \n",
       "..                                 ...  \n",
       "476            caberoy2005coordinating  \n",
       "479                    kroos1986global  \n",
       "410                                NaN  \n",
       "477             kuspa1986intercellular  \n",
       "478             kuspa1986intercellular  \n",
       "\n",
       "[293 rows x 14 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strain</th>\n",
       "      <th>Run</th>\n",
       "      <th>Movies</th>\n",
       "      <th>Final Movies</th>\n",
       "      <th>Original Mutant #</th>\n",
       "      <th>Original Source</th>\n",
       "      <th>genotype</th>\n",
       "      <th>phenotype</th>\n",
       "      <th>Reference</th>\n",
       "      <th>motility</th>\n",
       "      <th>Source_labelled</th>\n",
       "      <th>Source</th>\n",
       "      <th>References</th>\n",
       "      <th>Bib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>ASX1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>ASX1</td>\n",
       "      <td>M Fontes, D Kaiser - … of the National Academy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M Fontes, D Kaiser - … of the National Academy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>DK101</td>\n",
       "      <td>407, 620, 660, 767</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9</td>\n",
       "      <td>DK101 ,DK101</td>\n",
       "      <td>J Hodgkin and D Kaiser, 1977, PNAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://link.springer.com/article/10.1007/bf00...</td>\n",
       "      <td>WT</td>\n",
       "      <td>Killeen &amp; Nelson 1988</td>\n",
       "      <td>J Hodgkin and D Kaiser, 1977, PNAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hodgkin1979genetics, cheng1989dsg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>DK1013</td>\n",
       "      <td>341</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>DK1013</td>\n",
       "      <td>LJ Shimkets - Journal of bacteriology, 1986 - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mx1R, Mx4R, Mx8S, Mx8Cp2R, Mx9R, non-fruiting ...</td>\n",
       "      <td>https://journals.asm.org/doi/abs/10.1128/jb.16...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LJ Shimkets - Journal of bacteriology, 1986 - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shimkets1986correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>DK1016</td>\n",
       "      <td>342</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>DK1016</td>\n",
       "      <td>LJ Shimkets - Journal of bacteriology, 1986 - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mx1R, Mx4R, Mx8S, Mx8Cp2S, non-fruiting in col...</td>\n",
       "      <td>https://journals.asm.org/doi/epdf/10.1128/jb.1...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LJ Shimkets - Journal of bacteriology, 1986 - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shimkets1986role</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>DK1031</td>\n",
       "      <td>338</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>DK1031</td>\n",
       "      <td>LJ Shimkets - Journal of bacteriology, 1986 - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mx1R, Mx4R, Mx8S, Mx8Cp2S, Mx9R, non-fruiting ...</td>\n",
       "      <td>https://journals.asm.org/doi/abs/10.1128/jb.16...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LJ Shimkets - Journal of bacteriology, 1986 - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shimkets1986correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>MXAN7164</td>\n",
       "      <td>570</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>MXAN7164</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.proquest.com/docview/305383337?pq-...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>caberoy2005coordinating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Omega4531</td>\n",
       "      <td>626</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Omega4531</td>\n",
       "      <td>L Kroos, A Kuspa, D Kaiser - Developmental bio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>None</td>\n",
       "      <td>Shimkets 1998</td>\n",
       "      <td>L Kroos, A Kuspa, D Kaiser - Developmental bio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kroos1986global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>esgWen</td>\n",
       "      <td>227</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>esgWen</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>omega4469</td>\n",
       "      <td>685</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>omega4469</td>\n",
       "      <td>L Kroos, A Kuspa, D Kaiser - Developmental bio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>None</td>\n",
       "      <td>Shimkets 1998</td>\n",
       "      <td>L Kroos, A Kuspa, D Kaiser - Developmental bio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kuspa1986intercellular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>omega4473</td>\n",
       "      <td>658</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>omega4473</td>\n",
       "      <td>L Kroos, A Kuspa, D Kaiser - Developmental bio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>None</td>\n",
       "      <td>Kroos, Kuspa &amp; Kaiser 1986, Shimkets 1998</td>\n",
       "      <td>L Kroos, A Kuspa, D Kaiser - Developmental bio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kuspa1986intercellular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 14 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot WT aggregating distribution",
   "id": "2cd414c1a243d281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "WT_LABEL_PATH = f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\"\n",
    "\n",
    "wt_df = pd.read_csv(WT_LABEL_PATH)\n",
    "# Group by run_id and count T/F in aggregates_formed\n",
    "counts = wt_df.groupby(\"run_id\")[\"aggregates_formed\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Create bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# F on top, T on bottom → stack bars with T first, then F\n",
    "counts.plot(kind=\"bar\", stacked=True, ax=ax, color={\"T\": \"tab:blue\", \"F\": \"tab:orange\"})\n",
    "\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Counts of Aggregates Formed (T and F) by run_id\")\n",
    "ax.legend(title=\"Aggregates Formed\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2cab1996a746a7fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Perform AUC and PCA analysis for WT aggregate formation and motility",
   "id": "314e693c8294feae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:37:41.788875905Z",
     "start_time": "2025-09-28T06:29:51.209324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Integrated Script: Trajectory Distance, Predictive Analysis, Dimensionality Reduction, and Frame Extraction\n",
    "# ==============================================================================\n",
    "# Purpose:\n",
    "# 1. Load and process trajectory data, imputing NaN values with the last valid frame.\n",
    "# 2. Print a summary of sample and run counts for each class.\n",
    "# 3. Calculate pairwise distances and train SVM classifiers to evaluate predictive power (AUC).\n",
    "#    Includes a fallback from StratifiedGroupKFold to StratifiedKFold if splits are invalid.\n",
    "# 4. Generate bar plots of AUC scores and SAVE THE UNDERLYING DATA to a CSV file.\n",
    "# 5. Perform dimensionality reduction (PCA/UMAP) on the feature space at specified time points,\n",
    "#    and SAVE THE REDUCED COORDINATES to a CSV file.\n",
    "# 6. Optionally, visualize the SVM decision boundary on the dimensionality reduction plots,\n",
    "#    now including a color bar to indicate class probability.\n",
    "# 7. For specified time points, find the corresponding raw images, perform a center crop,\n",
    "#    and save the processed images to an output folder, organized by time and class.\n",
    "#\n",
    "# REVISIONS IN THIS VERSION:\n",
    "# - Optimized the SVM boundary visualization in `plot_dimensionality_reduction` for significant speed improvement.\n",
    "#   - Increased the `meshgrid` step size to reduce the number of prediction points.\n",
    "#   - Removed the unnecessary cross-validation loop for plotting; a single SVM is now trained on the\n",
    "#     sampled 2D data for a much faster, yet still representative, visualization.\n",
    "# - Maintained all advanced features: probability-based shading, color consistency, and the color bar.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.stats import sem\n",
    "from itertools import combinations, product\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "import warnings\n",
    "import math\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CONFIG = {\n",
    "    # --- Analysis Setup ---\n",
    "    \"analysis_type\": \"WT\",  # \"WT\" or \"motility\"\n",
    "    \"random_seed\": 42,  # Seed for all random operations to ensure reproducibility\n",
    "\n",
    "    # --- Data and Model Paths ---\n",
    "    \"features_base_dir_wt\": f\"{WORKING_DIR}/encoded_features/WT_features\",\n",
    "    \"labeling_csv_path_wt\": f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\",\n",
    "    \"WT_img_dir\": f\"{WORKING_DIR}/dataset/WT/images\",\n",
    "\n",
    "    \"features_base_dir_motility\": f\"{WORKING_DIR}/encoded_features/Roy_training_features\",\n",
    "    \"motility_csv_path\": f'{WORKING_DIR}/dataset/Roy_training/merged_strain_data.xlsx',\n",
    "    \"motility_img_dir\": f\"{WORKING_DIR}/dataset/Roy_training/images\",\n",
    "\n",
    "    # --- Output Configuration ---\n",
    "    \"analysis_output_dir\": f\"{WORKING_DIR}/images/figure6/%s_analysis\",\n",
    "    \"output_figure_name\": \"distance_and_prediction_summary.pdf\",\n",
    "    \"roc_figure_name\": \"roc_curves_combined.pdf\",\n",
    "    \"auc_barplot_name\": \"auc_barplot.pdf\",\n",
    "    \"pca_plot_name\": \"pca_plot.pdf\",\n",
    "    \"umap_plot_name\": \"umap_plot.pdf\",\n",
    "\n",
    "    # --- Rerun and Visualization Settings ---\n",
    "    \"force_rerun\": True,\n",
    "    \"show_movie_distance_analysis\": True,\n",
    "    \"n_splits\": 3,\n",
    "\n",
    "    # --- Frame Copying Settings ---\n",
    "    \"copy_frames\": False,\n",
    "    \"copied_frames_dir\": \"copied_frames\",\n",
    "\n",
    "    # --- Analysis Parameters ---\n",
    "    \"selected_time_points\": [1440, 0],\n",
    "    \"dist_method\": \"euclidean\",\n",
    "    \"tolerance\": 90,\n",
    "    \"num_workers\": max(1, cpu_count() - 2),\n",
    "    \"required_frames_motility\": 1441,\n",
    "\n",
    "    # --- Dimensionality Reduction Settings ---\n",
    "    \"dimensionality_reduction\": {\n",
    "        \"run\": True,\n",
    "        \"method\": \"PCA\",  # \"PCA\" or \"UMAP\"\n",
    "        \"sample_equal\": True,\n",
    "        \"plot_in_one_figure\": False,\n",
    "        \"show_svm_boundary\": False,\n",
    "        \"plot_mean_features_at_times\": []\n",
    "    },\n",
    "\n",
    "    # --- Motility Analysis Specific ---\n",
    "    \"motility_target_classes\": ['WT', 'A+S-', 'A-S+', 'A-S-'],\n",
    "    \"motility_comparison_pairs\": [\n",
    "        ('WT', 'A+S-'),\n",
    "        ('WT', 'A-S+'),\n",
    "        ('WT', 'A-S-'),\n",
    "        ('A+S-', 'A-S+'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Set the global random seed from the config for numpy operations\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "\n",
    "CONFIG[\"analysis_output_dir\"] = CONFIG[\"analysis_output_dir\"] % CONFIG[\"analysis_type\"]\n",
    "# Add paths for cached files\n",
    "CONFIG[\"cached_distances_path\"] = os.path.join(CONFIG['analysis_output_dir'], \"cached_movie_distances.npz\")\n",
    "CONFIG[\"cached_dist_matrix_path\"] = os.path.join(CONFIG['analysis_output_dir'], \"cached_dist_matrix.npz\")\n",
    "\n",
    "\n",
    "# --- Image Processing Helper Functions ---\n",
    "\n",
    "def resize_crop(img_dir, resize_by=1., resolution=512, brightness_norm=True, brightness_mean=107):\n",
    "    \"\"\"\n",
    "    Loads an image, resizes it, and takes a center crop.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_dir, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    if img.dtype != np.uint8:\n",
    "        img = (img / 256).astype(np.uint8)\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    img_shape = img.shape\n",
    "    resize_shape = (int(img_shape[1] * resize_by), int(img_shape[0] * resize_by))\n",
    "\n",
    "    if resize_by != 1:\n",
    "        img = cv2.resize(img, resize_shape, cv2.INTER_LANCZOS4)\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    start_y = max(0, (h - resolution) // 2)\n",
    "    start_x = max(0, (w - resolution) // 2)\n",
    "    new_img = img[start_y:start_y + resolution, start_x:start_x + resolution]\n",
    "\n",
    "    if brightness_norm:\n",
    "        obj_v = np.mean(new_img)\n",
    "        value = brightness_mean - obj_v\n",
    "        value_array = np.full(new_img.shape, value, dtype=np.float64)\n",
    "        new_img = np.clip(new_img.astype(np.float64) + value_array, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return new_img\n",
    "\n",
    "\n",
    "def copy_and_process_frames(run_id, scope_id, class_name, time_points, base_img_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Finds, processes, and saves specific frames for a given sample.\n",
    "    Finds the closest available frame if the exact frame is not found.\n",
    "    \"\"\"\n",
    "    for t in time_points:\n",
    "        try:\n",
    "            frame_output_dir = os.path.join(output_dir, f\"{t}min\", class_name)\n",
    "            os.makedirs(frame_output_dir, exist_ok=True)\n",
    "\n",
    "            run_dir_pattern = os.path.join(base_img_dir, f\"*Run{run_id:04d}*\")\n",
    "            matching_run_dirs = glob.glob(run_dir_pattern)\n",
    "\n",
    "            scope_dir_path = \"\"\n",
    "            if matching_run_dirs:\n",
    "                run_dir = matching_run_dirs[0]\n",
    "                scope_dir_path = os.path.join(run_dir, f\"Scope{scope_id:02d}\")\n",
    "            else:\n",
    "                scope_dir_path = os.path.join(base_img_dir, f\"Run{run_id:04d}\", f\"Scope{scope_id:02d}\")\n",
    "\n",
    "            if not os.path.isdir(scope_dir_path):\n",
    "                print(\n",
    "                    f\"  Warning: Scope directory not found for Run {run_id}, Scope {scope_id}. Path: {scope_dir_path}\")\n",
    "                continue\n",
    "\n",
    "            all_images = glob.glob(os.path.join(scope_dir_path, \"*.jpg\"))\n",
    "            if not all_images:\n",
    "                print(f\"  Warning: No JPG images found for Run {run_id}, Scope {scope_id} in {scope_dir_path}\")\n",
    "                continue\n",
    "\n",
    "            target_frame_idx = t + 1\n",
    "            best_match_path = None\n",
    "            min_diff = float('inf')\n",
    "            frame_number_pattern = re.compile(r'_(\\d+)\\.jpg$')\n",
    "\n",
    "            for img_path in all_images:\n",
    "                match = frame_number_pattern.search(os.path.basename(img_path))\n",
    "                if match:\n",
    "                    frame_num = int(match.group(1))\n",
    "                    diff = abs(frame_num - target_frame_idx)\n",
    "                    if diff < min_diff:\n",
    "                        min_diff = diff\n",
    "                        best_match_path = img_path\n",
    "\n",
    "            if best_match_path:\n",
    "                source_path = best_match_path\n",
    "                found_frame_num_match = frame_number_pattern.search(os.path.basename(source_path))\n",
    "                if found_frame_num_match:\n",
    "                    found_frame_num = int(found_frame_num_match.group(1))\n",
    "                    if found_frame_num != target_frame_idx:\n",
    "                        print(\n",
    "                            f\"  Info: For Run {run_id}, Scope {scope_id}, Time {t} min, using closest frame {found_frame_num}.\")\n",
    "\n",
    "                processed_img = resize_crop(source_path)\n",
    "                if processed_img is not None:\n",
    "                    dest_filename = f\"{class_name}_Run{run_id:04d}_Scope{scope_id:02d}.jpg\"\n",
    "                    dest_path = os.path.join(frame_output_dir, dest_filename)\n",
    "                    cv2.imwrite(dest_path, processed_img)\n",
    "                else:\n",
    "                    print(f\"  Warning: Failed to process image: {source_path}\")\n",
    "            else:\n",
    "                print(f\"  Warning: Image not found for Run {run_id}, Scope {scope_id}, Time {t} min.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing frame for Run {run_id}, Scope {scope_id} at time {t}: {e}\")\n",
    "\n",
    "\n",
    "# --- Analysis Helper Functions ---\n",
    "\n",
    "def impute_nans_with_previous_frame(trajectory):\n",
    "    for i in range(1, trajectory.shape[0]):\n",
    "        if np.isnan(trajectory[i]).any():\n",
    "            trajectory[i] = trajectory[i - 1]\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def get_closest_frame_index(requested_frame, total_frames):\n",
    "    if total_frames == 0:\n",
    "        raise ValueError(\"Cannot get frame index from a trajectory with zero frames.\")\n",
    "    max_index = total_frames - 1\n",
    "    return min(requested_frame, max_index)\n",
    "\n",
    "\n",
    "def train_and_get_roc_data(features, labels, groups, use_group_kfold, analysis_name, precomputed_kernel=False):\n",
    "    n_splits = CONFIG['n_splits']\n",
    "    random_seed = CONFIG['random_seed']\n",
    "\n",
    "    if use_group_kfold:\n",
    "        cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "        split_iterator = cv.split(features, labels, groups)\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "        split_iterator = cv.split(features, labels)\n",
    "\n",
    "    tprs, aucs = [], []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    try:\n",
    "        for train_idx, test_idx in split_iterator:\n",
    "            if len(np.unique(labels[test_idx])) < 2:\n",
    "                print(f\"  Skipping invalid fold in CV for {analysis_name}: test set contains only one class.\")\n",
    "                continue\n",
    "\n",
    "            if precomputed_kernel:\n",
    "                model = SVC(kernel='precomputed', class_weight='balanced', probability=True, random_state=random_seed)\n",
    "                model.fit(features[np.ix_(train_idx, train_idx)], labels[train_idx])\n",
    "                probas_ = model.predict_proba(features[np.ix_(test_idx, train_idx)])\n",
    "            else:\n",
    "                pipeline = make_pipeline(StandardScaler(), SVC(kernel='rbf', class_weight='balanced', probability=True,\n",
    "                                                               random_state=random_seed))\n",
    "                pipeline.fit(features[train_idx], labels[train_idx])\n",
    "                probas_ = pipeline.predict_proba(features[test_idx])\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(labels[test_idx], probas_[:, 1])\n",
    "            tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            aucs.append(auc(fpr, tpr))\n",
    "    except Exception as e:\n",
    "        print(f\"  CV failed for {analysis_name} with error: {e}. Cannot generate ROC data.\")\n",
    "        return None\n",
    "\n",
    "    if not tprs:\n",
    "        print(f\"  Could not generate any valid CV folds for {analysis_name}. Cannot generate ROC data.\")\n",
    "        return None\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "\n",
    "    return mean_fpr, mean_tpr, std_tpr, mean_auc, std_auc\n",
    "\n",
    "\n",
    "# --- Plotting Functions ---\n",
    "\n",
    "def plot_auc_barplot(auc_scores, time_points, output_path, title_prefix=\"\", force_rerun=False):\n",
    "    if not force_rerun and os.path.exists(output_path):\n",
    "        print(f\"Skipping existing AUC bar plot: {os.path.basename(output_path)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Generating AUC bar plot: {os.path.basename(output_path)}\")\n",
    "    labels = [f'{t // 60} h' for t in time_points]\n",
    "    auc_means = [auc_scores.get(f'Time {t} min', (np.nan, np.nan))[0] for t in time_points]\n",
    "    auc_stds = [auc_scores.get(f'Time {t} min', (np.nan, np.nan))[1] for t in time_points]\n",
    "\n",
    "    y_err_lower, y_err_upper = [], []\n",
    "    for mean, std in zip(auc_means, auc_stds):\n",
    "        if np.isnan(mean) or np.isnan(std):\n",
    "            y_err_lower.append(0)\n",
    "            y_err_upper.append(0)\n",
    "            continue\n",
    "        margin = 1.96 * std\n",
    "        y_err_upper.append(min(mean + margin, 1.0) - mean)\n",
    "        y_err_lower.append(mean - max(mean - margin, 0.0))\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(x, auc_means, yerr=[y_err_lower, y_err_upper], capsize=5, color='skyblue', ecolor='gray',\n",
    "           label='Mean AUC (95% CI)')\n",
    "    ax.set_ylabel('Mean AUC Score')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylim(0.45, 1.05)\n",
    "    ax.axhline(y=0.5, color='r', linestyle='--', label='Random Chance (AUC=0.5)')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(output_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_dimensionality_reduction(features, labels, time_points, method='PCA', sample_equal=True,\n",
    "                                  plot_in_one_figure=True, output_path='dim_red.pdf', title_prefix=\"\",\n",
    "                                  legend_map=None, show_svm_boundary=False, force_rerun=False,\n",
    "                                  plot_mean_features_at_times=None):\n",
    "    \"\"\"\n",
    "    Samples data, performs PCA/UMAP, plots results, saves the data to a CSV,\n",
    "    and optionally shows a shaded SVM decision boundary with a color bar.\n",
    "    OPTIMIZED for performance.\n",
    "    \"\"\"\n",
    "    if not force_rerun and os.path.exists(output_path) and plot_in_one_figure:\n",
    "        print(f\"Skipping existing dimensionality reduction plot: {os.path.basename(output_path)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Generating {method} plot(s): {os.path.basename(output_path)}\")\n",
    "    print(f\"  Initial number of samples for plotting: {features.shape[0]}\")\n",
    "\n",
    "    random_seed = CONFIG['random_seed']\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "\n",
    "    if plot_in_one_figure:\n",
    "        fig, axes = plt.subplots(1, len(time_points), figsize=(5.5 * len(time_points), 5), sharex=False, sharey=False)\n",
    "        if len(time_points) == 1: axes = [axes]\n",
    "    else:\n",
    "        fig, axes = None, None\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) < 2:\n",
    "        print(\"Warning: Only one class present. Skipping dimensionality reduction plot.\")\n",
    "        return\n",
    "\n",
    "    total_frames = features.shape[1]\n",
    "    contour_object = None\n",
    "\n",
    "    color_class_0 = '#3b75af'\n",
    "    color_class_1 = '#d1495b'\n",
    "    custom_palette_dict = None\n",
    "    if legend_map:\n",
    "        sorted_keys = sorted(legend_map.keys())\n",
    "        if len(sorted_keys) == 2:\n",
    "            custom_palette_dict = {\n",
    "                legend_map[sorted_keys[0]]: color_class_0,\n",
    "                legend_map[sorted_keys[1]]: color_class_1\n",
    "            }\n",
    "\n",
    "    for i, t_req in enumerate(time_points):\n",
    "        t = get_closest_frame_index(t_req, total_frames)\n",
    "        individual_output_path = output_path.replace('.pdf', f'_{t_req}min.pdf')\n",
    "        if not plot_in_one_figure and not force_rerun and os.path.exists(individual_output_path):\n",
    "            print(f\"Skipping existing plot: {os.path.basename(individual_output_path)}\")\n",
    "            continue\n",
    "\n",
    "        features_at_t = features[:, t, :]\n",
    "\n",
    "        if sample_equal:\n",
    "            min_samples = min(np.sum(labels == unique_labels[0]), np.sum(labels == unique_labels[1]))\n",
    "            indices_0 = rng.choice(np.where(labels == unique_labels[0])[0], min_samples, replace=False)\n",
    "            indices_1 = rng.choice(np.where(labels == unique_labels[1])[0], min_samples, replace=False)\n",
    "            sampled_indices = np.concatenate([indices_0, indices_1])\n",
    "            features_for_dim_red = features_at_t[sampled_indices]\n",
    "            labels_for_dim_red = labels[sampled_indices]\n",
    "        else:\n",
    "            features_for_dim_red = features_at_t\n",
    "            labels_for_dim_red = labels\n",
    "\n",
    "        reducer = PCA(n_components=2, random_state=random_seed) if method == 'PCA' else UMAP(n_components=2,\n",
    "                                                                                             random_state=random_seed)\n",
    "\n",
    "        try:\n",
    "            transformed_features = reducer.fit_transform(features_for_dim_red)\n",
    "        except ValueError as e:\n",
    "            print(f\"  ERROR: Could not perform {method} at time {t_req} min. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        plot_labels = pd.Series(labels_for_dim_red).map(legend_map) if legend_map else labels_for_dim_red\n",
    "\n",
    "        base_name = os.path.splitext(os.path.basename(output_path))[0]\n",
    "        data_filename = f\"{base_name}_{t_req}min_data.csv\"\n",
    "        data_output_path = os.path.join(os.path.dirname(output_path), data_filename)\n",
    "        if not os.path.exists(data_output_path) or force_rerun:\n",
    "            print(f\"  Saving {method} data for time {t_req} min to {os.path.basename(data_output_path)}...\")\n",
    "            pd.DataFrame({\n",
    "                f'{method} Component 1': transformed_features[:, 0],\n",
    "                f'{method} Component 2': transformed_features[:, 1],\n",
    "                'Group': plot_labels\n",
    "            }).to_csv(data_output_path, index=False, float_format='%.4f')\n",
    "        else:\n",
    "            print(f\"  Skipping existing {method} data file: {os.path.basename(data_output_path)}\")\n",
    "\n",
    "        ax = axes[i] if plot_in_one_figure else plt.subplots(figsize=(7, 6))[1]\n",
    "        if not plot_in_one_figure: fig_single = ax.get_figure()\n",
    "\n",
    "        if show_svm_boundary:\n",
    "            print(f\"  Visualizing SVM boundaries for time {t_req} min...\")\n",
    "            x_min, x_max = transformed_features[:, 0].min() - 1, transformed_features[:, 0].max() + 1\n",
    "            y_min, y_max = transformed_features[:, 1].min() - 1, transformed_features[:, 1].max() + 1\n",
    "\n",
    "            # OPTIMIZATION: Increased meshgrid step size from 0.02 to 0.1 for a >20x speedup.\n",
    "            # This creates a coarser grid for visualization without significant loss of quality.\n",
    "            step_size = 0.1\n",
    "            xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n",
    "\n",
    "            # OPTIMIZATION: Removed CV loop for visualization. A single SVM trained on the\n",
    "            # sampled 2D data is sufficient and much faster for plotting a representative boundary.\n",
    "            svm_2d = SVC(kernel='rbf', gamma='auto', probability=True, random_state=random_seed)\n",
    "            svm_2d.fit(transformed_features, labels_for_dim_red)\n",
    "            Z = svm_2d.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "            Z = Z.reshape(xx.shape)\n",
    "\n",
    "            contour = ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.5, levels=np.linspace(0, 1, 21))\n",
    "            contour_object = contour\n",
    "\n",
    "            if not plot_in_one_figure:\n",
    "                cbar = fig_single.colorbar(contour, ax=ax)\n",
    "                cbar.set_label('Class Probability', rotation=270, labelpad=15)\n",
    "                cbar.set_ticks([0, 0.5, 1])\n",
    "                if legend_map:\n",
    "                    class0_label, class1_label = legend_map.get(0, 'Class 0'), legend_map.get(1, 'Class 1')\n",
    "                    cbar.ax.set_yticklabels([class0_label, 'Boundary', class1_label], fontsize=8, rotation=90,\n",
    "                                            va='center')\n",
    "\n",
    "        sns.scatterplot(x=transformed_features[:, 0], y=transformed_features[:, 1], hue=plot_labels,\n",
    "                        palette=custom_palette_dict if custom_palette_dict else 'Set2',\n",
    "                        ax=ax, alpha=0.8, edgecolor='k')\n",
    "        ax.set_title(f'Time {t_req // 60} h')\n",
    "        ax.set_xlabel(f'{method} 1')\n",
    "        ax.set_ylabel(f'{method} 2')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        if plot_mean_features_at_times and t_req in plot_mean_features_at_times:\n",
    "            print(f\"  Calculating and plotting mean features for time {t_req} min...\")\n",
    "            mean_feature_c0 = np.mean(features_at_t[labels == unique_labels[0]], axis=0)\n",
    "            mean_feature_c1 = np.mean(features_at_t[labels == unique_labels[1]], axis=0)\n",
    "\n",
    "            transformed_mean_c0 = reducer.transform(mean_feature_c0.reshape(1, -1))\n",
    "            transformed_mean_c1 = reducer.transform(mean_feature_c1.reshape(1, -1))\n",
    "            mean_of_means = (transformed_mean_c0 + transformed_mean_c1) / 2.0\n",
    "            ax.scatter(transformed_mean_c0[:, 0], transformed_mean_c0[:, 1], marker='*', s=300, c=color_class_0,\n",
    "                       edgecolor='white', zorder=10)\n",
    "            ax.scatter(transformed_mean_c1[:, 0], transformed_mean_c1[:, 1], marker='*', s=300, c=color_class_1,\n",
    "                       edgecolor='white', zorder=10)\n",
    "            ax.scatter(mean_of_means[:, 0], mean_of_means[:, 1], marker='*', s=300, c='yellow', edgecolor='black',\n",
    "                       zorder=10)\n",
    "\n",
    "        if not plot_in_one_figure:\n",
    "            plt.tight_layout()\n",
    "            fig_single.savefig(individual_output_path, dpi=300)\n",
    "            plt.close(fig_single)\n",
    "\n",
    "    if plot_in_one_figure:\n",
    "        fig.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "        if contour_object:\n",
    "            cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "            cbar = fig.colorbar(contour_object, cax=cbar_ax)\n",
    "            cbar.set_label('Class Probability', rotation=270, labelpad=15)\n",
    "            cbar.set_ticks([0, 0.5, 1])\n",
    "            if legend_map:\n",
    "                class0_label, class1_label = legend_map.get(0, 'Class 0'), legend_map.get(1, 'Class 1')\n",
    "                cbar.ax.set_yticklabels([class0_label, 'Boundary', class1_label], fontsize=8, rotation=90, va='center')\n",
    "        fig.savefig(output_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# --- Main Analysis Functions ---\n",
    "\n",
    "def run_wt_analysis():\n",
    "    print(\"\\n[STAGE 1/4] Loading and preparing WT data...\")\n",
    "    try:\n",
    "        labels_df = pd.read_csv(CONFIG['labeling_csv_path_wt'])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Labeling sheet not found at '{CONFIG['labeling_csv_path_wt']}'.\")\n",
    "        return\n",
    "\n",
    "    labels_df = labels_df.dropna(subset=['aggregates_formed'])\n",
    "    labels_df = labels_df[labels_df['aggregates_formed'].isin(['T', 'F'])]\n",
    "    labels_df['label'] = labels_df['aggregates_formed'].map({'T': 1, 'F': 0})\n",
    "\n",
    "    print(\"\\n--- Data Summary ---\")\n",
    "    for class_label, count in labels_df['aggregates_formed'].value_counts().items():\n",
    "        class_name = 'Aggregate' if class_label == 'T' else 'No Aggregate'\n",
    "        num_runs = labels_df[labels_df['aggregates_formed'] == class_label]['run_id'].nunique()\n",
    "        print(f\"  Class '{class_name}': {count} samples from {num_runs} unique runs.\")\n",
    "    print(\"--------------------\\n\")\n",
    "\n",
    "    all_features = {}\n",
    "    if CONFIG['copy_frames']:\n",
    "        print(\"[STAGE 1.5/4] Copying and processing selected frames for WT analysis...\")\n",
    "        copied_frames_output_dir = os.path.join(CONFIG['analysis_output_dir'], CONFIG['copied_frames_dir'], 'WT')\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        run_id, scope_id = row['run_id'], row['scope_id']\n",
    "        key = f\"R{run_id:04d}_S{scope_id:02d}\"\n",
    "        npz_path = os.path.join(CONFIG['features_base_dir_wt'], f\"Run{run_id:04d}\", f\"Scope{scope_id:02d}\",\n",
    "                                \"features.npz\")\n",
    "        if not os.path.exists(npz_path):\n",
    "            print(f\"Warning: Feature file not found for {key}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        features_data = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "        all_features[key] = {'label': row['label'], 'run_id': row['run_id'], 'features': features_data}\n",
    "\n",
    "        if CONFIG['copy_frames']:\n",
    "            class_name = 'Aggregate' if row['label'] == 1 else 'No_Aggregate'\n",
    "            copy_and_process_frames(run_id, scope_id, class_name, CONFIG['selected_time_points'], CONFIG['WT_img_dir'],\n",
    "                                    copied_frames_output_dir)\n",
    "\n",
    "    if not all_features: print(\"ERROR: No valid data could be loaded.\"); return\n",
    "\n",
    "    min_frames = min(v['features'].shape[0] for v in all_features.values())\n",
    "    all_trajectories = np.array([v['features'][:min_frames] for v in all_features.values()])\n",
    "    all_labels = np.array([v['label'] for v in all_features.values()])\n",
    "    all_groups = np.array([v['run_id'] for v in all_features.values()])\n",
    "\n",
    "    if len(np.unique(all_labels)) < 2: print(\"Warning: Insufficient data for one or both classes.\"); return\n",
    "\n",
    "    print(\"\\n[STAGE 2/4] Analyzing predictive power of features...\")\n",
    "    auc_scores = {}\n",
    "    n_splits = CONFIG['n_splits']\n",
    "    use_group_kfold = len(np.unique(all_groups[all_labels == 0])) >= n_splits and len(\n",
    "        np.unique(all_groups[all_labels == 1])) >= n_splits\n",
    "\n",
    "    for t in CONFIG['selected_time_points']:\n",
    "        features_at_t = all_trajectories[:, get_closest_frame_index(t, min_frames), :]\n",
    "        analysis_name = f\"Time {t} min\"\n",
    "        roc_data = train_and_get_roc_data(features_at_t, all_labels, all_groups, use_group_kfold, analysis_name)\n",
    "        if roc_data is None and use_group_kfold:\n",
    "            print(f\"  Warning: StratifiedGroupKFold failed for {analysis_name}. Retrying with StratifiedKFold.\")\n",
    "            roc_data = train_and_get_roc_data(features_at_t, all_labels, all_groups, False, analysis_name)\n",
    "        if roc_data: auc_scores[analysis_name] = (roc_data[3], roc_data[4])\n",
    "\n",
    "    print(\"\\n[STAGE 3/4] Saving result data...\")\n",
    "    if auc_scores:\n",
    "        auc_data_path = os.path.join(CONFIG['analysis_output_dir'], \"WT_auc_scores.csv\")\n",
    "        if not os.path.exists(auc_data_path) or CONFIG['force_rerun']:\n",
    "            auc_data = [\n",
    "                {'Time (min)': int(re.search(r'(\\d+)', name).group(1)), 'Mean AUC': mean_auc, 'Std Dev AUC': std_auc}\n",
    "                for name, (mean_auc, std_auc) in auc_scores.items()]\n",
    "            pd.DataFrame(auc_data).sort_values('Time (min)').to_csv(auc_data_path, index=False, float_format='%.4f')\n",
    "            print(f\"  Saved WT AUC scores to: {os.path.basename(auc_data_path)}\")\n",
    "        else:\n",
    "            print(f\"  Skipping existing WT AUC scores file: {os.path.basename(auc_data_path)}\")\n",
    "\n",
    "    print(\"\\n[STAGE 4/4] Generating analysis plots...\")\n",
    "    plot_auc_barplot(auc_scores, CONFIG['selected_time_points'],\n",
    "                     os.path.join(CONFIG['analysis_output_dir'], f\"WT_{CONFIG['auc_barplot_name']}\"), \"WT Analysis: \",\n",
    "                     CONFIG['force_rerun'])\n",
    "\n",
    "    if CONFIG['dimensionality_reduction']['run']:\n",
    "        dr_config = CONFIG['dimensionality_reduction']\n",
    "        output_name = f\"WT_{dr_config['method']}_plot.pdf\"\n",
    "        plot_dimensionality_reduction(all_trajectories, all_labels, time_points=CONFIG['selected_time_points'],\n",
    "                                      method=dr_config['method'], sample_equal=dr_config['sample_equal'],\n",
    "                                      plot_in_one_figure=dr_config['plot_in_one_figure'],\n",
    "                                      output_path=os.path.join(CONFIG['analysis_output_dir'], output_name),\n",
    "                                      legend_map={0: 'No Aggregate', 1: 'Aggregate'},\n",
    "                                      show_svm_boundary=dr_config['show_svm_boundary'],\n",
    "                                      force_rerun=CONFIG['force_rerun'],\n",
    "                                      plot_mean_features_at_times=dr_config.get('plot_mean_features_at_times'))\n",
    "\n",
    "\n",
    "def run_motility_analysis():\n",
    "    print(\"\\n[STAGE 1/4] Loading and preparing motility data...\")\n",
    "    try:\n",
    "        labels_df = pd.read_excel(CONFIG['motility_csv_path'])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Could not load motility data xlsx. Check path: {CONFIG['motility_csv_path']}\")\n",
    "        return\n",
    "\n",
    "    labels_df = labels_df.dropna(subset=['motility', 'Strain'])\n",
    "    labels_df = labels_df[labels_df['motility'].isin(CONFIG['motility_target_classes'])]\n",
    "    strain_to_label_map = pd.Series(labels_df.motility.values, index=labels_df.Strain).to_dict()\n",
    "\n",
    "    all_samples_dict = {class_name: [] for class_name in CONFIG['motility_target_classes']}\n",
    "    dir_pattern = re.compile(r'Run(\\d+)_Mutant(\\d+)')\n",
    "    scope_pattern = re.compile(r'Scope(\\d+)')\n",
    "\n",
    "    for dir_name in os.listdir(CONFIG['features_base_dir_motility']):\n",
    "        match = dir_pattern.match(dir_name)\n",
    "        if not match: continue\n",
    "        run_id, mutant_num = int(match.group(1)), int(match.group(2))\n",
    "        strain_id = f\"DK{mutant_num}\"\n",
    "        if strain_id in strain_to_label_map:\n",
    "            label = strain_to_label_map[strain_id]\n",
    "            for scope_dir_name in os.listdir(os.path.join(CONFIG['features_base_dir_motility'], dir_name)):\n",
    "                scope_match = scope_pattern.match(scope_dir_name)\n",
    "                if scope_match:\n",
    "                    scope_id = int(scope_match.group(1))\n",
    "                    npz_path = os.path.join(CONFIG['features_base_dir_motility'], dir_name, scope_dir_name,\n",
    "                                            \"features.npz\")\n",
    "                    if os.path.exists(npz_path):\n",
    "                        features = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "                        all_samples_dict[label].append({'features': features, 'run_id': run_id, 'scope_id': scope_id})\n",
    "\n",
    "    print(\"\\n--- Data Summary ---\")\n",
    "    for class_name, samples in all_samples_dict.items():\n",
    "        print(\n",
    "            f\"  Class '{class_name}': {len(samples)} samples from {len(np.unique([s['run_id'] for s in samples]))} unique runs.\")\n",
    "    print(\"--------------------\\n\")\n",
    "\n",
    "    if CONFIG['copy_frames']:\n",
    "        print(\"[STAGE 1.5/4] Copying and processing selected frames for motility analysis...\")\n",
    "        copied_frames_output_dir = os.path.join(CONFIG['analysis_output_dir'], CONFIG['copied_frames_dir'], 'motility')\n",
    "        for class_name, samples in all_samples_dict.items():\n",
    "            for sample in samples:\n",
    "                copy_and_process_frames(sample['run_id'], sample['scope_id'], class_name,\n",
    "                                        CONFIG['selected_time_points'], CONFIG['motility_img_dir'],\n",
    "                                        copied_frames_output_dir)\n",
    "\n",
    "    all_trajectories_dict = {k: [s['features'] for s in v] for k, v in all_samples_dict.items()}\n",
    "    all_runs_dict = {k: [s['run_id'] for s in v] for k, v in all_samples_dict.items()}\n",
    "    required_frames = CONFIG['required_frames_motility']\n",
    "    for class_name, trajs in all_trajectories_dict.items():\n",
    "        processed = [\n",
    "            np.vstack([t, np.repeat(t[-1:], required_frames - len(t), axis=0)]) if len(t) < required_frames else t[\n",
    "                :required_frames] for t in trajs]\n",
    "        all_trajectories_dict[class_name] = np.array(processed) if processed else np.array([])\n",
    "\n",
    "    for class1_name, class2_name in CONFIG['motility_comparison_pairs']:\n",
    "        print(f\"\\n--- Comparing '{class1_name}' vs. '{class2_name}' ---\")\n",
    "        trajs1, trajs2 = all_trajectories_dict.get(class1_name), all_trajectories_dict.get(class2_name)\n",
    "        if trajs1 is None or len(trajs1) == 0 or trajs2 is None or len(trajs2) == 0:\n",
    "            print(f\"  Warning: Insufficient data. Skipping pair.\");\n",
    "            continue\n",
    "\n",
    "        runs1, runs2 = all_runs_dict.get(class1_name), all_runs_dict.get(class2_name)\n",
    "        pair_trajs = np.concatenate([trajs1, trajs2])\n",
    "        pair_labels = np.array([0] * len(trajs1) + [1] * len(trajs2))\n",
    "        pair_groups = np.concatenate([runs1, runs2])\n",
    "\n",
    "        print(\"  [Step 1/3] Analyzing predictive power...\")\n",
    "        auc_scores = {}\n",
    "        n_splits = CONFIG['n_splits']\n",
    "        use_group_kfold = len(np.unique(runs1)) >= n_splits and len(np.unique(runs2)) >= n_splits\n",
    "\n",
    "        for t in CONFIG['selected_time_points']:\n",
    "            features_at_t = pair_trajs[:, get_closest_frame_index(t, required_frames), :]\n",
    "            analysis_name = f\"Time {t} min\"\n",
    "            roc_data = train_and_get_roc_data(features_at_t, pair_labels, pair_groups, use_group_kfold, analysis_name)\n",
    "            if roc_data is None and use_group_kfold:\n",
    "                print(f\"    Warning: StratifiedGroupKFold failed. Retrying with StratifiedKFold.\")\n",
    "                roc_data = train_and_get_roc_data(features_at_t, pair_labels, pair_groups, False, analysis_name)\n",
    "            if roc_data: auc_scores[analysis_name] = (roc_data[3], roc_data[4])\n",
    "\n",
    "        print(\"  [Step 2/3] Saving result data...\")\n",
    "        if auc_scores:\n",
    "            auc_data_path = os.path.join(CONFIG['analysis_output_dir'],\n",
    "                                         f\"motility_{class1_name}_vs_{class2_name}_auc_scores.csv\")\n",
    "            if not os.path.exists(auc_data_path) or CONFIG['force_rerun']:\n",
    "                auc_data = [{'Time (min)': int(re.search(r'(\\d+)', name).group(1)), 'Mean AUC': mean_auc,\n",
    "                             'Std Dev AUC': std_auc} for name, (mean_auc, std_auc) in auc_scores.items()]\n",
    "                pd.DataFrame(auc_data).sort_values('Time (min)').to_csv(auc_data_path, index=False, float_format='%.4f')\n",
    "                print(f\"    Saved AUC scores to: {os.path.basename(auc_data_path)}\")\n",
    "            else:\n",
    "                print(f\"    Skipping existing AUC scores file: {os.path.basename(auc_data_path)}\")\n",
    "\n",
    "        print(\"  [Step 3/3] Generating analysis plots...\")\n",
    "        title_prefix = f\"{class1_name}_vs_{class2_name}: \"\n",
    "        plot_auc_barplot(auc_scores, CONFIG['selected_time_points'], os.path.join(CONFIG['analysis_output_dir'],\n",
    "                                                                                  f\"motility_{class1_name}_vs_{class2_name}_{CONFIG['auc_barplot_name']}\"),\n",
    "                         title_prefix, CONFIG['force_rerun'])\n",
    "\n",
    "        if CONFIG['dimensionality_reduction']['run']:\n",
    "            dr_config = CONFIG['dimensionality_reduction']\n",
    "            output_name = f\"motility_{class1_name}_vs_{class2_name}_{dr_config['method']}_plot.pdf\"\n",
    "            plot_dimensionality_reduction(pair_trajs, pair_labels, time_points=CONFIG['selected_time_points'],\n",
    "                                          method=dr_config['method'], sample_equal=dr_config['sample_equal'],\n",
    "                                          plot_in_one_figure=dr_config['plot_in_one_figure'],\n",
    "                                          output_path=os.path.join(CONFIG['analysis_output_dir'], output_name),\n",
    "                                          legend_map={0: class1_name, 1: class2_name},\n",
    "                                          show_svm_boundary=dr_config['show_svm_boundary'],\n",
    "                                          force_rerun=CONFIG['force_rerun'],\n",
    "                                          plot_mean_features_at_times=dr_config.get('plot_mean_features_at_times'))\n",
    "\n",
    "\n",
    "def main():\n",
    "    output_dir = CONFIG['analysis_output_dir']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    analysis_type = CONFIG['analysis_type']\n",
    "    print(f\"--- Starting Analysis Script for '{analysis_type}' ---\")\n",
    "\n",
    "    if analysis_type == 'WT':\n",
    "        run_wt_analysis()\n",
    "    elif analysis_type == 'motility':\n",
    "        run_motility_analysis()\n",
    "    else:\n",
    "        print(f\"ERROR: Unknown analysis type '{analysis_type}'. Choose \\'WT\\' or \\'motility\\'.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Script execution complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Before running, ensure the WORKING_DIR at the top of the script is set correctly.\n",
    "    if WORKING_DIR == \".\":\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"!!! WARNING: `WORKING_DIR` is not set. Please update it to  !!!\")\n",
    "        print(\"!!! your project's base directory before running this script. !!!\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    main()\n"
   ],
   "id": "99572da4faf1a542",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Analysis Script for 'WT' ---\n",
      "\n",
      "[STAGE 1/4] Loading and preparing WT data...\n",
      "\n",
      "--- Data Summary ---\n",
      "  Class 'Aggregate': 220 samples from 34 unique runs.\n",
      "  Class 'No Aggregate': 132 samples from 38 unique runs.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[STAGE 2/4] Analyzing predictive power of features...\n",
      "\n",
      "[STAGE 3/4] Saving result data...\n",
      "  Saved WT AUC scores to: WT_auc_scores.csv\n",
      "\n",
      "[STAGE 4/4] Generating analysis plots...\n",
      "Generating AUC bar plot: WT_auc_barplot.pdf\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combine bar plot",
   "id": "13dd9a317ae55e41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T23:40:27.596011Z",
     "start_time": "2025-09-27T23:40:27.223190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration\n",
    "# ==============================================================================\n",
    "# --- File Paths ---\n",
    "# Assumes the CSV files are in the same directory as the script.\n",
    "# If not, provide the full path to the files.\n",
    "motility_csv_path = f'{WORKING_DIR}/images/figure6/motility_analysis/motility_A+S-_vs_A-S+_auc_scores.csv'\n",
    "wt_csv_path = f'{WORKING_DIR}/images/figure6/WT_analysis/WT_auc_scores.csv'\n",
    "output_plot_path = f'{WORKING_DIR}/images/figure6/motility_analysis/combined_auc_barplot.pdf'\n",
    "\n",
    "# --- Plotting Parameters ---\n",
    "time_points_to_plot = [1440, 0]  # The order of time points for each group\n",
    "bar_colors = ['#812db3', '#812db3', '#51ab4f', '#51ab4f']\n",
    "figure_size = (12, 6)\n",
    "y_axis_limit = [0.45, 1.05]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Script\n",
    "# ==============================================================================\n",
    "\n",
    "def create_combined_auc_plot():\n",
    "    \"\"\"\n",
    "    Loads AUC score data from two CSV files and plots them on a single\n",
    "    bar chart in a specified order.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Combined AUC Plot Generation ---\")\n",
    "\n",
    "    # --- 1. Load and Validate Data ---\n",
    "    if not os.path.exists(motility_csv_path):\n",
    "        print(f\"ERROR: Motility data file not found at '{motility_csv_path}'\")\n",
    "        return\n",
    "    if not os.path.exists(wt_csv_path):\n",
    "        print(f\"ERROR: WT data file not found at '{wt_csv_path}'\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading motility data from: {motility_csv_path}\")\n",
    "    motility_df = pd.read_csv(motility_csv_path)\n",
    "\n",
    "    print(f\"Loading WT data from: {wt_csv_path}\")\n",
    "    wt_df = pd.read_csv(wt_csv_path)\n",
    "\n",
    "    # --- 2. Prepare Data for Plotting ---\n",
    "    plot_data = {\n",
    "        'labels': [],\n",
    "        'means': [],\n",
    "        'stds': []\n",
    "    }\n",
    "\n",
    "    # Extract Motility data in the specified order\n",
    "    for time in time_points_to_plot:\n",
    "        row = motility_df[motility_df['Time (min)'] == time]\n",
    "        if not row.empty:\n",
    "            plot_data['labels'].append(f'A+S- vs A-S+\\n{time // 60} h')\n",
    "            plot_data['means'].append(row['Mean AUC'].iloc[0])\n",
    "            plot_data['stds'].append(row['Std Dev AUC'].iloc[0])\n",
    "        else:\n",
    "            print(f\"Warning: Time point {time} min not found in motility data.\")\n",
    "\n",
    "    # Extract WT data in the specified order\n",
    "    for time in time_points_to_plot:\n",
    "        row = wt_df[wt_df['Time (min)'] == time]\n",
    "        if not row.empty:\n",
    "            plot_data['labels'].append(f'WT Agg. vs No Agg.\\n{time // 60} h')\n",
    "            plot_data['means'].append(row['Mean AUC'].iloc[0])\n",
    "            plot_data['stds'].append(row['Std Dev AUC'].iloc[0])\n",
    "        else:\n",
    "            print(f\"Warning: Time point {time} min not found in WT data.\")\n",
    "\n",
    "    if not plot_data['means']:\n",
    "        print(\"ERROR: No data was extracted for plotting. Please check CSV files and time points.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Plotting data for labels: {plot_data['labels']}\")\n",
    "\n",
    "    # --- 3. Calculate Asymmetric 95% Confidence Intervals ---\n",
    "    y_err_lower = []\n",
    "    y_err_upper = []\n",
    "    for mean, std in zip(plot_data['means'], plot_data['stds']):\n",
    "        margin = 1.96 * std  # 95% CI margin\n",
    "        upper_error = min(mean + margin, 1.0) - mean\n",
    "        lower_error = mean - max(mean - margin, 0.0)\n",
    "        y_err_upper.append(upper_error)\n",
    "        y_err_lower.append(lower_error)\n",
    "\n",
    "    asymmetric_error = [y_err_lower, y_err_upper]\n",
    "\n",
    "    # --- 4. Generate the Plot ---\n",
    "    print(\"Generating the bar plot...\")\n",
    "    fig, ax = plt.subplots(figsize=figure_size)\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    x_pos = np.arange(len(plot_data['labels']))\n",
    "\n",
    "    ax.bar(x_pos, plot_data['means'], yerr=asymmetric_error,\n",
    "           color=bar_colors, capsize=5, ecolor='gray', zorder=2)\n",
    "\n",
    "    ax.set_ylabel('Mean AUC Score')\n",
    "    # ax.set_title('Comparison of Predictive Power')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(plot_data['labels'])\n",
    "    ax.set_ylim(y_axis_limit)\n",
    "\n",
    "    # Add a line for random chance\n",
    "    ax.axhline(y=0.5, color='r', linestyle='--', label='Random Chance (AUC=0.5)')\n",
    "\n",
    "    # Create custom legend handles\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=bar_colors[0], edgecolor=bar_colors[0], label='A+S- vs A-S+'),\n",
    "        Patch(facecolor=bar_colors[len(time_points_to_plot)], edgecolor=bar_colors[len(time_points_to_plot)],\n",
    "              label='WT Agg. vs No Agg.')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- 5. Save the Plot ---\n",
    "    try:\n",
    "        fig.savefig(output_plot_path, dpi=300)\n",
    "        print(f\"Successfully saved plot to: {output_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save the plot. Reason: {e}\")\n",
    "\n",
    "    plt.close(fig)\n",
    "    print(\"--- Script finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_combined_auc_plot()\n"
   ],
   "id": "ba44997982966f05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Combined AUC Plot Generation ---\n",
      "Loading motility data from: D:/Projects/DAE_project/images/figure6/motility_analysis/motility_A+S-_vs_A-S+_auc_scores.csv\n",
      "Loading WT data from: D:/Projects/DAE_project/images/figure6/WT_analysis/WT_auc_scores.csv\n",
      "Plotting data for labels: ['A+S- vs A-S+\\n24 h', 'A+S- vs A-S+\\n0 h', 'WT Agg. vs No Agg.\\n24 h', 'WT Agg. vs No Agg.\\n0 h']\n",
      "Generating the bar plot...\n",
      "Successfully saved plot to: D:/Projects/DAE_project/images/figure6/motility_analysis/combined_auc_barplot.pdf\n",
      "--- Script finished ---\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:47:43.750822Z",
     "start_time": "2025-09-28T06:47:40.555034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Standalone Script for Image Reconstruction from Mean Features\n",
    "# ==============================================================================\n",
    "# Purpose:\n",
    "# This script loads encoded features for specified classes, calculates the\n",
    "# mean feature vector for each class and their midpoint, and then uses a\n",
    "# pre-trained generator network (e.g., StyleGAN) to synthesize representative\n",
    "# images from these vectors.\n",
    "#\n",
    "# It is designed to be independent of the main analysis pipeline.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "CONFIG = {\n",
    "    # --- Analysis Target ---\n",
    "    \"analysis_type\": \"WT\",  # \"WT\" or \"motility\"\n",
    "    \"time_point_to_reconstruct\": 0,  # Time in minutes (e.g., 0, 720, 1440)\n",
    "\n",
    "    # --- Paths ---\n",
    "    \"network_pkl_path\": f\"{WORKING_DIR}/models/network-snapshot-001512-patched.pkl\",\n",
    "    \"output_dir\": f\"{WORKING_DIR}/images/figure6/WT_analysis/centers\",\n",
    "\n",
    "    # WT analysis paths\n",
    "    \"features_base_dir_wt\": f\"{WORKING_DIR}/encoded_features/WT_features\",\n",
    "    \"labeling_csv_path_wt\": f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\",\n",
    "\n",
    "    # Motility analysis paths\n",
    "    \"features_base_dir_motility\": f\"{WORKING_DIR}/encoded_features/Roy_training_features\",\n",
    "    \"motility_csv_path\": f'{WORKING_DIR}/dataset/Roy_training/merged_strain_data.xlsx',\n",
    "\n",
    "    # --- Class Selection ---\n",
    "    # For \"WT\" analysis_type\n",
    "    \"wt_classes_to_compare\": ('F', 'T'),  # ('F' = No Aggregate, 'T' = Aggregate)\n",
    "\n",
    "    # For \"motility\" analysis_type\n",
    "    \"motility_classes_to_compare\": ('WT', 'A-S-'),  # e.g., ('WT', 'A+S-'), ('A-S+', 'A-S-')\n",
    "}\n",
    "\n",
    "os.environ['CC'] = \"/usr/bin/gcc-9\"\n",
    "os.environ['CXX'] = \"/usr/bin/g++-9\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ---                          HELPER FUNCTIONS                            ---\n",
    "# ==============================================================================\n",
    "\n",
    "def impute_nans_with_previous_frame(trajectory):\n",
    "    \"\"\"Fills NaN values in a trajectory with the values from the last valid frame.\"\"\"\n",
    "    for i in range(1, trajectory.shape[0]):\n",
    "        if np.isnan(trajectory[i]).any():\n",
    "            trajectory[i] = trajectory[i - 1]\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def get_closest_frame_index(requested_frame, total_frames):\n",
    "    \"\"\"Finds the valid index for a requested frame number.\"\"\"\n",
    "    if total_frames == 0:\n",
    "        raise ValueError(\"Cannot get frame index from a trajectory with zero frames.\")\n",
    "    max_index = total_frames - 1\n",
    "    return min(requested_frame, max_index)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ---                        CORE RECONSTRUCTION LOGIC                     ---\n",
    "# ==============================================================================\n",
    "\n",
    "def reconstruct_images(features_c0, features_c1, class_name_c0, class_name_c1, config):\n",
    "    \"\"\"\n",
    "    Calculates mean vectors and generates images using the StyleGAN generator.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Image Reconstruction ---\")\n",
    "\n",
    "    if len(features_c0) == 0 or len(features_c1) == 0:\n",
    "        print(f\"Error: Not enough data for one or both classes. \"\n",
    "              f\"Found {len(features_c0)} samples for '{class_name_c0}' and \"\n",
    "              f\"{len(features_c1)} for '{class_name_c1}'. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # 1. Calculate Mean Feature Vectors\n",
    "    print(\"Calculating mean feature vectors...\")\n",
    "    mean_feature_c0 = np.mean(features_c0, axis=0)\n",
    "    mean_feature_c1 = np.mean(features_c1, axis=0)\n",
    "    mean_of_means = (mean_feature_c0 + mean_feature_c1) / 2.0\n",
    "    print(f\"  - Calculated mean for {len(features_c0)} '{class_name_c0}' samples.\")\n",
    "    print(f\"  - Calculated mean for {len(features_c1)} '{class_name_c1}' samples.\")\n",
    "    print(\"  - Calculated midpoint vector.\")\n",
    "\n",
    "    # 2. Setup Device and Load Generator\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(f\"Loading generator from '{config['network_pkl_path']}'...\")\n",
    "    try:\n",
    "        with dnnlib.util.open_url(config['network_pkl_path']) as fp:\n",
    "            models = legacy.load_network_pkl(fp)\n",
    "            G = models['G_ema'].to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not load the network PKL file. Check the path. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Generate Images\n",
    "    print(\"Synthesizing images from mean vectors...\")\n",
    "    batch_zs = np.vstack([mean_feature_c0, mean_feature_c1, mean_of_means])\n",
    "    batch_zs_tensor = torch.from_numpy(batch_zs).to(device)\n",
    "\n",
    "    # Assumes an unconditional generator (class labels `c` is None)\n",
    "    synth_images = G(batch_zs_tensor, None, noise_mode=\"const\")\n",
    "\n",
    "    # 4. Post-process and Save Images\n",
    "    synth_images = (synth_images + 1) * 127.5  # Denormalize from [-1, 1] to [0, 255]\n",
    "    synth_images = synth_images.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "\n",
    "    # Create a unique sub-directory for this reconstruction run\n",
    "    output_subdir_name = f\"{config['analysis_type']}_T{config['time_point_to_reconstruct']}_{class_name_c0}_vs_{class_name_c1}\"\n",
    "    final_output_dir = os.path.join(config['output_dir'], output_subdir_name)\n",
    "    os.makedirs(final_output_dir, exist_ok=True)\n",
    "    print(f\"Saving images to: {final_output_dir}\")\n",
    "\n",
    "    # Define filenames and save\n",
    "    paths = {\n",
    "        \"class0\": os.path.join(final_output_dir, f\"reconstruction_{class_name_c0}.png\"),\n",
    "        \"class1\": os.path.join(final_output_dir, f\"reconstruction_{class_name_c1}.png\"),\n",
    "        \"midpoint\": os.path.join(final_output_dir, f\"reconstruction_Midpoint.png\"),\n",
    "    }\n",
    "\n",
    "    # OpenCV expects BGR format, so convert from RGB\n",
    "    cv2.imwrite(paths[\"class0\"], cv2.cvtColor(synth_images[0], cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(paths[\"class1\"], cv2.cvtColor(synth_images[1], cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(paths[\"midpoint\"], cv2.cvtColor(synth_images[2], cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"  - Saved: {os.path.basename(paths['class0'])}\")\n",
    "    print(f\"  - Saved: {os.path.basename(paths['class1'])}\")\n",
    "    print(f\"  - Saved: {os.path.basename(paths['midpoint'])}\")\n",
    "    print(\"--- Reconstruction complete! ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ---                            MAIN EXECUTION                            ---\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load data and orchestrate the reconstruction.\"\"\"\n",
    "    cfg = CONFIG\n",
    "\n",
    "    # --- Data Loading Logic ---\n",
    "    if cfg['analysis_type'] == 'WT':\n",
    "        print(f\"--- Starting WT Analysis for Reconstruction ---\")\n",
    "        class0_label, class1_label = cfg['wt_classes_to_compare']\n",
    "        class_name_map = {'F': 'No_Aggregate', 'T': 'Aggregate'}\n",
    "        class_name_c0, class_name_c1 = class_name_map[class0_label], class_name_map[class1_label]\n",
    "\n",
    "        print(f\"Comparing classes: '{class_name_c0}' ({class0_label}) vs. '{class_name_c1}' ({class1_label})\")\n",
    "\n",
    "        try:\n",
    "            labels_df = pd.read_csv(cfg['labeling_csv_path_wt'])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Labeling sheet not found at '{cfg['labeling_csv_path_wt']}'.\")\n",
    "            return\n",
    "\n",
    "        labels_df = labels_df[labels_df['aggregates_formed'].isin([class0_label, class1_label])]\n",
    "\n",
    "        features_c0, features_c1 = [], []\n",
    "\n",
    "        for _, row in labels_df.iterrows():\n",
    "            run_id, scope_id = row['run_id'], row['scope_id']\n",
    "            npz_path = os.path.join(cfg['features_base_dir_wt'], f\"Run{run_id:04d}\", f\"Scope{scope_id:02d}\",\n",
    "                                    \"features.npz\")\n",
    "\n",
    "            if not os.path.exists(npz_path): continue\n",
    "\n",
    "            features_data = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "            frame_idx = get_closest_frame_index(cfg['time_point_to_reconstruct'], features_data.shape[0])\n",
    "            feature_vec = features_data[frame_idx]\n",
    "\n",
    "            if row['aggregates_formed'] == class0_label:\n",
    "                features_c0.append(feature_vec)\n",
    "            else:\n",
    "                features_c1.append(feature_vec)\n",
    "\n",
    "        reconstruct_images(features_c0, features_c1, class_name_c0, class_name_c1, cfg)\n",
    "\n",
    "    elif cfg['analysis_type'] == 'motility':\n",
    "        print(f\"--- Starting Motility Analysis for Reconstruction ---\")\n",
    "        class_name_c0, class_name_c1 = cfg['motility_classes_to_compare']\n",
    "        print(f\"Comparing classes: '{class_name_c0}' vs. '{class_name_c1}'\")\n",
    "\n",
    "        try:\n",
    "            labels_df = pd.read_excel(cfg['motility_csv_path'])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Motility data not found at '{cfg['motility_csv_path']}'.\")\n",
    "            return\n",
    "\n",
    "        strain_to_label_map = pd.Series(labels_df.motility.values, index=labels_df.Strain).to_dict()\n",
    "        dir_pattern = re.compile(r'Run(\\d+)_Mutant(\\d+)')\n",
    "\n",
    "        features_c0, features_c1 = [], []\n",
    "\n",
    "        for dir_name in os.listdir(cfg['features_base_dir_motility']):\n",
    "            match = dir_pattern.match(dir_name)\n",
    "            if not match: continue\n",
    "\n",
    "            mutant_num = int(match.group(2))\n",
    "            strain_id = f\"DK{mutant_num}\"\n",
    "\n",
    "            if strain_id in strain_to_label_map:\n",
    "                label = strain_to_label_map[strain_id]\n",
    "                if label not in [class_name_c0, class_name_c1]: continue\n",
    "\n",
    "                # Find all scope directories within this run/mutant folder\n",
    "                run_mutant_path = os.path.join(cfg['features_base_dir_motility'], dir_name)\n",
    "                for scope_dir_name in os.listdir(run_mutant_path):\n",
    "                    npz_path = os.path.join(run_mutant_path, scope_dir_name, \"features.npz\")\n",
    "                    if os.path.exists(npz_path):\n",
    "                        features_data = impute_nans_with_previous_frame(np.load(npz_path)['z'])\n",
    "                        frame_idx = get_closest_frame_index(cfg['time_point_to_reconstruct'], features_data.shape[0])\n",
    "                        feature_vec = features_data[frame_idx]\n",
    "\n",
    "                        if label == class_name_c0:\n",
    "                            features_c0.append(feature_vec)\n",
    "                        else:\n",
    "                            features_c1.append(feature_vec)\n",
    "\n",
    "        reconstruct_images(features_c0, features_c1, class_name_c0, class_name_c1, cfg)\n",
    "\n",
    "    else:\n",
    "        print(f\"ERROR: Unknown analysis type '{cfg['analysis_type']}'. Choose 'WT' or 'motility'.\")\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(CONFIG[\"network_pkl_path\"]):\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"!!! WARNING: `network_pkl_path` is not set or file not found.   !!!\")\n",
    "        print(\"!!! Please update the CONFIG section before running this script.  !!!\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    else:\n",
    "        main()\n"
   ],
   "id": "b5adf71f874c472c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting WT Analysis for Reconstruction ---\n",
      "Comparing classes: 'No_Aggregate' (F) vs. 'Aggregate' (T)\n",
      "\n",
      "--- Starting Image Reconstruction ---\n",
      "Calculating mean feature vectors...\n",
      "  - Calculated mean for 132 'No_Aggregate' samples.\n",
      "  - Calculated mean for 220 'Aggregate' samples.\n",
      "  - Calculated midpoint vector.\n",
      "Using device: cuda\n",
      "Loading generator from '/home/xavier/Documents/DAE_project/models/network-snapshot-001512-patched.pkl'...\n",
      "Synthesizing images from mean vectors...\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "Saving images to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/centers/WT_T0_No_Aggregate_vs_Aggregate\n",
      "  - Saved: reconstruction_No_Aggregate.png\n",
      "  - Saved: reconstruction_Aggregate.png\n",
      "  - Saved: reconstruction_Midpoint.png\n",
      "--- Reconstruction complete! ---\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optional",
   "id": "7ba977b1d7ab00cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reconstruct label centers",
   "id": "7e72ccae922facfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sample images",
   "id": "6b3f36392452be3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T06:58:39.886035Z",
     "start_time": "2025-09-28T06:58:39.715062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Script: Image Sampler and Processor\n",
    "# ==============================================================================\n",
    "# Purpose:\n",
    "# This script processes image files by first sampling from a labeling sheet.\n",
    "# 1. Loads a CSV file that classifies experiments (by run_id and scope_id)\n",
    "#    into different classes (e.g., aggregates formed 'T' or 'F').\n",
    "# 2. Randomly samples a specified number of experiments from each class.\n",
    "# 3. For each sampled experiment, it processes a list of specified frame numbers.\n",
    "# 4. For each frame, it locates the image, resizes, crops, normalizes brightness,\n",
    "#    and saves the result as a PNG file.\n",
    "#\n",
    "# Instructions:\n",
    "# - Update the CONFIG dictionary with your specific parameters.\n",
    "# - 'labeling_csv_path': Path to the CSV file with run/scope classifications.\n",
    "# - 'samples_per_class': How many experiments to randomly sample from each class.\n",
    "# - 'target_frames_to_process': A list of frame numbers to process for each\n",
    "#   sampled experiment.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from scikit-image about low contrast images\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='skimage.io')\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    \"base_image_dir\": f\"{WORKING_DIR}/dataset/WT/images\",\n",
    "    \"output_dir\": f\"{WORKING_DIR}/images/figure6/WT_analysis/sampled\",\n",
    "    \"labeling_csv_path\": f\"{WORKING_DIR}/dataset/WT/labeling_sheet.csv\",\n",
    "\n",
    "    # --- Sampling Parameters ---\n",
    "    \"samples_per_class\": 3,  # Number of experiments to sample from each class\n",
    "    \"target_frames_to_process\": [1, 1441],  # Frames to process for each sampled experiment\n",
    "    \"random_seed\": 42,  # Seed for reproducible random sampling\n",
    "\n",
    "    # --- Image Processing Parameters ---\n",
    "    \"resize_by\": 1.0,\n",
    "    \"resolution\": 512,\n",
    "    \"brightness_norm\": True,\n",
    "    \"brightness_mean\": 107.2,\n",
    "    \"locations\": [\"center\"],  # Can be a list, e.g., [\"left\", \"center\", \"right\"]\n",
    "    \"crop_offset\": 128,\n",
    "}\n",
    "\n",
    "\n",
    "def resize_crop(img_name, strain_dir, resize_by=1.0, resolution=512, brightness_norm=True, brightness_mean=107.2,\n",
    "                locations=None, crop_offset=128):\n",
    "    \"\"\"\n",
    "    Loads, resizes, and crops an image from multiple locations.\n",
    "    \"\"\"\n",
    "    if locations is None:\n",
    "        locations = [\"center\"]\n",
    "    img_path = os.path.join(strain_dir, img_name)\n",
    "    if not os.path.exists(img_path): return None\n",
    "\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None: return None\n",
    "    if img.dtype != np.uint8: img = np.uint8(img / 256)\n",
    "\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    resize_w, resize_h = int(img_w * resize_by), int(img_h * resize_by)\n",
    "\n",
    "    if resize_by != 1.0:\n",
    "        img = cv2.resize(img, (resize_w, resize_h), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    cropped_imgs = []\n",
    "    for location in locations:\n",
    "        y_start = (resize_h - resolution) // 2\n",
    "        y_end = y_start + resolution\n",
    "\n",
    "        if location == \"left\":\n",
    "            x_start = crop_offset\n",
    "        elif location == \"right\":\n",
    "            x_start = resize_w - crop_offset - resolution\n",
    "        else:  # \"center\" or default\n",
    "            x_start = (resize_w - resolution) // 2\n",
    "\n",
    "        x_end = x_start + resolution\n",
    "        new_img = img[y_start:y_end, x_start:x_end]\n",
    "\n",
    "        if brightness_norm:\n",
    "            obj_v = np.mean(new_img)\n",
    "            value = brightness_mean - obj_v\n",
    "            new_img = cv2.add(new_img, value)\n",
    "        cropped_imgs.append((new_img, location))  # Return image and its location\n",
    "    return cropped_imgs\n",
    "\n",
    "\n",
    "def process_and_save_frame(run_id, scope_id, frame_num, label, base_dir, output_dir, processing_params):\n",
    "    \"\"\"\n",
    "    Loads, processes, and saves a single image frame.\n",
    "\n",
    "    Args:\n",
    "        run_id (int): The run identifier.\n",
    "        scope_id (int): The scope identifier.\n",
    "        frame_num (int): The specific frame number to process.\n",
    "        label (str): The class label for the experiment (e.g., 'T' or 'F').\n",
    "        base_dir (str): The root directory containing the image data.\n",
    "        output_dir (str): The directory where the output .png files will be saved.\n",
    "        processing_params (dict): A dictionary with image processing settings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. Find the correct run folder and construct the image path ---\n",
    "        run_suffix = f\"Run{run_id:04d}\"\n",
    "        run_folder_name = None\n",
    "\n",
    "        try:\n",
    "            run_folder_name = next(\n",
    "                d for d in os.listdir(base_dir) if d.endswith(run_suffix) and os.path.isdir(os.path.join(base_dir, d)))\n",
    "        except StopIteration:\n",
    "            print(f\"Warning: No folder found ending with '{run_suffix}' in '{base_dir}'. Skipping frame {frame_num}.\")\n",
    "            return\n",
    "\n",
    "        scope_dir = os.path.join(base_dir, run_folder_name, f\"Scope{scope_id:02d}\")\n",
    "        image_filename = f\"Run{run_id:04d}_scope{scope_id:d}-00_{frame_num:04d}.jpg\"\n",
    "\n",
    "        # --- 2. Process the image using the resize_crop function ---\n",
    "        cropped_results = resize_crop(\n",
    "            img_name=image_filename,\n",
    "            strain_dir=scope_dir,\n",
    "            **processing_params\n",
    "        )\n",
    "\n",
    "        if not cropped_results:\n",
    "            print(\n",
    "                f\"Warning: Cropping failed for run {run_id}, scope {scope_id}, frame {frame_num}. Image might not exist or be invalid.\")\n",
    "            return\n",
    "\n",
    "        # --- 3. Save the resulting image(s) ---\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for img, location in cropped_results:\n",
    "            # Construct a descriptive output filename including the class label\n",
    "            output_filename = f\"run{run_id:04d}_scope{scope_id:02d}_class{label}_{frame_num:04d}_{location}.png\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "            cv2.imwrite(output_path, img)\n",
    "            print(f\"Successfully processed frame {frame_num} ({location}). Saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing run {run_id}, scope {scope_id}, frame {frame_num}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to sample experiments and process frames.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Image Sampler and Processor Script ---\")\n",
    "\n",
    "    # --- 1. Load and prepare the labeling data ---\n",
    "    try:\n",
    "        labels_df = pd.read_csv(CONFIG['labeling_csv_path'])\n",
    "        labels_df = labels_df.dropna(subset=['aggregates_formed', 'run_id', 'scope_id'])\n",
    "        labels_df = labels_df[labels_df['aggregates_formed'].isin(['T', 'F'])]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Labeling sheet not found at '{CONFIG['labeling_csv_path']}'. Exiting.\")\n",
    "        return\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: The CSV file is missing a required column: {e}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Randomly sample from each class ---\n",
    "    samples_per_class = CONFIG['samples_per_class']\n",
    "    class_T_df = labels_df[labels_df['aggregates_formed'] == 'T']\n",
    "    class_F_df = labels_df[labels_df['aggregates_formed'] == 'F']\n",
    "\n",
    "    if len(class_T_df) < samples_per_class or len(class_F_df) < samples_per_class:\n",
    "        print(\"Warning: Not enough samples in the CSV for the requested number.\")\n",
    "        print(f\"  - Class 'T' has {len(class_T_df)} samples.\")\n",
    "        print(f\"  - Class 'F' has {len(class_F_df)} samples.\")\n",
    "        print(f\"  - Requested {samples_per_class} samples per class.\")\n",
    "        # Adjusting sample count to the minimum available\n",
    "        samples_per_class = min(len(class_T_df), len(class_F_df))\n",
    "        if samples_per_class == 0:\n",
    "            print(\"ERROR: Cannot proceed with 0 samples in one of the classes. Exiting.\")\n",
    "            return\n",
    "        print(f\"  Proceeding with {samples_per_class} samples per class.\")\n",
    "\n",
    "    sampled_T = class_T_df.sample(n=samples_per_class, random_state=CONFIG['random_seed'])\n",
    "    sampled_F = class_F_df.sample(n=samples_per_class, random_state=CONFIG['random_seed'])\n",
    "\n",
    "    combined_samples = pd.concat([sampled_T, sampled_F])\n",
    "    print(f\"\\nSuccessfully sampled {len(combined_samples)} total experiments.\")\n",
    "\n",
    "    # --- 3. Process the sampled frames ---\n",
    "    base_dir = CONFIG[\"base_image_dir\"]\n",
    "    output_dir = CONFIG[\"output_dir\"]\n",
    "    target_frames = CONFIG[\"target_frames_to_process\"]\n",
    "\n",
    "    processing_params = {\n",
    "        \"resize_by\": CONFIG[\"resize_by\"],\n",
    "        \"resolution\": CONFIG[\"resolution\"],\n",
    "        \"brightness_norm\": CONFIG[\"brightness_norm\"],\n",
    "        \"brightness_mean\": CONFIG[\"brightness_mean\"],\n",
    "        \"locations\": CONFIG[\"locations\"],\n",
    "        \"crop_offset\": CONFIG[\"crop_offset\"],\n",
    "    }\n",
    "\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Base Directory: {base_dir}\")\n",
    "    print(f\"  - Output Directory: {output_dir}\")\n",
    "    print(f\"  - Frames to process per sample: {target_frames}\")\n",
    "\n",
    "    # Loop through each sampled experiment\n",
    "    for index, row in combined_samples.iterrows():\n",
    "        run_id = int(row['run_id'])\n",
    "        scope_id = int(row['scope_id'])\n",
    "        label = row['aggregates_formed']\n",
    "\n",
    "        print(f\"\\nProcessing sampled experiment: Run {run_id}, Scope {scope_id} (Class: {label})\")\n",
    "\n",
    "        # Loop through each specified frame number for that experiment\n",
    "        for frame in target_frames:\n",
    "            process_and_save_frame(run_id, scope_id, frame, label, base_dir, output_dir, processing_params)\n",
    "\n",
    "    print(\"\\n--- Script execution complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "123d14a12d5bc30e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Image Sampler and Processor Script ---\n",
      "\n",
      "Successfully sampled 6 total experiments.\n",
      "Configuration:\n",
      "  - Base Directory: /home/xavier/Documents/DAE_project/dataset/WT/images\n",
      "  - Output Directory: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis\n",
      "  - Frames to process per sample: [1, 1441]\n",
      "\n",
      "Processing sampled experiment: Run 220, Scope 46 (Class: T)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0220_scope46_classT_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0220_scope46_classT_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 373, Scope 40 (Class: T)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0373_scope40_classT_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0373_scope40_classT_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 199, Scope 21 (Class: T)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0199_scope21_classT_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0199_scope21_classT_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 900, Scope 27 (Class: F)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0900_scope27_classF_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0900_scope27_classF_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 903, Scope 77 (Class: F)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0903_scope77_classF_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0903_scope77_classF_1441_center.png\n",
      "\n",
      "Processing sampled experiment: Run 825, Scope 22 (Class: F)\n",
      "Successfully processed frame 1 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0825_scope22_classF_0001_center.png\n",
      "Successfully processed frame 1441 (center). Saved to: /home/xavier/Documents/DAE_project/images/figure6/WT_analysis/run0825_scope22_classF_1441_center.png\n",
      "\n",
      "--- Script execution complete ---\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b8eb14e52303743"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
